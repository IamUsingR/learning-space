{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O aspecto mais importante antes de qualquer aplicação de Machine Learning é sem sombra de dúvidas o pré-processamento dos dados. \n",
    "\n",
    "Por mais que grande parte dos tutoriais e livros atenham-se somente à parte de volume de dados (e.g. Big Data) ou mesmo na parte de técnicas propriamente ditas (e.g. Regressão Linear, Support Vector Machines, Redes Neurais, et cetera...) o conjunto de dados (a.k.a. dataset, base de dados, et cetera) exerce função determinante no sucesso ou a falta dele na aplicação de Machine Learning. \n",
    "\n",
    "Em uma analogia rápida, os dados são como uma chave e os algoritmos são como uma fechadura de uma porta.\n",
    "\n",
    "Você pode ter uma chave com desgaste... \n",
    "(e.g. truncamento de dados, missing data, dados não normalizados, et cetera)\n",
    "\n",
    "Você pode ter a fechadura da porta com desgaste... \n",
    "(e.g. ajuste paramétrico para 'fittar', eliminação de steps de cálculos, utilização de 'ensembles' para gambiarra no pipeline de algoritmos, et cetera)\n",
    "\n",
    "Mas a fechadura somente vai abrir com a chave correspondente. \n",
    "(e.g. base de dados com os atributos de predição e de resultado (target) corretas).\n",
    "\n",
    "Agora que já sabemos a importância de se ter os dados para aplicação dos algoritmos, e por consequência criação de modelos, vamos falar de um aspecto importante na questão do input dos dados que é o Pré-Processamento (Preprocessing)\n",
    "\n",
    "O pré-processamento de dados é toda e qualquer atividade relativa ao ajuste dos dados tanto em termos de formatação, amostragem para que os algoritmos consigam realizar as atividades de construção dos modelos de forma a ter um melhor aproveitamento em termos computacionais (complexidade temporal + complexidade espacial).\n",
    "\n",
    "As principais atividades do pré-processamento de dados são:\n",
    "\n",
    "- Amostragem (Sampling)\n",
    "- Feature Engineering que contém (i) Feature Extraction e (ii) Feature Selection\n",
    "- Binning \n",
    "- Scaling\n",
    "- Padronização (Standarize)\n",
    "- Normalização\n",
    "- Dimensionality Reduction\n",
    "\n",
    "Vamos começar com primeiramente com a importação dos dados.\n",
    "\n",
    "Nesse primeiro chunk vamos utilizar os dados da própria bilbioteca do Scikit-Learn relativa aos dados da base de dados Iris.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente vamos importar os datasets do pacote do Scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos criar um objeto com o nome de 'datasets' e vamos carregar a base de dados iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa base de dados Iris é dividida em duas partes, uma chamada 'data' e outra chamada 'target'.\n",
    "\n",
    "O conjunto de dados 'data' possuí todos os atributos (i.e. variáveis independentes, predictors, features, etc) relativos à base Iris.\n",
    "\n",
    "O conjunto de dados 'target' possuí todos as classes (i.e. variável dependente, target features, etc.) da mesma base Iris.\n",
    "\n",
    "Vamos ver agora a quantidade de dados em cada um dos datasets usando a função 'shape'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(datasets.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dataset 'data' temos 150 registros (i.e. tuplas, linhas, etc) e 4 atributos. \n",
    "\n",
    "Vejamos o que temos no datset 'target'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(datasets.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de se esperar, temos somente uma coluna, que é a variável dependente.\n",
    "\n",
    "Mas uma das boas práticas de pré-processamento é sempre conferir se os dados foram realmente carregados. Pra isso vamos ver cada um dos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já vimos que os nossos dados foram carregados, apenas para fins de exemplificação vamos colocar cada um desses dados em um objeto distinto. \n",
    "\n",
    "\n",
    "Para padronização de nomenclatura, sempre que usarmos um conjunto de dados que consta os atributos, vamos atribuir o objeto 'X', e no caso da variável dependente vamos atribuir um objeto de dados com o nome de 'y'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = datasets.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = datasets.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos conferir os nossos objetos 'X' e 'y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para importar dados que estejam na web, podemos usar o numpy que é uma bilbioteca específica para tratamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um objeto chamado url que terá a string do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(\"http://goo.gl/j0Rvxq\") as url:\n",
    "    s = url.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objeto 'raw_data' vai abrir uma url e vai armazenar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o objeto armazenado, vamos realizar a atribuição, já realizar a separaçnao pelo delimitador que será uma vírgula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(raw_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos conferir o nosso dataset como está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23279,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aos moldes do exemplo anterior, vamos atribuir as variáveis independentes no objeto X e a variável dependente no objeto y; sendo que:\n",
    "\n",
    "- Da variável 0 até a 7 serão as variáveis independentes (X)\n",
    "- A variável 8 será a variável dependente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset[:,0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados carregados e checados, vamos agora realizar algumas atividades de pré-processamento. \n",
    "\n",
    "A primeira de pré-processamento que vamos utilizar será o Scaling. \n",
    "\n",
    "O Scaling é bastante utilizado quando se tem um conjunto de dados com valores com uma variância muito alta, e que em termos custo computacional seria custoso.\n",
    "\n",
    "\n",
    "\n",
    "Primeiramente, vamos importar a biblioteca 'preprocessing' do pacote do Scikit-Learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um objeto chamado 'normalized_X' em que vamos aplicar a função scale da biblioteca preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objeto criado, vamos agora fazer uma pequena comparação entre o objeto X (nossos dados) e o objeto normalized_X (o que acabamos de normalizar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos atestar visualmente, tivemos uma mudança significativa de dos valores, em que  praticamente os valores sairam da casa da metade de dezena, para um nível muito maior de precisão decimal e também possuí valores negativos.\n",
    "\n",
    "\n",
    "Outra forma de pré-processamento é a normalização.\n",
    "\n",
    "A normalização tem o mesmo princípio que a padronização, porém com a diferença fundamental que o nível de escala é reduzido.\n",
    "\n",
    "Da mesma forma que fizemos na padronização, vamos criar um objeto e vamos usar a função 'normalize' da biblioteca preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standarized_X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora comparar o output dos dados dos objetos X e standarized_X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "standarized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos os dados foram para a escala de 0 a 1. \n",
    "\n",
    "Com isso os algoritmos podem fazer um trabalho muito melhor, dado que o range de dados foi diminuído. \n",
    "\n",
    "Agora, um aspecto fundamental quando lidamos com um grande volume de dados em que diz respeito ao número de atributos (ou dimensionalidade) é saber quais variáveis independentes são importantes para treinamento do modelo de predição. \n",
    "\n",
    "Para fazer essa atividade, vamos usar uma das bibliotecas do Scikit chamada 'ExtraTreesClassifier' que tem uma função que atesta quais variáveis tem o maior poder preditivo. \n",
    "\n",
    "Primeiramente, vamos importar a biblioteca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biblioteca importada, agora vamos criar o nosso modelo realizando uma chamada na biblioteca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos realizar o 'fitting' do modelo. \n",
    "\n",
    "Uma dica é que sempre que for ser utilizado algum modelo para fitting, usando bibliotecas de aprendizado supervisionado, a regra sempre será:\n",
    "\n",
    "- objeto_do_modelo.fit (X,y)\n",
    "\n",
    "Em que objeto_do_modelo é o modelo preditivo utilizado, X as variáveis independentes e y a variável dependente a ser predita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(datasets.data, datasets.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos o modelo, porém vamos usar as configurações básicas. Mas de maneira geral usamos o algoritmo de árvores de decisão para obtermos a métrica de importância de cada variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse resultado mostra que em termos de importância preditiva, as variáveis mais significativas são a 3(57%), 4(33%), 1 e (5% cada).\n",
    "\n",
    "Caso houvessem muitas variáveis a serem consideradas, e quisessemos diminuir o custo computacional (e consequentemente o tempo) poderiamos eliminar essas duas variáveis, mas como estamos simplificando vamos considerar tudo.\n",
    "\n",
    "Com isso terminamos o pré-processamento básico. \n",
    "\n",
    "Como regra geral de modelagem, o ideal é que essas informações venham já processadas do banco de dados, dado que o mecanismo do SGBD é apto para esse tipo de atividade, tanto em termos de velocidade quanto em robustez de processamento. \n",
    "\n",
    "E além do mais, o importante é que quanto mais modularizado em que há uma cadeia de ferramentas fazendo aquilo o que ela faz de melhor funcionando, a complexidade do sistema como um todo caí, e aumenta a robuztez quanto à problemas que possam acontecer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

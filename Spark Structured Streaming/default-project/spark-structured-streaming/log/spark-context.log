[34m[INFO ][0;39m [35m[2017-11-03 08:02:22,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running Spark version 2.2.0
[31m[WARN ][0;39m [35m[2017-11-03 08:02:22,547][0;39m [33m[][0;39m [35m[org.apache.hadoop.util.NativeCodeLoader-><clinit>][0;39m | Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitted application: Spark Structured Streaming Job
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,259][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing view acls to: flavio.clesio
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing modify acls to: flavio.clesio
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing view acls groups to: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing modify acls groups to: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(flavio.clesio); groups with view permissions: Set(); users  with modify permissions: Set(flavio.clesio); groups with modify permissions: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 63882.
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManagerMasterEndpoint up
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/blockmgr-19c40575-1396-423f-a4b1-f163ffd1704c
[34m[INFO ][0;39m [35m[2017-11-03 08:02:23,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MemoryStore started with capacity 912.3 MB
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Bound SparkUI to 0.0.0.0, and started at http://192.168.216.37:4040
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63883.
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Server created on 192.168.216.37:63883
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,724][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering BlockManager BlockManagerId(driver, 192.168.216.37, 63883, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering block manager 192.168.216.37:63883 with 912.3 MB RAM, BlockManagerId(driver, 192.168.216.37, 63883, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,742][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registered BlockManager BlockManagerId(driver, 192.168.216.37, 63883, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:24,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Initialized BlockManager: BlockManagerId(driver, 192.168.216.37, 63883, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:25,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/spark-warehouse').
[34m[INFO ][0;39m [35m[2017-11-03 08:02:25,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Warehouse path is 'file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/spark-warehouse'.
[34m[INFO ][0;39m [35m[2017-11-03 08:02:28,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registered StateStoreCoordinator endpoint
[34m[INFO ][0;39m [35m[2017-11-03 08:02:31,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:31,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:31,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<name: string, country: string, city: string, phone: string, age: int ... 5 more fields>
[34m[INFO ][0;39m [35m[2017-11-03 08:02:31,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:32,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 446.608397 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:32,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 221.7 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on 192.168.216.37:63883 (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 0 from show at SparkSQL.scala:42
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,077][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: show at SparkSQL.scala:42
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 0 (show at SparkSQL.scala:42) with 1 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (show at SparkSQL.scala:42)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[2] at show at SparkSQL.scala:42), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 10.9 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.3 KB, free 912.0 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on 192.168.216.37:63883 (size: 6.3 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at SparkSQL.scala:42) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5341 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.1.csv, range: 0-6730, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 31.629545 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 3070 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 237 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 0 (show at SparkSQL.scala:42) finished in 0.275 s
[34m[INFO ][0;39m [35m[2017-11-03 08:02:33,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 0 finished: show at SparkSQL.scala:42, took 0.761908 s
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 34.466722 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parsing command: user_records
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parsing command: 
        SELECT carrier, marital_status, COUNT(1) as num_users
        FROM user_records
        GROUP BY carrier, marital_status
        ORDER BY carrier, marital_status
      
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<carrier: string, marital_status: string>
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 18.854819 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 75.837192 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 88.925089 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 221.7 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on 192.168.216.37:63883 (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 2 from show at SparkSQL.scala:55
[34m[INFO ][0;39m [35m[2017-11-03 08:02:34,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: show at SparkSQL.scala:55
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering RDD 5 (show at SparkSQL.scala:55)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 1 (show at SparkSQL.scala:55) with 200 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 2 (show at SparkSQL.scala:55)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 1)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 1)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at show at SparkSQL.scala:55), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 26.9 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.1 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on 192.168.216.37:63883 (size: 13.1 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at show at SparkSQL.scala:55) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 1.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 13.498242 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 8.95064 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 15.550971 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 13.910129 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 7.449804 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.1.csv, range: 0-6730, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 2307 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 279 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,432][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ShuffleMapStage 1 (show at SparkSQL.scala:55) finished in 0.281 s
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,434][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,435][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,436][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 2)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 2 (MapPartitionsRDD[9] at show at SparkSQL.scala:55), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 26.8 KB, free 911.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 13.4 KB, free 911.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on 192.168.216.37:63883 (size: 13.4 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at show at SparkSQL.scala:55) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 2.0 with 200 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 4, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 4.0 in stage 2.0 (TID 5, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 3)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 2)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 4)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 4.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 14 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 14 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 15 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 3). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 5.0 in stage 2.0 (TID 6, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 4). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 5.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 2.0 (TID 5). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 6.0 in stage 2.0 (TID 7, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,728][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 7.0 in stage 2.0 (TID 8, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,728][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 3) in 193 ms on localhost (executor driver) (1/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 7.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 4) in 193 ms on localhost (executor driver) (2/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 6.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 2.0 (TID 5) in 192 ms on localhost (executor driver) (3/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 2). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 8.0 in stage 2.0 (TID 9, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 8.0 in stage 2.0 (TID 9)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 2) in 209 ms on localhost (executor driver) (4/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 2.0 (TID 6). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 9.0 in stage 2.0 (TID 10, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 9.0 in stage 2.0 (TID 10)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 2.0 (TID 6) in 23 ms on localhost (executor driver) (5/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 2.0 (TID 7). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,756][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 10.0 in stage 2.0 (TID 11, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 10.0 in stage 2.0 (TID 11)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 2.0 (TID 7) in 34 ms on localhost (executor driver) (6/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,789][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 2.0 (TID 8). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 11.0 in stage 2.0 (TID 12, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 2.0 (TID 8) in 63 ms on localhost (executor driver) (7/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 11.0 in stage 2.0 (TID 12)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 2.0 (TID 9). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 12.0 in stage 2.0 (TID 13, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 2.0 (TID 9) in 92 ms on localhost (executor driver) (8/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 12.0 in stage 2.0 (TID 13)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 2.0 (TID 11). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 13.0 in stage 2.0 (TID 14, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 2.0 (TID 11) in 87 ms on localhost (executor driver) (9/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,854][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 13.0 in stage 2.0 (TID 14)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 59 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 2.0 (TID 10). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 14.0 in stage 2.0 (TID 15, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 2.0 (TID 10) in 183 ms on localhost (executor driver) (10/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 14.0 in stage 2.0 (TID 15)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 9 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 2.0 (TID 14). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 15.0 in stage 2.0 (TID 16, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 2.0 (TID 14) in 113 ms on localhost (executor driver) (11/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 15.0 in stage 2.0 (TID 16)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 2.0 (TID 12). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 2.0 (TID 13). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 16.0 in stage 2.0 (TID 17, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 2.0 (TID 15). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 16.0 in stage 2.0 (TID 17)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 17.0 in stage 2.0 (TID 18, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 18.0 in stage 2.0 (TID 19, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 17.0 in stage 2.0 (TID 18)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 2.0 (TID 12) in 180 ms on localhost (executor driver) (12/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 2.0 (TID 13) in 173 ms on localhost (executor driver) (13/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 2.0 (TID 15) in 49 ms on localhost (executor driver) (14/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 18.0 in stage 2.0 (TID 19)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,983][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 2.0 (TID 18). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 20.0 in stage 2.0 (TID 20, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 2.0 (TID 18) in 19 ms on localhost (executor driver) (15/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 20.0 in stage 2.0 (TID 20)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 2.0 (TID 19). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 21.0 in stage 2.0 (TID 21, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 2.0 (TID 19) in 24 ms on localhost (executor driver) (16/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,994][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 21.0 in stage 2.0 (TID 21)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:35,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 2.0 (TID 20). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 22.0 in stage 2.0 (TID 22, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 22.0 in stage 2.0 (TID 22)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 2.0 (TID 20) in 19 ms on localhost (executor driver) (17/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 2.0 (TID 21). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 23.0 in stage 2.0 (TID 23, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 2.0 (TID 17). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 24.0 in stage 2.0 (TID 24, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 2.0 (TID 21) in 54 ms on localhost (executor driver) (18/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 24.0 in stage 2.0 (TID 24)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 2.0 (TID 17) in 81 ms on localhost (executor driver) (19/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 23.0 in stage 2.0 (TID 23)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 2.0 (TID 16). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 25.0 in stage 2.0 (TID 25, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 25.0 in stage 2.0 (TID 25)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 2.0 (TID 16) in 106 ms on localhost (executor driver) (20/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 2.0 (TID 24). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,102][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 15 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 26.0 in stage 2.0 (TID 26, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 26.0 in stage 2.0 (TID 26)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 2.0 (TID 24) in 71 ms on localhost (executor driver) (21/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 2.0 (TID 22). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 27.0 in stage 2.0 (TID 27, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 2.0 (TID 23). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 28.0 in stage 2.0 (TID 28, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 27.0 in stage 2.0 (TID 27)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 2.0 (TID 22) in 114 ms on localhost (executor driver) (22/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 2.0 (TID 23) in 79 ms on localhost (executor driver) (23/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 28.0 in stage 2.0 (TID 28)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 2.0 (TID 27). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 29.0 in stage 2.0 (TID 29, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 29.0 in stage 2.0 (TID 29)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 2.0 (TID 27) in 14 ms on localhost (executor driver) (24/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 2.0 (TID 25). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 31.0 in stage 2.0 (TID 30, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 2.0 (TID 25) in 77 ms on localhost (executor driver) (25/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 31.0 in stage 2.0 (TID 30)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 2.0 (TID 28). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 32.0 in stage 2.0 (TID 31, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 2.0 (TID 26). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,179][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 12 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 2.0 (TID 28) in 66 ms on localhost (executor driver) (26/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 32.0 in stage 2.0 (TID 31)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 33.0 in stage 2.0 (TID 32, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,184][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 2.0 (TID 26) in 80 ms on localhost (executor driver) (27/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 33.0 in stage 2.0 (TID 32)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 2.0 (TID 30). 3968 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 34.0 in stage 2.0 (TID 33, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 34.0 in stage 2.0 (TID 33)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 2.0 (TID 30) in 61 ms on localhost (executor driver) (28/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on 192.168.216.37:63883 in memory (size: 6.3 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_3_piece0 on 192.168.216.37:63883 in memory (size: 13.1 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 53
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 2.0 (TID 33). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 22 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 2.0 (TID 31). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 35.0 in stage 2.0 (TID 34, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 36.0 in stage 2.0 (TID 35, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 36.0 in stage 2.0 (TID 35)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 2.0 (TID 33) in 54 ms on localhost (executor driver) (29/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 2.0 (TID 31) in 73 ms on localhost (executor driver) (30/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 2.0 (TID 32). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 37.0 in stage 2.0 (TID 36, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,259][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 37.0 in stage 2.0 (TID 36)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,259][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 35.0 in stage 2.0 (TID 34)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 2.0 (TID 32) in 80 ms on localhost (executor driver) (31/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 2.0 (TID 29). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 20 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 38.0 in stage 2.0 (TID 37, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 2.0 (TID 36). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 2.0 (TID 29) in 177 ms on localhost (executor driver) (32/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 38.0 in stage 2.0 (TID 37)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 39.0 in stage 2.0 (TID 38, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 2.0 (TID 36) in 59 ms on localhost (executor driver) (33/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 39.0 in stage 2.0 (TID 38)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,313][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,313][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 2.0 (TID 35). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 40.0 in stage 2.0 (TID 39, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 2.0 (TID 35) in 75 ms on localhost (executor driver) (34/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 2.0 (TID 38). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 41.0 in stage 2.0 (TID 40, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 41.0 in stage 2.0 (TID 40)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 2.0 (TID 38) in 22 ms on localhost (executor driver) (35/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 11 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 2.0 (TID 37). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 42.0 in stage 2.0 (TID 41, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 2.0 (TID 37) in 47 ms on localhost (executor driver) (36/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 42.0 in stage 2.0 (TID 41)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 2.0 (TID 40). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 43.0 in stage 2.0 (TID 42, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 43.0 in stage 2.0 (TID 42)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 2.0 (TID 40) in 30 ms on localhost (executor driver) (37/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 2.0 (TID 34). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 44.0 in stage 2.0 (TID 43, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 44.0 in stage 2.0 (TID 43)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 2.0 (TID 34) in 120 ms on localhost (executor driver) (38/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 40.0 in stage 2.0 (TID 39)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 24 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 2.0 (TID 41). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 45.0 in stage 2.0 (TID 44, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 50 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 2.0 (TID 39). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 2.0 (TID 43). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 2.0 (TID 41) in 74 ms on localhost (executor driver) (39/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 45.0 in stage 2.0 (TID 44)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 2.0 (TID 42). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 47.0 in stage 2.0 (TID 45, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 48.0 in stage 2.0 (TID 46, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 47.0 in stage 2.0 (TID 45)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 2.0 (TID 39) in 114 ms on localhost (executor driver) (40/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 49.0 in stage 2.0 (TID 47, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 2.0 (TID 43) in 76 ms on localhost (executor driver) (41/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 2.0 (TID 42) in 85 ms on localhost (executor driver) (42/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 49.0 in stage 2.0 (TID 47)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,435][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 48.0 in stage 2.0 (TID 46)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 2.0 (TID 47). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 50.0 in stage 2.0 (TID 48, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 50.0 in stage 2.0 (TID 48)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 2.0 (TID 47) in 30 ms on localhost (executor driver) (43/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 2.0 (TID 48). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 2.0 (TID 46). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 2.0 (TID 45). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 2.0 (TID 44). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 51.0 in stage 2.0 (TID 49, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 51.0 in stage 2.0 (TID 49)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 52.0 in stage 2.0 (TID 50, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 53.0 in stage 2.0 (TID 51, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 52.0 in stage 2.0 (TID 50)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 54.0 in stage 2.0 (TID 52, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 2.0 (TID 48) in 60 ms on localhost (executor driver) (44/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 2.0 (TID 46) in 86 ms on localhost (executor driver) (45/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 2.0 (TID 44) in 103 ms on localhost (executor driver) (46/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 2.0 (TID 45) in 96 ms on localhost (executor driver) (47/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 53.0 in stage 2.0 (TID 51)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,516][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 2.0 (TID 49). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 2.0 (TID 50). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 55.0 in stage 2.0 (TID 53, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 57.0 in stage 2.0 (TID 54, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 55.0 in stage 2.0 (TID 53)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 2.0 (TID 49) in 16 ms on localhost (executor driver) (48/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 57.0 in stage 2.0 (TID 54)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 2.0 (TID 50) in 13 ms on localhost (executor driver) (49/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 54.0 in stage 2.0 (TID 52)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 2.0 (TID 51). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 58.0 in stage 2.0 (TID 55, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 58.0 in stage 2.0 (TID 55)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 2.0 (TID 51) in 15 ms on localhost (executor driver) (50/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 2.0 (TID 53). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 59.0 in stage 2.0 (TID 56, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 2.0 (TID 54). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 2.0 (TID 52). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 59.0 in stage 2.0 (TID 56)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 60.0 in stage 2.0 (TID 57, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 2.0 (TID 55). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 61.0 in stage 2.0 (TID 58, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 62.0 in stage 2.0 (TID 59, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 2.0 (TID 53) in 16 ms on localhost (executor driver) (51/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 2.0 (TID 54) in 16 ms on localhost (executor driver) (52/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 2.0 (TID 55) in 12 ms on localhost (executor driver) (53/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 61.0 in stage 2.0 (TID 58)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 2.0 (TID 52) in 26 ms on localhost (executor driver) (54/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 62.0 in stage 2.0 (TID 59)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 2.0 (TID 56). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 63.0 in stage 2.0 (TID 60, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,542][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,542][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 2.0 (TID 56) in 12 ms on localhost (executor driver) (55/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,542][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 63.0 in stage 2.0 (TID 60)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 60.0 in stage 2.0 (TID 57)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 2.0 (TID 58). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 64.0 in stage 2.0 (TID 61, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 2.0 (TID 58) in 17 ms on localhost (executor driver) (56/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 64.0 in stage 2.0 (TID 61)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 2.0 (TID 60). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 2.0 (TID 57). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 65.0 in stage 2.0 (TID 62, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 68.0 in stage 2.0 (TID 63, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 2.0 (TID 60) in 27 ms on localhost (executor driver) (57/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 2.0 (TID 57) in 37 ms on localhost (executor driver) (58/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 68.0 in stage 2.0 (TID 63)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 65.0 in stage 2.0 (TID 62)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 2.0 (TID 59). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 69.0 in stage 2.0 (TID 64, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 2.0 (TID 61). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 70.0 in stage 2.0 (TID 65, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 69.0 in stage 2.0 (TID 64)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 2.0 (TID 59) in 72 ms on localhost (executor driver) (59/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 2.0 (TID 61) in 62 ms on localhost (executor driver) (60/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 70.0 in stage 2.0 (TID 65)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 2.0 (TID 64). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 71.0 in stage 2.0 (TID 66, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 71.0 in stage 2.0 (TID 66)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 2.0 (TID 64) in 25 ms on localhost (executor driver) (61/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 2.0 (TID 65). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 73.0 in stage 2.0 (TID 67, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 2.0 (TID 65) in 44 ms on localhost (executor driver) (62/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 2.0 (TID 63). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 2.0 (TID 62). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 73.0 in stage 2.0 (TID 67)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 74.0 in stage 2.0 (TID 68, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 75.0 in stage 2.0 (TID 69, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 74.0 in stage 2.0 (TID 68)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 2.0 (TID 66). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 2.0 (TID 63) in 121 ms on localhost (executor driver) (63/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 75.0 in stage 2.0 (TID 69)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 76.0 in stage 2.0 (TID 70, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 2.0 (TID 62) in 125 ms on localhost (executor driver) (64/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 2.0 (TID 67). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 76.0 in stage 2.0 (TID 70)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 2.0 (TID 66) in 70 ms on localhost (executor driver) (65/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 2.0 (TID 68). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 77.0 in stage 2.0 (TID 71, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,700][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 78.0 in stage 2.0 (TID 72, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 77.0 in stage 2.0 (TID 71)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 2.0 (TID 69). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 2.0 (TID 67) in 60 ms on localhost (executor driver) (66/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,705][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 78.0 in stage 2.0 (TID 72)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 2.0 (TID 68) in 29 ms on localhost (executor driver) (67/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 2.0 (TID 70). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 79.0 in stage 2.0 (TID 73, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 80.0 in stage 2.0 (TID 74, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 2.0 (TID 69) in 32 ms on localhost (executor driver) (68/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 79.0 in stage 2.0 (TID 73)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 2.0 (TID 71). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 80.0 in stage 2.0 (TID 74)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 2.0 (TID 70) in 29 ms on localhost (executor driver) (69/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 81.0 in stage 2.0 (TID 75, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 81.0 in stage 2.0 (TID 75)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 2.0 (TID 71) in 22 ms on localhost (executor driver) (70/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 2.0 (TID 72). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 82.0 in stage 2.0 (TID 76, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,724][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 2.0 (TID 72) in 25 ms on localhost (executor driver) (71/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 2.0 (TID 74). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 82.0 in stage 2.0 (TID 76)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 2.0 (TID 73). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 83.0 in stage 2.0 (TID 77, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 84.0 in stage 2.0 (TID 78, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 83.0 in stage 2.0 (TID 77)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 2.0 (TID 75). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 2.0 (TID 74) in 26 ms on localhost (executor driver) (72/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 84.0 in stage 2.0 (TID 78)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 85.0 in stage 2.0 (TID 79, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 2.0 (TID 73) in 32 ms on localhost (executor driver) (73/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 85.0 in stage 2.0 (TID 79)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 2.0 (TID 75) in 25 ms on localhost (executor driver) (74/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 2.0 (TID 76). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 86.0 in stage 2.0 (TID 80, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 2.0 (TID 76) in 29 ms on localhost (executor driver) (75/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 86.0 in stage 2.0 (TID 80)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 2.0 (TID 77). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 2.0 (TID 79). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 87.0 in stage 2.0 (TID 81, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 87.0 in stage 2.0 (TID 81)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 88.0 in stage 2.0 (TID 82, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 2.0 (TID 77) in 30 ms on localhost (executor driver) (76/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 88.0 in stage 2.0 (TID 82)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 2.0 (TID 78). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 2.0 (TID 80). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 2.0 (TID 79) in 26 ms on localhost (executor driver) (77/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 89.0 in stage 2.0 (TID 83, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 90.0 in stage 2.0 (TID 84, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 2.0 (TID 78) in 31 ms on localhost (executor driver) (78/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 89.0 in stage 2.0 (TID 83)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 2.0 (TID 80) in 20 ms on localhost (executor driver) (79/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 2.0 (TID 81). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 91.0 in stage 2.0 (TID 85, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 90.0 in stage 2.0 (TID 84)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,775][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 91.0 in stage 2.0 (TID 85)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 2.0 (TID 82). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 92.0 in stage 2.0 (TID 86, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 2.0 (TID 82) in 22 ms on localhost (executor driver) (80/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 92.0 in stage 2.0 (TID 86)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 2.0 (TID 86). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 93.0 in stage 2.0 (TID 87, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 2.0 (TID 86) in 16 ms on localhost (executor driver) (81/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,797][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 2.0 (TID 81) in 39 ms on localhost (executor driver) (82/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 93.0 in stage 2.0 (TID 87)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 2.0 (TID 85). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 94.0 in stage 2.0 (TID 88, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 2.0 (TID 84). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 2.0 (TID 83). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 2.0 (TID 85) in 39 ms on localhost (executor driver) (83/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 94.0 in stage 2.0 (TID 88)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 95.0 in stage 2.0 (TID 89, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 96.0 in stage 2.0 (TID 90, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 95.0 in stage 2.0 (TID 89)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 2.0 (TID 84) in 47 ms on localhost (executor driver) (84/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 96.0 in stage 2.0 (TID 90)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 2.0 (TID 83) in 50 ms on localhost (executor driver) (85/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 2.0 (TID 90). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 97.0 in stage 2.0 (TID 91, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 97.0 in stage 2.0 (TID 91)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 2.0 (TID 90) in 13 ms on localhost (executor driver) (86/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 2.0 (TID 88). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 98.0 in stage 2.0 (TID 92, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 98.0 in stage 2.0 (TID 92)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 2.0 (TID 88) in 28 ms on localhost (executor driver) (87/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 2.0 (TID 91). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 99.0 in stage 2.0 (TID 93, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 2.0 (TID 89). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 99.0 in stage 2.0 (TID 93)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 2.0 (TID 91) in 15 ms on localhost (executor driver) (88/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 100.0 in stage 2.0 (TID 94, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 2.0 (TID 89) in 32 ms on localhost (executor driver) (89/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 100.0 in stage 2.0 (TID 94)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 2.0 (TID 92). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 101.0 in stage 2.0 (TID 95, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 101.0 in stage 2.0 (TID 95)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 2.0 (TID 92) in 16 ms on localhost (executor driver) (90/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 2.0 (TID 93). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 2.0 (TID 94). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 2.0 (TID 95). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 102.0 in stage 2.0 (TID 96, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 102.0 in stage 2.0 (TID 96)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 103.0 in stage 2.0 (TID 97, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 2.0 (TID 87). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 103.0 in stage 2.0 (TID 97)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 104.0 in stage 2.0 (TID 98, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 105.0 in stage 2.0 (TID 99, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 104.0 in stage 2.0 (TID 98)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 2.0 (TID 93) in 37 ms on localhost (executor driver) (91/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 105.0 in stage 2.0 (TID 99)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 2.0 (TID 96). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 2.0 (TID 94) in 43 ms on localhost (executor driver) (92/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 2.0 (TID 95) in 39 ms on localhost (executor driver) (93/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 2.0 (TID 87) in 92 ms on localhost (executor driver) (94/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 106.0 in stage 2.0 (TID 100, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 2.0 (TID 97). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 106.0 in stage 2.0 (TID 100)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 2.0 (TID 96) in 32 ms on localhost (executor driver) (95/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 2.0 (TID 98). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 107.0 in stage 2.0 (TID 101, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 2.0 (TID 99). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 109.0 in stage 2.0 (TID 102, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 107.0 in stage 2.0 (TID 101)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,903][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 110.0 in stage 2.0 (TID 103, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 109.0 in stage 2.0 (TID 102)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 2.0 (TID 97) in 45 ms on localhost (executor driver) (96/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,910][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 110.0 in stage 2.0 (TID 103)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 2.0 (TID 100). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 2.0 (TID 99) in 40 ms on localhost (executor driver) (97/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 111.0 in stage 2.0 (TID 104, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 2.0 (TID 101). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 2.0 (TID 98) in 51 ms on localhost (executor driver) (98/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 111.0 in stage 2.0 (TID 104)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 112.0 in stage 2.0 (TID 105, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 2.0 (TID 102). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 2.0 (TID 103). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 113.0 in stage 2.0 (TID 106, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 114.0 in stage 2.0 (TID 107, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 113.0 in stage 2.0 (TID 106)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 114.0 in stage 2.0 (TID 107)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 2.0 (TID 100) in 43 ms on localhost (executor driver) (99/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 112.0 in stage 2.0 (TID 105)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 2.0 (TID 103) in 30 ms on localhost (executor driver) (100/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 2.0 (TID 102) in 33 ms on localhost (executor driver) (101/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 2.0 (TID 101) in 41 ms on localhost (executor driver) (102/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 2.0 (TID 104). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 115.0 in stage 2.0 (TID 108, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 2.0 (TID 104) in 32 ms on localhost (executor driver) (103/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 115.0 in stage 2.0 (TID 108)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 2.0 (TID 105). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 117.0 in stage 2.0 (TID 109, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 2.0 (TID 106). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 2.0 (TID 105) in 28 ms on localhost (executor driver) (104/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 117.0 in stage 2.0 (TID 109)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 119.0 in stage 2.0 (TID 110, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 2.0 (TID 106) in 24 ms on localhost (executor driver) (105/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 119.0 in stage 2.0 (TID 110)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 2.0 (TID 107). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 120.0 in stage 2.0 (TID 111, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 120.0 in stage 2.0 (TID 111)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 2.0 (TID 107) in 27 ms on localhost (executor driver) (106/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 2.0 (TID 109). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 121.0 in stage 2.0 (TID 112, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 2.0 (TID 109) in 15 ms on localhost (executor driver) (107/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 121.0 in stage 2.0 (TID 112)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 2.0 (TID 110). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 122.0 in stage 2.0 (TID 113, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 122.0 in stage 2.0 (TID 113)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 2.0 (TID 110) in 16 ms on localhost (executor driver) (108/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 2.0 (TID 111). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 123.0 in stage 2.0 (TID 114, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 2.0 (TID 113). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 124.0 in stage 2.0 (TID 115, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 124.0 in stage 2.0 (TID 115)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 123.0 in stage 2.0 (TID 114)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 2.0 (TID 113) in 12 ms on localhost (executor driver) (109/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 2.0 (TID 112). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 2.0 (TID 111) in 26 ms on localhost (executor driver) (110/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,983][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 125.0 in stage 2.0 (TID 116, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 125.0 in stage 2.0 (TID 116)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 2.0 (TID 112) in 24 ms on localhost (executor driver) (111/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 2.0 (TID 108). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,994][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 2.0 (TID 115). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 2.0 (TID 114). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 126.0 in stage 2.0 (TID 117, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 127.0 in stage 2.0 (TID 118, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 128.0 in stage 2.0 (TID 119, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:36,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 2.0 (TID 116). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 127.0 in stage 2.0 (TID 118)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 128.0 in stage 2.0 (TID 119)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 130.0 in stage 2.0 (TID 120, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 2.0 (TID 108) in 56 ms on localhost (executor driver) (112/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 130.0 in stage 2.0 (TID 120)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 2.0 (TID 115) in 25 ms on localhost (executor driver) (113/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 2.0 (TID 114) in 30 ms on localhost (executor driver) (114/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 2.0 (TID 116) in 21 ms on localhost (executor driver) (115/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 126.0 in stage 2.0 (TID 117)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 2.0 (TID 118). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 2.0 (TID 120). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 131.0 in stage 2.0 (TID 121, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 2.0 (TID 119). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 132.0 in stage 2.0 (TID 122, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 131.0 in stage 2.0 (TID 121)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 133.0 in stage 2.0 (TID 123, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 2.0 (TID 118) in 23 ms on localhost (executor driver) (116/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 2.0 (TID 117). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 134.0 in stage 2.0 (TID 124, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 2.0 (TID 120) in 21 ms on localhost (executor driver) (117/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 132.0 in stage 2.0 (TID 122)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 2.0 (TID 119) in 24 ms on localhost (executor driver) (118/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 134.0 in stage 2.0 (TID 124)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 2.0 (TID 117) in 30 ms on localhost (executor driver) (119/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 133.0 in stage 2.0 (TID 123)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 2.0 (TID 121). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 135.0 in stage 2.0 (TID 125, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 135.0 in stage 2.0 (TID 125)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 2.0 (TID 121) in 12 ms on localhost (executor driver) (120/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 2.0 (TID 122). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 136.0 in stage 2.0 (TID 126, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 2.0 (TID 122) in 14 ms on localhost (executor driver) (121/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 2.0 (TID 123). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 136.0 in stage 2.0 (TID 126)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 2.0 (TID 124). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 137.0 in stage 2.0 (TID 127, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 138.0 in stage 2.0 (TID 128, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 137.0 in stage 2.0 (TID 127)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 2.0 (TID 124) in 14 ms on localhost (executor driver) (122/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 138.0 in stage 2.0 (TID 128)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 2.0 (TID 123) in 18 ms on localhost (executor driver) (123/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 2.0 (TID 125). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 139.0 in stage 2.0 (TID 129, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 2.0 (TID 125) in 12 ms on localhost (executor driver) (124/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 139.0 in stage 2.0 (TID 129)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 2.0 (TID 126). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 140.0 in stage 2.0 (TID 130, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 2.0 (TID 126) in 15 ms on localhost (executor driver) (125/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 2.0 (TID 127). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 140.0 in stage 2.0 (TID 130)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 141.0 in stage 2.0 (TID 131, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 2.0 (TID 128). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 141.0 in stage 2.0 (TID 131)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 142.0 in stage 2.0 (TID 132, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 142.0 in stage 2.0 (TID 132)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 2.0 (TID 127) in 16 ms on localhost (executor driver) (126/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 2.0 (TID 128) in 17 ms on localhost (executor driver) (127/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 2.0 (TID 129). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,051][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,051][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 143.0 in stage 2.0 (TID 133, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,051][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 143.0 in stage 2.0 (TID 133)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 2.0 (TID 129) in 15 ms on localhost (executor driver) (128/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 2.0 (TID 131). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 144.0 in stage 2.0 (TID 134, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 144.0 in stage 2.0 (TID 134)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 2.0 (TID 131) in 10 ms on localhost (executor driver) (129/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 2.0 (TID 132). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 145.0 in stage 2.0 (TID 135, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 2.0 (TID 130). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 145.0 in stage 2.0 (TID 135)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 146.0 in stage 2.0 (TID 136, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,058][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 2.0 (TID 132) in 13 ms on localhost (executor driver) (130/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 146.0 in stage 2.0 (TID 136)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 2.0 (TID 133). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 2.0 (TID 130) in 18 ms on localhost (executor driver) (131/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 2.0 (TID 134). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 147.0 in stage 2.0 (TID 137, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 148.0 in stage 2.0 (TID 138, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 147.0 in stage 2.0 (TID 137)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 2.0 (TID 133) in 16 ms on localhost (executor driver) (132/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 148.0 in stage 2.0 (TID 138)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 2.0 (TID 135). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 2.0 (TID 134) in 15 ms on localhost (executor driver) (133/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 149.0 in stage 2.0 (TID 139, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 149.0 in stage 2.0 (TID 139)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 2.0 (TID 135) in 17 ms on localhost (executor driver) (134/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,075][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 2.0 (TID 137). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,076][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 14 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 150.0 in stage 2.0 (TID 140, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 2.0 (TID 136). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 14 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 150.0 in stage 2.0 (TID 140)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 151.0 in stage 2.0 (TID 141, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 2.0 (TID 137) in 33 ms on localhost (executor driver) (135/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 2.0 (TID 136) in 42 ms on localhost (executor driver) (136/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 2.0 (TID 139). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 151.0 in stage 2.0 (TID 141)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 152.0 in stage 2.0 (TID 142, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 152.0 in stage 2.0 (TID 142)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 2.0 (TID 139) in 30 ms on localhost (executor driver) (137/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 2.0 (TID 140). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 153.0 in stage 2.0 (TID 143, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 2.0 (TID 140) in 16 ms on localhost (executor driver) (138/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 153.0 in stage 2.0 (TID 143)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 2.0 (TID 142). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 2.0 (TID 138). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 154.0 in stage 2.0 (TID 144, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 2.0 (TID 141). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 155.0 in stage 2.0 (TID 145, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 154.0 in stage 2.0 (TID 144)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 156.0 in stage 2.0 (TID 146, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 155.0 in stage 2.0 (TID 145)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 2.0 (TID 142) in 22 ms on localhost (executor driver) (139/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 2.0 (TID 143). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 156.0 in stage 2.0 (TID 146)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 2.0 (TID 141) in 28 ms on localhost (executor driver) (140/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 157.0 in stage 2.0 (TID 147, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 2.0 (TID 138) in 62 ms on localhost (executor driver) (141/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 2.0 (TID 144). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 157.0 in stage 2.0 (TID 147)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 2.0 (TID 143) in 24 ms on localhost (executor driver) (142/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 2.0 (TID 145). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 158.0 in stage 2.0 (TID 148, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 160.0 in stage 2.0 (TID 149, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 2.0 (TID 146). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 158.0 in stage 2.0 (TID 148)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 161.0 in stage 2.0 (TID 150, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 160.0 in stage 2.0 (TID 149)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 2.0 (TID 144) in 23 ms on localhost (executor driver) (143/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 161.0 in stage 2.0 (TID 150)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 2.0 (TID 145) in 22 ms on localhost (executor driver) (144/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 2.0 (TID 146) in 23 ms on localhost (executor driver) (145/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 2.0 (TID 148). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 162.0 in stage 2.0 (TID 151, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 2.0 (TID 149). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 2.0 (TID 148) in 13 ms on localhost (executor driver) (146/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 162.0 in stage 2.0 (TID 151)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 163.0 in stage 2.0 (TID 152, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 2.0 (TID 150). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 163.0 in stage 2.0 (TID 152)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 164.0 in stage 2.0 (TID 153, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 2.0 (TID 149) in 12 ms on localhost (executor driver) (147/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 164.0 in stage 2.0 (TID 153)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 2.0 (TID 150) in 12 ms on localhost (executor driver) (148/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 2.0 (TID 147). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 165.0 in stage 2.0 (TID 154, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 2.0 (TID 151). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 166.0 in stage 2.0 (TID 155, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 2.0 (TID 151) in 11 ms on localhost (executor driver) (149/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 2.0 (TID 147) in 27 ms on localhost (executor driver) (150/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 165.0 in stage 2.0 (TID 154)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 166.0 in stage 2.0 (TID 155)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 2.0 (TID 152). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 2.0 (TID 153). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 167.0 in stage 2.0 (TID 156, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,161][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 2.0 (TID 154). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,162][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 168.0 in stage 2.0 (TID 157, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 169.0 in stage 2.0 (TID 158, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 2.0 (TID 155). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 168.0 in stage 2.0 (TID 157)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 2.0 (TID 152) in 20 ms on localhost (executor driver) (151/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 169.0 in stage 2.0 (TID 158)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 2.0 (TID 154) in 16 ms on localhost (executor driver) (152/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 170.0 in stage 2.0 (TID 159, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 2.0 (TID 153) in 23 ms on localhost (executor driver) (153/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 170.0 in stage 2.0 (TID 159)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 2.0 (TID 155) in 17 ms on localhost (executor driver) (154/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,172][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 2.0 (TID 157). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 2.0 (TID 158). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 171.0 in stage 2.0 (TID 160, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 172.0 in stage 2.0 (TID 161, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 2.0 (TID 159). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 2.0 (TID 157) in 14 ms on localhost (executor driver) (155/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 171.0 in stage 2.0 (TID 160)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 172.0 in stage 2.0 (TID 161)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 173.0 in stage 2.0 (TID 162, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,180][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 2.0 (TID 158) in 18 ms on localhost (executor driver) (156/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 2.0 (TID 159) in 14 ms on localhost (executor driver) (157/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 173.0 in stage 2.0 (TID 162)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,184][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 2.0 (TID 160). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 174.0 in stage 2.0 (TID 163, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 2.0 (TID 161). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 2.0 (TID 160) in 13 ms on localhost (executor driver) (158/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 174.0 in stage 2.0 (TID 163)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 175.0 in stage 2.0 (TID 164, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 175.0 in stage 2.0 (TID 164)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 2.0 (TID 161) in 16 ms on localhost (executor driver) (159/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 2.0 (TID 162). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 176.0 in stage 2.0 (TID 165, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 2.0 (TID 162) in 21 ms on localhost (executor driver) (160/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 176.0 in stage 2.0 (TID 165)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 2.0 (TID 163). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 177.0 in stage 2.0 (TID 166, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 177.0 in stage 2.0 (TID 166)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 2.0 (TID 163) in 43 ms on localhost (executor driver) (161/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 2.0 (TID 164). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 178.0 in stage 2.0 (TID 167, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 2.0 (TID 164) in 53 ms on localhost (executor driver) (162/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 178.0 in stage 2.0 (TID 167)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 167.0 in stage 2.0 (TID 156)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 2.0 (TID 166). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,259][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 179.0 in stage 2.0 (TID 168, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 179.0 in stage 2.0 (TID 168)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 2.0 (TID 166) in 63 ms on localhost (executor driver) (163/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 2.0 (TID 167). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 180.0 in stage 2.0 (TID 169, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 180.0 in stage 2.0 (TID 169)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 2.0 (TID 167) in 30 ms on localhost (executor driver) (164/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 2.0 (TID 156). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 181.0 in stage 2.0 (TID 170, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 181.0 in stage 2.0 (TID 170)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,281][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 2.0 (TID 156) in 121 ms on localhost (executor driver) (165/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 2.0 (TID 170). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 182.0 in stage 2.0 (TID 171, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 182.0 in stage 2.0 (TID 171)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 2.0 (TID 170) in 18 ms on localhost (executor driver) (166/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 2.0 (TID 165). 3968 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 15 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 183.0 in stage 2.0 (TID 172, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 2.0 (TID 165) in 122 ms on localhost (executor driver) (167/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 183.0 in stage 2.0 (TID 172)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 2.0 (TID 171). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 184.0 in stage 2.0 (TID 173, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 184.0 in stage 2.0 (TID 173)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 2.0 (TID 171) in 30 ms on localhost (executor driver) (168/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 2.0 (TID 169). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 185.0 in stage 2.0 (TID 174, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 2.0 (TID 169) in 81 ms on localhost (executor driver) (169/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 2.0 (TID 168). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 185.0 in stage 2.0 (TID 174)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 186.0 in stage 2.0 (TID 175, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 2.0 (TID 168) in 95 ms on localhost (executor driver) (170/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 2.0 (TID 172). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 187.0 in stage 2.0 (TID 176, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 186.0 in stage 2.0 (TID 175)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 2.0 (TID 172) in 44 ms on localhost (executor driver) (171/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 187.0 in stage 2.0 (TID 176)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 2.0 (TID 173). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 188.0 in stage 2.0 (TID 177, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 188.0 in stage 2.0 (TID 177)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 2.0 (TID 173) in 63 ms on localhost (executor driver) (172/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,384][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,384][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 2.0 (TID 175). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 189.0 in stage 2.0 (TID 178, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 2.0 (TID 175) in 36 ms on localhost (executor driver) (173/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 189.0 in stage 2.0 (TID 178)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 2.0 (TID 177). 3925 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 191.0 in stage 2.0 (TID 179, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 2.0 (TID 174). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 2.0 (TID 177) in 19 ms on localhost (executor driver) (174/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 193.0 in stage 2.0 (TID 180, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 2.0 (TID 174) in 65 ms on localhost (executor driver) (175/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 193.0 in stage 2.0 (TID 180)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 2.0 (TID 176). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 194.0 in stage 2.0 (TID 181, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 194.0 in stage 2.0 (TID 181)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 2.0 (TID 176) in 54 ms on localhost (executor driver) (176/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 2.0 (TID 178). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 2.0 (TID 180). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 195.0 in stage 2.0 (TID 182, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 196.0 in stage 2.0 (TID 183, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 2.0 (TID 178) in 33 ms on localhost (executor driver) (177/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 2.0 (TID 181). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 195.0 in stage 2.0 (TID 182)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 197.0 in stage 2.0 (TID 184, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,422][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 2.0 (TID 180) in 21 ms on localhost (executor driver) (178/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,423][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 2.0 (TID 181) in 15 ms on localhost (executor driver) (179/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 197.0 in stage 2.0 (TID 184)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 191.0 in stage 2.0 (TID 179)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,432][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 2.0 (TID 182). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 196.0 in stage 2.0 (TID 183)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 198.0 in stage 2.0 (TID 185, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 8 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 2.0 (TID 182) in 33 ms on localhost (executor driver) (180/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 2.0 (TID 179). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 199.0 in stage 2.0 (TID 186, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 199.0 in stage 2.0 (TID 186)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 2.0 (TID 179) in 64 ms on localhost (executor driver) (181/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 2.0 (TID 183). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 187, localhost, executor driver, partition 0, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 187)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 2.0 (TID 183) in 48 ms on localhost (executor driver) (182/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 2.0 (TID 184). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 19.0 in stage 2.0 (TID 188, localhost, executor driver, partition 19, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 19.0 in stage 2.0 (TID 188)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 2.0 (TID 184) in 56 ms on localhost (executor driver) (183/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,478][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 198.0 in stage 2.0 (TID 185)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,490][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 2.0 (TID 186). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,493][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 30.0 in stage 2.0 (TID 189, localhost, executor driver, partition 30, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 2.0 (TID 186) in 39 ms on localhost (executor driver) (184/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 2.0 (TID 185). 3882 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,500][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 46.0 in stage 2.0 (TID 190, localhost, executor driver, partition 46, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,501][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 2.0 (TID 185) in 52 ms on localhost (executor driver) (185/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,501][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 30.0 in stage 2.0 (TID 189)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 46.0 in stage 2.0 (TID 190)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 2.0 (TID 188). 4108 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 2.0 (TID 190). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 2.0 (TID 189). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 187). 4108 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 56.0 in stage 2.0 (TID 191, localhost, executor driver, partition 56, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 66.0 in stage 2.0 (TID 192, localhost, executor driver, partition 66, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 67.0 in stage 2.0 (TID 193, localhost, executor driver, partition 67, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 72.0 in stage 2.0 (TID 194, localhost, executor driver, partition 72, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 187) in 55 ms on localhost (executor driver) (186/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 72.0 in stage 2.0 (TID 194)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 2.0 (TID 188) in 48 ms on localhost (executor driver) (187/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 2.0 (TID 190) in 22 ms on localhost (executor driver) (188/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 2.0 (TID 189) in 30 ms on localhost (executor driver) (189/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 66.0 in stage 2.0 (TID 192)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 67.0 in stage 2.0 (TID 193)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 2.0 (TID 194). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 108.0 in stage 2.0 (TID 195, localhost, executor driver, partition 108, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 108.0 in stage 2.0 (TID 195)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 2.0 (TID 193). 4108 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 2.0 (TID 194) in 11 ms on localhost (executor driver) (190/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 116.0 in stage 2.0 (TID 196, localhost, executor driver, partition 116, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 2.0 (TID 192). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 2.0 (TID 193) in 14 ms on localhost (executor driver) (191/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 116.0 in stage 2.0 (TID 196)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 118.0 in stage 2.0 (TID 197, localhost, executor driver, partition 118, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 2.0 (TID 192) in 19 ms on localhost (executor driver) (192/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 2.0 (TID 195). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 118.0 in stage 2.0 (TID 197)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 129.0 in stage 2.0 (TID 198, localhost, executor driver, partition 129, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 129.0 in stage 2.0 (TID 198)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 2.0 (TID 195) in 15 ms on localhost (executor driver) (193/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 2.0 (TID 197). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 159.0 in stage 2.0 (TID 199, localhost, executor driver, partition 159, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 159.0 in stage 2.0 (TID 199)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 2.0 (TID 197) in 17 ms on localhost (executor driver) (194/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 2.0 (TID 198). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 190.0 in stage 2.0 (TID 200, localhost, executor driver, partition 190, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,553][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 190.0 in stage 2.0 (TID 200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,553][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 2.0 (TID 198) in 12 ms on localhost (executor driver) (195/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,559][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 2.0 (TID 199). 4108 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 192.0 in stage 2.0 (TID 201, localhost, executor driver, partition 192, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 2.0 (TID 200). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 2.0 (TID 199) in 11 ms on localhost (executor driver) (196/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 192.0 in stage 2.0 (TID 201)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 56.0 in stage 2.0 (TID 191)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 2.0 (TID 200) in 10 ms on localhost (executor driver) (197/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 2.0 (TID 196). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 2.0 (TID 196) in 33 ms on localhost (executor driver) (198/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 2.0 (TID 191). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 2.0 (TID 191) in 54 ms on localhost (executor driver) (199/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 2.0 (TID 201). 4065 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,582][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 2.0 (TID 201) in 23 ms on localhost (executor driver) (200/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 2 (show at SparkSQL.scala:55) finished in 2.054 s
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 1 finished: show at SparkSQL.scala:55, took 2.465072 s
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 9.612763 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,608][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Invoking stop() from shutdown hook
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.216.37:4040
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2017-11-03 08:02:37,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/spark-4f03bf21-b935-4d1b-b451-28be4a779c85
[34m[INFO ][0;39m [35m[2017-11-03 08:03:12,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running Spark version 2.2.0
[31m[WARN ][0;39m [35m[2017-11-03 08:03:12,801][0;39m [33m[][0;39m [35m[org.apache.hadoop.util.NativeCodeLoader-><clinit>][0;39m | Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitted application: Spark Structured Streaming Job
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing view acls to: flavio.clesio
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing modify acls to: flavio.clesio
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing view acls groups to: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing modify acls groups to: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(flavio.clesio); groups with view permissions: Set(); users  with modify permissions: Set(flavio.clesio); groups with modify permissions: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 63890.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManagerMasterEndpoint up
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/blockmgr-6c26187b-07bc-42e3-a04e-085725dd62d1
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MemoryStore started with capacity 912.3 MB
[34m[INFO ][0;39m [35m[2017-11-03 08:03:13,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Bound SparkUI to 0.0.0.0, and started at http://192.168.216.37:4040
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63892.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Server created on 192.168.216.37:63892
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering BlockManager BlockManagerId(driver, 192.168.216.37, 63892, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering block manager 192.168.216.37:63892 with 912.3 MB RAM, BlockManagerId(driver, 192.168.216.37, 63892, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registered BlockManager BlockManagerId(driver, 192.168.216.37, 63892, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:14,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Initialized BlockManager: BlockManagerId(driver, 192.168.216.37, 63892, None)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:15,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/spark-warehouse/').
[34m[INFO ][0;39m [35m[2017-11-03 08:03:15,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Warehouse path is 'file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/spark-warehouse/'.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:16,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registered StateStoreCoordinator endpoint
[34m[INFO ][0;39m [35m[2017-11-03 08:03:19,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parsing command: user_records
[34m[INFO ][0;39m [35m[2017-11-03 08:03:19,490][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parsing command: 
        SELECT carrier, marital_status, COUNT(1) as num_users
        FROM user_records
        GROUP BY carrier, marital_status
      
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting [id = 7d2a611d-52e5-41ef-bc4b-86e8687960cf, runId = c65b161d-c88c-47f1-9c92-568c90dbff87]. Use /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80 to store the query checkpoint.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Set the compact interval to 10 [defaultCompactInterval: 10]
[31m[WARN ][0;39m [35m[2017-11-03 08:03:20,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | 'latestFirst' is true. New files will be processed first, which may affect the watermark
value. In addition, 'maxFileAge' will be ignored.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | maxFilesPerBatch = None, maxFileAgeMs = 604800000
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting new streaming query.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,162][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Log offset set to 0 with 1 new files
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1509703400165,Map(spark.sql.shuffle.partitions -> 200))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Processing 1 files from 0:0
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<carrier: string, marital_status: string>
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:20,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 0
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 280.637624 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 68.148417 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 77.345612 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 221.7 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 0 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:21,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 220.5 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 1 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 2 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering RDD 2 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 0 (start at StreamingFile.scala:61) with 200 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 0)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 0)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 26.9 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.1 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on 192.168.216.37:63892 (size: 13.1 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,384][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 25.643443 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 10.996917 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 16.994443 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,705][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 15.2882 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,752][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 11.125596 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.1.csv, range: 0-6730, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2307 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 504 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ShuffleMapStage 0 (start at StreamingFile.scala:61) finished in 0.529 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 1)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:22,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[9] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 52.3 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,077][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.9 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,078][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on 192.168.216.37:63892 (size: 21.9 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,079][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,083][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,083][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 1.0 with 200 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 4.0 in stage 1.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 4.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | State Store maintenance task started
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_3_piece0 on 192.168.216.37:63892 in memory (size: 13.1 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 1.0 (TID 4). 3781 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 3). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 2). 3781 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 5.0 in stage 1.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 5.0 in stage 1.0 (TID 5)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 6.0 in stage 1.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 1.0 (TID 4) in 231 ms on localhost (executor driver) (1/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 6.0 in stage 1.0 (TID 6)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 7.0 in stage 1.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 7.0 in stage 1.0 (TID 7)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 3) in 241 ms on localhost (executor driver) (2/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 2) in 242 ms on localhost (executor driver) (3/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,337][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 1). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 8.0 in stage 1.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 1) in 268 ms on localhost (executor driver) (4/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 8.0 in stage 1.0 (TID 8)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,384][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 1.0 (TID 7). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 9.0 in stage 1.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 1.0 (TID 7) in 205 ms on localhost (executor driver) (5/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,542][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 1.0 (TID 6). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 10.0 in stage 1.0 (TID 10, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 10.0 in stage 1.0 (TID 10)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 1.0 (TID 6) in 230 ms on localhost (executor driver) (6/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 9.0 in stage 1.0 (TID 9)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,582][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 1.0 (TID 5). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 11.0 in stage 1.0 (TID 11, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 11.0 in stage 1.0 (TID 11)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 1.0 (TID 5) in 267 ms on localhost (executor driver) (7/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 1.0 (TID 8). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 12.0 in stage 1.0 (TID 12, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 1.0 (TID 8) in 273 ms on localhost (executor driver) (8/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 12.0 in stage 1.0 (TID 12)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,631][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,665][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 1.0 (TID 9). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 13.0 in stage 1.0 (TID 13, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 1.0 (TID 9) in 142 ms on localhost (executor driver) (9/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 13.0 in stage 1.0 (TID 13)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,677][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 1.0 (TID 11). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 14.0 in stage 1.0 (TID 14, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 1.0 (TID 11) in 103 ms on localhost (executor driver) (10/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 14.0 in stage 1.0 (TID 14)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 1.0 (TID 10). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 1.0 (TID 12). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 15.0 in stage 1.0 (TID 15, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 16.0 in stage 1.0 (TID 16, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 1.0 (TID 10) in 179 ms on localhost (executor driver) (11/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 16.0 in stage 1.0 (TID 16)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 15.0 in stage 1.0 (TID 15)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 1.0 (TID 12) in 102 ms on localhost (executor driver) (12/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,789][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 1.0 (TID 14). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 17.0 in stage 1.0 (TID 17, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 1.0 (TID 14) in 103 ms on localhost (executor driver) (13/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 17.0 in stage 1.0 (TID 17)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,797][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 1.0 (TID 15). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 18.0 in stage 1.0 (TID 18, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 1.0 (TID 15) in 94 ms on localhost (executor driver) (14/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 18.0 in stage 1.0 (TID 18)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 1.0 (TID 13). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,822][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 20.0 in stage 1.0 (TID 19, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 1.0 (TID 13) in 158 ms on localhost (executor driver) (15/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 20.0 in stage 1.0 (TID 19)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 1.0 (TID 16). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 21.0 in stage 1.0 (TID 20, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 21.0 in stage 1.0 (TID 20)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 1.0 (TID 16) in 104 ms on localhost (executor driver) (16/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 1.0 (TID 17). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 22.0 in stage 1.0 (TID 21, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 1.0 (TID 17) in 146 ms on localhost (executor driver) (17/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 22.0 in stage 1.0 (TID 21)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 1.0 (TID 19). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 23.0 in stage 1.0 (TID 22, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 1.0 (TID 19) in 151 ms on localhost (executor driver) (18/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 23.0 in stage 1.0 (TID 22)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 1.0 (TID 20). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 24.0 in stage 1.0 (TID 23, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 1.0 (TID 20) in 154 ms on localhost (executor driver) (19/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 24.0 in stage 1.0 (TID 23)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 1.0 (TID 18). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 25.0 in stage 1.0 (TID 24, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 1.0 (TID 18) in 169 ms on localhost (executor driver) (20/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,994][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 25.0 in stage 1.0 (TID 24)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:23,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 1.0 (TID 21). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 26.0 in stage 1.0 (TID 25, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 1.0 (TID 21) in 112 ms on localhost (executor driver) (21/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 26.0 in stage 1.0 (TID 25)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,058][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 1.0 (TID 23). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 27.0 in stage 1.0 (TID 26, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 1.0 (TID 23) in 76 ms on localhost (executor driver) (22/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 27.0 in stage 1.0 (TID 26)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 1.0 (TID 24). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 28.0 in stage 1.0 (TID 27, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 1.0 (TID 24) in 87 ms on localhost (executor driver) (23/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,075][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 28.0 in stage 1.0 (TID 27)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 1.0 (TID 25). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 29.0 in stage 1.0 (TID 28, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 1.0 (TID 25) in 63 ms on localhost (executor driver) (24/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 29.0 in stage 1.0 (TID 28)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 1.0 (TID 22). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 31.0 in stage 1.0 (TID 29, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 1.0 (TID 22) in 144 ms on localhost (executor driver) (25/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 31.0 in stage 1.0 (TID 29)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 1.0 (TID 27). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 32.0 in stage 1.0 (TID 30, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 32.0 in stage 1.0 (TID 30)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 1.0 (TID 27) in 85 ms on localhost (executor driver) (26/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 1.0 (TID 28). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 33.0 in stage 1.0 (TID 31, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 1.0 (TID 28) in 68 ms on localhost (executor driver) (27/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 33.0 in stage 1.0 (TID 31)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,183][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,183][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 1.0 (TID 29). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 34.0 in stage 1.0 (TID 32, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 1.0 (TID 29) in 73 ms on localhost (executor driver) (28/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 34.0 in stage 1.0 (TID 32)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 1.0 (TID 26). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 35.0 in stage 1.0 (TID 33, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 1.0 (TID 26) in 132 ms on localhost (executor driver) (29/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 35.0 in stage 1.0 (TID 33)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 1.0 (TID 31). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 36.0 in stage 1.0 (TID 34, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 1.0 (TID 31) in 62 ms on localhost (executor driver) (30/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 36.0 in stage 1.0 (TID 34)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 1.0 (TID 32). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 37.0 in stage 1.0 (TID 35, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 1.0 (TID 32) in 80 ms on localhost (executor driver) (31/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 37.0 in stage 1.0 (TID 35)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 1.0 (TID 33). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 38.0 in stage 1.0 (TID 36, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 38.0 in stage 1.0 (TID 36)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 1.0 (TID 33) in 97 ms on localhost (executor driver) (32/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 1.0 (TID 34). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 39.0 in stage 1.0 (TID 37, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 1.0 (TID 30). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 1.0 (TID 34) in 65 ms on localhost (executor driver) (33/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 39.0 in stage 1.0 (TID 37)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 40.0 in stage 1.0 (TID 38, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 1.0 (TID 30) in 144 ms on localhost (executor driver) (34/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 40.0 in stage 1.0 (TID 38)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,306][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,310][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 1.0 (TID 35). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 1.0 (TID 36). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 41.0 in stage 1.0 (TID 39, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 42.0 in stage 1.0 (TID 40, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 41.0 in stage 1.0 (TID 39)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 1.0 (TID 35) in 89 ms on localhost (executor driver) (35/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 42.0 in stage 1.0 (TID 40)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 1.0 (TID 36) in 69 ms on localhost (executor driver) (36/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 1.0 (TID 37). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 43.0 in stage 1.0 (TID 41, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 1.0 (TID 37) in 70 ms on localhost (executor driver) (37/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 43.0 in stage 1.0 (TID 41)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 1.0 (TID 38). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 44.0 in stage 1.0 (TID 42, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 1.0 (TID 38) in 71 ms on localhost (executor driver) (38/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 44.0 in stage 1.0 (TID 42)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 1.0 (TID 40). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 45.0 in stage 1.0 (TID 43, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 45.0 in stage 1.0 (TID 43)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 1.0 (TID 40) in 63 ms on localhost (executor driver) (39/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,423][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 1.0 (TID 41). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 47.0 in stage 1.0 (TID 44, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 1.0 (TID 41) in 60 ms on localhost (executor driver) (40/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 47.0 in stage 1.0 (TID 44)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,434][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,435][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,435][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,436][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 1.0 (TID 42). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 48.0 in stage 1.0 (TID 45, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 1.0 (TID 42) in 73 ms on localhost (executor driver) (41/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 48.0 in stage 1.0 (TID 45)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 1.0 (TID 39). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 49.0 in stage 1.0 (TID 46, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 1.0 (TID 39) in 100 ms on localhost (executor driver) (42/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 49.0 in stage 1.0 (TID 46)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,504][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,506][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 1.0 (TID 45). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 50.0 in stage 1.0 (TID 47, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 1.0 (TID 44). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 1.0 (TID 45) in 100 ms on localhost (executor driver) (43/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 50.0 in stage 1.0 (TID 47)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 51.0 in stage 1.0 (TID 48, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 1.0 (TID 44) in 118 ms on localhost (executor driver) (44/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,547][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 51.0 in stage 1.0 (TID 48)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,553][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,553][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 1.0 (TID 43). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 52.0 in stage 1.0 (TID 49, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 1.0 (TID 43) in 144 ms on localhost (executor driver) (45/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 52.0 in stage 1.0 (TID 49)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 1.0 (TID 46). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 53.0 in stage 1.0 (TID 50, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 1.0 (TID 46) in 131 ms on localhost (executor driver) (46/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 53.0 in stage 1.0 (TID 50)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 1.0 (TID 48). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 54.0 in stage 1.0 (TID 51, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 54.0 in stage 1.0 (TID 51)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 1.0 (TID 48) in 96 ms on localhost (executor driver) (47/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 1.0 (TID 49). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 55.0 in stage 1.0 (TID 52, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 1.0 (TID 49) in 82 ms on localhost (executor driver) (48/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 55.0 in stage 1.0 (TID 52)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,665][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 1.0 (TID 50). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,667][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 11 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 57.0 in stage 1.0 (TID 53, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 1.0 (TID 47). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 1.0 (TID 50) in 92 ms on localhost (executor driver) (49/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 57.0 in stage 1.0 (TID 53)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,677][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 58.0 in stage 1.0 (TID 54, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,678][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 1.0 (TID 47) in 134 ms on localhost (executor driver) (50/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,678][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 58.0 in stage 1.0 (TID 54)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 1.0 (TID 51). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 1.0 (TID 54). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 59.0 in stage 1.0 (TID 55, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 1.0 (TID 51) in 92 ms on localhost (executor driver) (51/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 59.0 in stage 1.0 (TID 55)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 60.0 in stage 1.0 (TID 56, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 1.0 (TID 54) in 56 ms on localhost (executor driver) (52/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 60.0 in stage 1.0 (TID 56)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 1.0 (TID 52). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 61.0 in stage 1.0 (TID 57, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 1.0 (TID 52) in 106 ms on localhost (executor driver) (53/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 61.0 in stage 1.0 (TID 57)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 1.0 (TID 53). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 62.0 in stage 1.0 (TID 58, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 1.0 (TID 53) in 92 ms on localhost (executor driver) (54/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,767][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 62.0 in stage 1.0 (TID 58)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 1.0 (TID 56). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 63.0 in stage 1.0 (TID 59, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 1.0 (TID 56) in 71 ms on localhost (executor driver) (55/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 63.0 in stage 1.0 (TID 59)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,822][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 1.0 (TID 57). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 64.0 in stage 1.0 (TID 60, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 1.0 (TID 57) in 77 ms on localhost (executor driver) (56/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 64.0 in stage 1.0 (TID 60)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 1.0 (TID 55). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 65.0 in stage 1.0 (TID 61, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 65.0 in stage 1.0 (TID 61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 1.0 (TID 55) in 107 ms on localhost (executor driver) (57/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 1.0 (TID 58). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 68.0 in stage 1.0 (TID 62, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 68.0 in stage 1.0 (TID 62)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 1.0 (TID 58) in 84 ms on localhost (executor driver) (58/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 1.0 (TID 59). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 69.0 in stage 1.0 (TID 63, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 69.0 in stage 1.0 (TID 63)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 1.0 (TID 59) in 75 ms on localhost (executor driver) (59/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 1.0 (TID 60). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 70.0 in stage 1.0 (TID 64, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,896][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 70.0 in stage 1.0 (TID 64)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,896][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 1.0 (TID 60) in 72 ms on localhost (executor driver) (60/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 1.0 (TID 61). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 71.0 in stage 1.0 (TID 65, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 1.0 (TID 61) in 63 ms on localhost (executor driver) (61/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 71.0 in stage 1.0 (TID 65)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 13 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 1.0 (TID 62). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 73.0 in stage 1.0 (TID 66, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 1.0 (TID 62) in 81 ms on localhost (executor driver) (62/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 73.0 in stage 1.0 (TID 66)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 1.0 (TID 63). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 74.0 in stage 1.0 (TID 67, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 1.0 (TID 64). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 74.0 in stage 1.0 (TID 67)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 1.0 (TID 63) in 76 ms on localhost (executor driver) (63/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 75.0 in stage 1.0 (TID 68, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 1.0 (TID 64) in 62 ms on localhost (executor driver) (64/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 75.0 in stage 1.0 (TID 68)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:24,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 1.0 (TID 65). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 1.0 (TID 66). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 76.0 in stage 1.0 (TID 69, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 76.0 in stage 1.0 (TID 69)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 77.0 in stage 1.0 (TID 70, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 1.0 (TID 65) in 120 ms on localhost (executor driver) (65/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 77.0 in stage 1.0 (TID 70)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 1.0 (TID 66) in 90 ms on localhost (executor driver) (66/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,083][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 1.0 (TID 67). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 78.0 in stage 1.0 (TID 71, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 78.0 in stage 1.0 (TID 71)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 1.0 (TID 67) in 131 ms on localhost (executor driver) (67/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 1.0 (TID 68). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 79.0 in stage 1.0 (TID 72, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 1.0 (TID 68) in 136 ms on localhost (executor driver) (68/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 79.0 in stage 1.0 (TID 72)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 1.0 (TID 70). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 80.0 in stage 1.0 (TID 73, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 80.0 in stage 1.0 (TID 73)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 1.0 (TID 70) in 101 ms on localhost (executor driver) (69/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 1.0 (TID 69). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 81.0 in stage 1.0 (TID 74, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 1.0 (TID 69) in 111 ms on localhost (executor driver) (70/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 81.0 in stage 1.0 (TID 74)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 1.0 (TID 71). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 82.0 in stage 1.0 (TID 75, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 82.0 in stage 1.0 (TID 75)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 1.0 (TID 71) in 59 ms on localhost (executor driver) (71/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 1.0 (TID 72). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 83.0 in stage 1.0 (TID 76, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 83.0 in stage 1.0 (TID 76)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 1.0 (TID 72) in 64 ms on localhost (executor driver) (72/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,161][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,161][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,162][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 1.0 (TID 73). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 1.0 (TID 74). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 84.0 in stage 1.0 (TID 77, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 84.0 in stage 1.0 (TID 77)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 85.0 in stage 1.0 (TID 78, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 1.0 (TID 73) in 72 ms on localhost (executor driver) (73/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 1.0 (TID 74) in 63 ms on localhost (executor driver) (74/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 85.0 in stage 1.0 (TID 78)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,206][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 1.0 (TID 75). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 86.0 in stage 1.0 (TID 79, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 1.0 (TID 75) in 82 ms on localhost (executor driver) (75/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,226][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 86.0 in stage 1.0 (TID 79)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,226][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 1.0 (TID 76). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 87.0 in stage 1.0 (TID 80, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 1.0 (TID 76) in 73 ms on localhost (executor driver) (76/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 87.0 in stage 1.0 (TID 80)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 1.0 (TID 77). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 88.0 in stage 1.0 (TID 81, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 1.0 (TID 77) in 60 ms on localhost (executor driver) (77/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 88.0 in stage 1.0 (TID 81)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 1.0 (TID 78). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 89.0 in stage 1.0 (TID 82, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 1.0 (TID 78) in 62 ms on localhost (executor driver) (78/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 89.0 in stage 1.0 (TID 82)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 1.0 (TID 80). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 90.0 in stage 1.0 (TID 83, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 1.0 (TID 80) in 53 ms on localhost (executor driver) (79/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 90.0 in stage 1.0 (TID 83)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 1.0 (TID 82). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 91.0 in stage 1.0 (TID 84, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 1.0 (TID 81). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 1.0 (TID 82) in 60 ms on localhost (executor driver) (80/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 91.0 in stage 1.0 (TID 84)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,313][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 92.0 in stage 1.0 (TID 85, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 1.0 (TID 81) in 63 ms on localhost (executor driver) (81/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 92.0 in stage 1.0 (TID 85)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 1.0 (TID 83). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 93.0 in stage 1.0 (TID 86, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 93.0 in stage 1.0 (TID 86)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 1.0 (TID 83) in 66 ms on localhost (executor driver) (82/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 1.0 (TID 79). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,363][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 94.0 in stage 1.0 (TID 87, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 94.0 in stage 1.0 (TID 87)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 1.0 (TID 79) in 139 ms on localhost (executor driver) (83/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,366][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 1.0 (TID 84). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 95.0 in stage 1.0 (TID 88, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 1.0 (TID 85). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 1.0 (TID 84) in 84 ms on localhost (executor driver) (84/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 95.0 in stage 1.0 (TID 88)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 1.0 (TID 86). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 96.0 in stage 1.0 (TID 89, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 1.0 (TID 85) in 83 ms on localhost (executor driver) (85/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 96.0 in stage 1.0 (TID 89)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 97.0 in stage 1.0 (TID 90, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 97.0 in stage 1.0 (TID 90)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 1.0 (TID 86) in 54 ms on localhost (executor driver) (86/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,434][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,435][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 1.0 (TID 87). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,436][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 98.0 in stage 1.0 (TID 91, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,436][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 1.0 (TID 87) in 73 ms on localhost (executor driver) (87/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,436][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 98.0 in stage 1.0 (TID 91)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 1.0 (TID 90). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 1.0 (TID 89). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 99.0 in stage 1.0 (TID 92, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 100.0 in stage 1.0 (TID 93, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 99.0 in stage 1.0 (TID 92)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 1.0 (TID 90) in 50 ms on localhost (executor driver) (88/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 100.0 in stage 1.0 (TID 93)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 1.0 (TID 89) in 51 ms on localhost (executor driver) (89/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,478][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,478][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,553][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 1.0 (TID 88). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,553][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 1.0 (TID 92). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 101.0 in stage 1.0 (TID 94, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 1.0 (TID 91). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 102.0 in stage 1.0 (TID 95, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 101.0 in stage 1.0 (TID 94)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 1.0 (TID 88) in 160 ms on localhost (executor driver) (90/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 102.0 in stage 1.0 (TID 95)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 1.0 (TID 92) in 110 ms on localhost (executor driver) (91/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 103.0 in stage 1.0 (TID 96, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 1.0 (TID 91) in 120 ms on localhost (executor driver) (92/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 103.0 in stage 1.0 (TID 96)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 1.0 (TID 93). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 104.0 in stage 1.0 (TID 97, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 104.0 in stage 1.0 (TID 97)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 1.0 (TID 93) in 134 ms on localhost (executor driver) (93/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 1.0 (TID 94). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 1.0 (TID 96). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 1.0 (TID 95). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 105.0 in stage 1.0 (TID 98, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 105.0 in stage 1.0 (TID 98)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 106.0 in stage 1.0 (TID 99, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 1.0 (TID 96) in 61 ms on localhost (executor driver) (94/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 106.0 in stage 1.0 (TID 99)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 1.0 (TID 94) in 63 ms on localhost (executor driver) (95/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 107.0 in stage 1.0 (TID 100, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 1.0 (TID 95) in 63 ms on localhost (executor driver) (96/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 107.0 in stage 1.0 (TID 100)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,631][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 1.0 (TID 97). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 109.0 in stage 1.0 (TID 101, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 1.0 (TID 97) in 54 ms on localhost (executor driver) (97/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 109.0 in stage 1.0 (TID 101)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 1.0 (TID 100). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 1.0 (TID 99). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 110.0 in stage 1.0 (TID 102, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 111.0 in stage 1.0 (TID 103, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 110.0 in stage 1.0 (TID 102)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 1.0 (TID 100) in 88 ms on localhost (executor driver) (98/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 111.0 in stage 1.0 (TID 103)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 1.0 (TID 99) in 89 ms on localhost (executor driver) (99/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 1.0 (TID 98). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 112.0 in stage 1.0 (TID 104, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 1.0 (TID 98) in 92 ms on localhost (executor driver) (100/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 112.0 in stage 1.0 (TID 104)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,724][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,728][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,756][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 1.0 (TID 101). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 113.0 in stage 1.0 (TID 105, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 1.0 (TID 101) in 125 ms on localhost (executor driver) (101/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 113.0 in stage 1.0 (TID 105)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,776][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 1.0 (TID 104). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 114.0 in stage 1.0 (TID 106, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 1.0 (TID 104) in 71 ms on localhost (executor driver) (102/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 114.0 in stage 1.0 (TID 106)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 1.0 (TID 103). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 115.0 in stage 1.0 (TID 107, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 1.0 (TID 103) in 73 ms on localhost (executor driver) (103/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 115.0 in stage 1.0 (TID 107)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 1.0 (TID 102). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 117.0 in stage 1.0 (TID 108, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 1.0 (TID 102) in 122 ms on localhost (executor driver) (104/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 117.0 in stage 1.0 (TID 108)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 1.0 (TID 105). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 119.0 in stage 1.0 (TID 109, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 119.0 in stage 1.0 (TID 109)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 1.0 (TID 105) in 53 ms on localhost (executor driver) (105/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 1.0 (TID 106). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 120.0 in stage 1.0 (TID 110, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 1.0 (TID 106) in 53 ms on localhost (executor driver) (106/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 120.0 in stage 1.0 (TID 110)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 1.0 (TID 107). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 1.0 (TID 108). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 121.0 in stage 1.0 (TID 111, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 121.0 in stage 1.0 (TID 111)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 122.0 in stage 1.0 (TID 112, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 1.0 (TID 107) in 71 ms on localhost (executor driver) (107/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 122.0 in stage 1.0 (TID 112)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 1.0 (TID 108) in 55 ms on localhost (executor driver) (108/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,854][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 1.0 (TID 109). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 123.0 in stage 1.0 (TID 113, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 1.0 (TID 109) in 49 ms on localhost (executor driver) (109/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 123.0 in stage 1.0 (TID 113)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 1.0 (TID 111). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 124.0 in stage 1.0 (TID 114, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 124.0 in stage 1.0 (TID 114)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 1.0 (TID 111) in 51 ms on localhost (executor driver) (110/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 1.0 (TID 113). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 125.0 in stage 1.0 (TID 115, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 1.0 (TID 113) in 49 ms on localhost (executor driver) (111/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 125.0 in stage 1.0 (TID 115)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,909][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 1.0 (TID 110). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,910][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 126.0 in stage 1.0 (TID 116, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,910][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 1.0 (TID 110) in 79 ms on localhost (executor driver) (112/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 126.0 in stage 1.0 (TID 116)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 1.0 (TID 112). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 127.0 in stage 1.0 (TID 117, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 127.0 in stage 1.0 (TID 117)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 1.0 (TID 112) in 92 ms on localhost (executor driver) (113/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 1.0 (TID 114). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 128.0 in stage 1.0 (TID 118, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 1.0 (TID 114) in 64 ms on localhost (executor driver) (114/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 128.0 in stage 1.0 (TID 118)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 1.0 (TID 115). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 130.0 in stage 1.0 (TID 119, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 1.0 (TID 115) in 71 ms on localhost (executor driver) (115/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 130.0 in stage 1.0 (TID 119)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:25,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 1.0 (TID 116). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 131.0 in stage 1.0 (TID 120, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 1.0 (TID 116) in 93 ms on localhost (executor driver) (116/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 131.0 in stage 1.0 (TID 120)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 1.0 (TID 117). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 132.0 in stage 1.0 (TID 121, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 1.0 (TID 117) in 86 ms on localhost (executor driver) (117/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 132.0 in stage 1.0 (TID 121)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 1.0 (TID 118). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 133.0 in stage 1.0 (TID 122, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 1.0 (TID 118) in 99 ms on localhost (executor driver) (118/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 133.0 in stage 1.0 (TID 122)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 1.0 (TID 120). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 134.0 in stage 1.0 (TID 123, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 1.0 (TID 120) in 64 ms on localhost (executor driver) (119/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 134.0 in stage 1.0 (TID 123)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 1.0 (TID 119). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 135.0 in stage 1.0 (TID 124, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 1.0 (TID 119) in 113 ms on localhost (executor driver) (120/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 135.0 in stage 1.0 (TID 124)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,096][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 1.0 (TID 121). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 136.0 in stage 1.0 (TID 125, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 1.0 (TID 121) in 82 ms on localhost (executor driver) (121/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 136.0 in stage 1.0 (TID 125)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 1.0 (TID 122). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 1.0 (TID 123). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 137.0 in stage 1.0 (TID 126, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 137.0 in stage 1.0 (TID 126)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 138.0 in stage 1.0 (TID 127, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 1.0 (TID 122) in 53 ms on localhost (executor driver) (122/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 1.0 (TID 123) in 51 ms on localhost (executor driver) (123/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 138.0 in stage 1.0 (TID 127)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,179][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 1.0 (TID 125). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 139.0 in stage 1.0 (TID 128, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 1.0 (TID 125) in 79 ms on localhost (executor driver) (124/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 139.0 in stage 1.0 (TID 128)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 1.0 (TID 126). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 1.0 (TID 127). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 140.0 in stage 1.0 (TID 129, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 140.0 in stage 1.0 (TID 129)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 141.0 in stage 1.0 (TID 130, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 1.0 (TID 126) in 91 ms on localhost (executor driver) (125/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 141.0 in stage 1.0 (TID 130)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 1.0 (TID 127) in 90 ms on localhost (executor driver) (126/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 1.0 (TID 124). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 142.0 in stage 1.0 (TID 131, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 1.0 (TID 124) in 143 ms on localhost (executor driver) (127/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 142.0 in stage 1.0 (TID 131)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 1.0 (TID 129). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 143.0 in stage 1.0 (TID 132, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 1.0 (TID 130). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 143.0 in stage 1.0 (TID 132)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 1.0 (TID 129) in 82 ms on localhost (executor driver) (128/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 144.0 in stage 1.0 (TID 133, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 144.0 in stage 1.0 (TID 133)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 1.0 (TID 130) in 82 ms on localhost (executor driver) (129/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,310][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 1.0 (TID 128). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 145.0 in stage 1.0 (TID 134, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 1.0 (TID 128) in 127 ms on localhost (executor driver) (130/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,313][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 1.0 (TID 131). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 146.0 in stage 1.0 (TID 135, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 146.0 in stage 1.0 (TID 135)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 1.0 (TID 131) in 84 ms on localhost (executor driver) (131/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 145.0 in stage 1.0 (TID 134)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,337][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 1.0 (TID 133). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 147.0 in stage 1.0 (TID 136, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 1.0 (TID 133) in 53 ms on localhost (executor driver) (132/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 147.0 in stage 1.0 (TID 136)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 1.0 (TID 132). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 148.0 in stage 1.0 (TID 137, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 1.0 (TID 132) in 98 ms on localhost (executor driver) (133/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 148.0 in stage 1.0 (TID 137)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 1.0 (TID 134). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 149.0 in stage 1.0 (TID 138, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 1.0 (TID 134) in 83 ms on localhost (executor driver) (134/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 149.0 in stage 1.0 (TID 138)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 1.0 (TID 135). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 1.0 (TID 136). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 150.0 in stage 1.0 (TID 139, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 151.0 in stage 1.0 (TID 140, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 150.0 in stage 1.0 (TID 139)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 1.0 (TID 135) in 108 ms on localhost (executor driver) (135/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 151.0 in stage 1.0 (TID 140)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 1.0 (TID 136) in 84 ms on localhost (executor driver) (136/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 1.0 (TID 137). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 152.0 in stage 1.0 (TID 141, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 1.0 (TID 137) in 61 ms on localhost (executor driver) (137/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 152.0 in stage 1.0 (TID 141)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 1.0 (TID 138). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 153.0 in stage 1.0 (TID 142, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 1.0 (TID 138) in 52 ms on localhost (executor driver) (138/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 153.0 in stage 1.0 (TID 142)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 1.0 (TID 140). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 1.0 (TID 139). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 154.0 in stage 1.0 (TID 143, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,478][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 155.0 in stage 1.0 (TID 144, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 1.0 (TID 140) in 59 ms on localhost (executor driver) (139/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 155.0 in stage 1.0 (TID 144)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 154.0 in stage 1.0 (TID 143)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 1.0 (TID 139) in 59 ms on localhost (executor driver) (140/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,495][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 1.0 (TID 141). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 156.0 in stage 1.0 (TID 145, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 1.0 (TID 141) in 79 ms on localhost (executor driver) (141/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 156.0 in stage 1.0 (TID 145)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 1.0 (TID 142). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 157.0 in stage 1.0 (TID 146, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 1.0 (TID 142) in 85 ms on localhost (executor driver) (142/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 157.0 in stage 1.0 (TID 146)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,547][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,558][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 1.0 (TID 143). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 158.0 in stage 1.0 (TID 147, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 1.0 (TID 143) in 88 ms on localhost (executor driver) (143/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 158.0 in stage 1.0 (TID 147)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 1.0 (TID 145). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 160.0 in stage 1.0 (TID 148, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 160.0 in stage 1.0 (TID 148)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 1.0 (TID 145) in 54 ms on localhost (executor driver) (144/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,575][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 1.0 (TID 144). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 161.0 in stage 1.0 (TID 149, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 1.0 (TID 144) in 102 ms on localhost (executor driver) (145/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 161.0 in stage 1.0 (TID 149)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 1.0 (TID 146). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 162.0 in stage 1.0 (TID 150, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 162.0 in stage 1.0 (TID 150)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 1.0 (TID 146) in 61 ms on localhost (executor driver) (146/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 1.0 (TID 148). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 163.0 in stage 1.0 (TID 151, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 1.0 (TID 148) in 66 ms on localhost (executor driver) (147/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 163.0 in stage 1.0 (TID 151)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 1.0 (TID 149). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 1.0 (TID 147). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 164.0 in stage 1.0 (TID 152, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 1.0 (TID 149) in 67 ms on localhost (executor driver) (148/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 164.0 in stage 1.0 (TID 152)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 165.0 in stage 1.0 (TID 153, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 1.0 (TID 147) in 83 ms on localhost (executor driver) (149/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 165.0 in stage 1.0 (TID 153)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,667][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 1.0 (TID 150). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 166.0 in stage 1.0 (TID 154, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 1.0 (TID 150) in 79 ms on localhost (executor driver) (150/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 166.0 in stage 1.0 (TID 154)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 1.0 (TID 151). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 167.0 in stage 1.0 (TID 155, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 1.0 (TID 151) in 53 ms on localhost (executor driver) (151/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 167.0 in stage 1.0 (TID 155)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,699][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,700][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 1.0 (TID 153). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 168.0 in stage 1.0 (TID 156, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 1.0 (TID 153) in 56 ms on localhost (executor driver) (152/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 168.0 in stage 1.0 (TID 156)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 1.0 (TID 152). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 169.0 in stage 1.0 (TID 157, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 1.0 (TID 152) in 72 ms on localhost (executor driver) (153/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 169.0 in stage 1.0 (TID 157)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 1.0 (TID 154). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 170.0 in stage 1.0 (TID 158, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 1.0 (TID 154) in 54 ms on localhost (executor driver) (154/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 170.0 in stage 1.0 (TID 158)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,734][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 1.0 (TID 156). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 171.0 in stage 1.0 (TID 159, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 1.0 (TID 156) in 79 ms on localhost (executor driver) (155/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 171.0 in stage 1.0 (TID 159)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 1.0 (TID 157). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 172.0 in stage 1.0 (TID 160, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 1.0 (TID 157) in 68 ms on localhost (executor driver) (156/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 172.0 in stage 1.0 (TID 160)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 1.0 (TID 155). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 173.0 in stage 1.0 (TID 161, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 1.0 (TID 155) in 104 ms on localhost (executor driver) (157/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 173.0 in stage 1.0 (TID 161)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 1.0 (TID 158). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 174.0 in stage 1.0 (TID 162, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 1.0 (TID 158) in 80 ms on localhost (executor driver) (158/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 174.0 in stage 1.0 (TID 162)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 1.0 (TID 160). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 175.0 in stage 1.0 (TID 163, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 175.0 in stage 1.0 (TID 163)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 1.0 (TID 160) in 49 ms on localhost (executor driver) (159/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 1.0 (TID 159). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 176.0 in stage 1.0 (TID 164, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 1.0 (TID 159) in 67 ms on localhost (executor driver) (160/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 176.0 in stage 1.0 (TID 164)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 1.0 (TID 161). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 177.0 in stage 1.0 (TID 165, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 1.0 (TID 161) in 68 ms on localhost (executor driver) (161/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 177.0 in stage 1.0 (TID 165)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 1.0 (TID 162). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 1.0 (TID 163). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 1.0 (TID 164). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 178.0 in stage 1.0 (TID 166, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 178.0 in stage 1.0 (TID 166)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 179.0 in stage 1.0 (TID 167, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 180.0 in stage 1.0 (TID 168, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 179.0 in stage 1.0 (TID 167)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 1.0 (TID 162) in 94 ms on localhost (executor driver) (162/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 1.0 (TID 164) in 50 ms on localhost (executor driver) (163/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 1.0 (TID 163) in 64 ms on localhost (executor driver) (164/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 180.0 in stage 1.0 (TID 168)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 1.0 (TID 165). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 181.0 in stage 1.0 (TID 169, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 1.0 (TID 165) in 57 ms on localhost (executor driver) (165/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 181.0 in stage 1.0 (TID 169)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,924][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,924][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 1.0 (TID 166). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 1.0 (TID 167). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 182.0 in stage 1.0 (TID 170, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 182.0 in stage 1.0 (TID 170)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 183.0 in stage 1.0 (TID 171, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 1.0 (TID 166) in 51 ms on localhost (executor driver) (166/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 183.0 in stage 1.0 (TID 171)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 1.0 (TID 167) in 50 ms on localhost (executor driver) (167/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:26,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 1.0 (TID 171). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 184.0 in stage 1.0 (TID 172, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 1.0 (TID 171) in 72 ms on localhost (executor driver) (168/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 184.0 in stage 1.0 (TID 172)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 1.0 (TID 168). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 185.0 in stage 1.0 (TID 173, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 1.0 (TID 168) in 133 ms on localhost (executor driver) (169/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 185.0 in stage 1.0 (TID 173)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,030][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 1.0 (TID 170). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 186.0 in stage 1.0 (TID 174, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 1.0 (TID 170) in 94 ms on localhost (executor driver) (170/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 186.0 in stage 1.0 (TID 174)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 1.0 (TID 173). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 1.0 (TID 172). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 187.0 in stage 1.0 (TID 175, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 1.0 (TID 173) in 66 ms on localhost (executor driver) (171/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 188.0 in stage 1.0 (TID 176, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 1.0 (TID 172) in 80 ms on localhost (executor driver) (172/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 188.0 in stage 1.0 (TID 176)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 187.0 in stage 1.0 (TID 175)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 1.0 (TID 169). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 1.0 (TID 174). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 189.0 in stage 1.0 (TID 177, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 189.0 in stage 1.0 (TID 177)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 191.0 in stage 1.0 (TID 178, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 1.0 (TID 174) in 63 ms on localhost (executor driver) (173/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 191.0 in stage 1.0 (TID 178)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 1.0 (TID 169) in 183 ms on localhost (executor driver) (174/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 1.0 (TID 176). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 1.0 (TID 178). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 1.0 (TID 177). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 193.0 in stage 1.0 (TID 179, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 193.0 in stage 1.0 (TID 179)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 194.0 in stage 1.0 (TID 180, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 1.0 (TID 176) in 58 ms on localhost (executor driver) (175/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 194.0 in stage 1.0 (TID 180)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 1.0 (TID 178) in 52 ms on localhost (executor driver) (176/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 195.0 in stage 1.0 (TID 181, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 1.0 (TID 177) in 54 ms on localhost (executor driver) (177/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 195.0 in stage 1.0 (TID 181)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,161][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 1.0 (TID 175). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 196.0 in stage 1.0 (TID 182, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 1.0 (TID 175) in 74 ms on localhost (executor driver) (178/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 196.0 in stage 1.0 (TID 182)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,259][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 1.0 (TID 181). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 1.0 (TID 180). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 197.0 in stage 1.0 (TID 183, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 198.0 in stage 1.0 (TID 184, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 197.0 in stage 1.0 (TID 183)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 198.0 in stage 1.0 (TID 184)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 1.0 (TID 181) in 109 ms on localhost (executor driver) (179/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 1.0 (TID 180) in 110 ms on localhost (executor driver) (180/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 1.0 (TID 179). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,281][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 199.0 in stage 1.0 (TID 185, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,281][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 199.0 in stage 1.0 (TID 185)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,281][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 1.0 (TID 179) in 131 ms on localhost (executor driver) (181/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 1.0 (TID 184). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 186, localhost, executor driver, partition 0, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 1.0 (TID 184) in 58 ms on localhost (executor driver) (182/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 186)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 1.0 (TID 183). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 19.0 in stage 1.0 (TID 187, localhost, executor driver, partition 19, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 1.0 (TID 182). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 19.0 in stage 1.0 (TID 187)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 1.0 (TID 183) in 63 ms on localhost (executor driver) (183/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 30.0 in stage 1.0 (TID 188, localhost, executor driver, partition 30, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 1.0 (TID 182) in 163 ms on localhost (executor driver) (184/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 30.0 in stage 1.0 (TID 188)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 1.0 (TID 185). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 46.0 in stage 1.0 (TID 189, localhost, executor driver, partition 46, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 1.0 (TID 185) in 72 ms on localhost (executor driver) (185/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 46.0 in stage 1.0 (TID 189)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 1.0 (TID 189). 3736 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 56.0 in stage 1.0 (TID 190, localhost, executor driver, partition 56, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 1.0 (TID 189) in 69 ms on localhost (executor driver) (186/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 56.0 in stage 1.0 (TID 190)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,438][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 1.0 (TID 188). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 66.0 in stage 1.0 (TID 191, localhost, executor driver, partition 66, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 1.0 (TID 188) in 115 ms on localhost (executor driver) (187/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 66.0 in stage 1.0 (TID 191)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 1.0 (TID 190). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 67.0 in stage 1.0 (TID 192, localhost, executor driver, partition 67, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 1.0 (TID 190) in 49 ms on localhost (executor driver) (188/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 67.0 in stage 1.0 (TID 192)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 1.0 (TID 187). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 186). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 72.0 in stage 1.0 (TID 193, localhost, executor driver, partition 72, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 108.0 in stage 1.0 (TID 194, localhost, executor driver, partition 108, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 72.0 in stage 1.0 (TID 193)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 1.0 (TID 187) in 163 ms on localhost (executor driver) (189/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 108.0 in stage 1.0 (TID 194)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 186) in 168 ms on localhost (executor driver) (190/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,490][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,490][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,493][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,494][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,506][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 13 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 1.0 (TID 192). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 116.0 in stage 1.0 (TID 195, localhost, executor driver, partition 116, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 1.0 (TID 192) in 64 ms on localhost (executor driver) (191/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 116.0 in stage 1.0 (TID 195)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 1.0 (TID 191). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 118.0 in stage 1.0 (TID 196, localhost, executor driver, partition 118, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 1.0 (TID 191) in 97 ms on localhost (executor driver) (192/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 118.0 in stage 1.0 (TID 196)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,542][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,559][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,559][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 1.0 (TID 193). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 129.0 in stage 1.0 (TID 197, localhost, executor driver, partition 129, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 1.0 (TID 193) in 77 ms on localhost (executor driver) (193/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 129.0 in stage 1.0 (TID 197)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 1.0 (TID 194). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 159.0 in stage 1.0 (TID 198, localhost, executor driver, partition 159, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 1.0 (TID 194) in 80 ms on localhost (executor driver) (194/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 159.0 in stage 1.0 (TID 198)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,566][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,566][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 1.0 (TID 195). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 190.0 in stage 1.0 (TID 199, localhost, executor driver, partition 190, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 190.0 in stage 1.0 (TID 199)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 1.0 (TID 195) in 48 ms on localhost (executor driver) (195/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 1.0 (TID 196). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 192.0 in stage 1.0 (TID 200, localhost, executor driver, partition 192, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 1.0 (TID 196) in 58 ms on localhost (executor driver) (196/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 192.0 in stage 1.0 (TID 200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192/1.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 1.0 (TID 198). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 1.0 (TID 197). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 1.0 (TID 198) in 62 ms on localhost (executor driver) (197/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 1.0 (TID 197) in 64 ms on localhost (executor driver) (198/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 1.0 (TID 199). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 1.0 (TID 199) in 58 ms on localhost (executor driver) (199/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 1.0 (TID 200). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 1.0 (TID 200) in 46 ms on localhost (executor driver) (200/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 1 (start at StreamingFile.scala:61) finished in 4.554 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 0 finished: start at StreamingFile.scala:61, took 5.320423 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 11.216848 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 1 (start at StreamingFile.scala:61) with 1 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 2 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 2 (MapPartitionsRDD[13] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.7 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on 192.168.216.37:63892 (size: 4.6 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 2.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 201, localhost, executor driver, partition 0, PROCESS_LOCAL, 6076 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 201)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 16.462818 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 30.682047 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 201). 1087 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 201) in 76 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 2 (start at StreamingFile.scala:61) finished in 0.076 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 1 finished: start at StreamingFile.scala:61, took 0.105457 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,915][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 2 (start at StreamingFile.scala:61) with 3 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[13] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_6 stored as values in memory (estimated size 8.7 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_6_piece0 in memory on 192.168.216.37:63892 (size: 4.6 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(1, 2, 3))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 3.0 with 3 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 202, localhost, executor driver, partition 1, PROCESS_LOCAL, 6130 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 203, localhost, executor driver, partition 2, PROCESS_LOCAL, 6124 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 204, localhost, executor driver, partition 3, PROCESS_LOCAL, 6127 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 204)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 203)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 202)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 202). 1116 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 203). 1114 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 202) in 6 ms on localhost (executor driver) (1/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 204). 1120 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 203) in 5 ms on localhost (executor driver) (2/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 204) in 6 ms on localhost (executor driver) (3/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 3 (start at StreamingFile.scala:61) finished in 0.007 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:27,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 2 finished: start at StreamingFile.scala:61, took 0.014125 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:28,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:03:20.106Z",
  "numInputRows" : 100,
  "processedRowsPerSecond" : 12.768130745658835,
  "durationMs" : {
    "addBatch" : 7491,
    "getBatch" : 116,
    "getOffset" : 55,
    "queryPlanning" : 110,
    "triggerExecution" : 7831,
    "walCommit" : 46
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 15
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : null,
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 100,
    "processedRowsPerSecond" : 12.768130745658835
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[31m[WARN ][0;39m [35m[2017-11-03 08:03:28,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 7950 milliseconds
[34m[INFO ][0;39m [35m[2017-11-03 08:03:28,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:03:28.057Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:03:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:03:40.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:03:50,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:03:50.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Log offset set to 1 with 2 new files
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1509703434067,Map(spark.sql.shuffle.partitions -> 200))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Processing 2 files from 1:1
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<carrier: string, marital_status: string>
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_7 stored as values in memory (estimated size 221.7 KB, free 911.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_7_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_7_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 7 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_8 stored as values in memory (estimated size 220.5 KB, free 911.0 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.0 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_8_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 8 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_9 stored as values in memory (estimated size 220.5 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.7 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_9_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 9 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering RDD 16 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 3 (start at StreamingFile.scala:61) with 200 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 5 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 4)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 4)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 4 (MapPartitionsRDD[16] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_10 stored as values in memory (estimated size 27.0 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.1 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_10_piece0 in memory on 192.168.216.37:63892 (size: 13.1 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[16] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 4.0 with 2 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 4.0 (TID 205, localhost, executor driver, partition 0, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 4.0 (TID 206, localhost, executor driver, partition 1, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 4.0 (TID 205)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 4.0 (TID 206)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.3.csv, range: 0-6907, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.2.csv, range: 0-6869, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,854][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 4.0 (TID 205). 2264 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 4.0 (TID 205) in 39 ms on localhost (executor driver) (1/2)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 4.0 (TID 206). 2264 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 4.0 (TID 206) in 39 ms on localhost (executor driver) (2/2)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 4.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ShuffleMapStage 4 (start at StreamingFile.scala:61) finished in 0.041 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 5)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 5 (MapPartitionsRDD[23] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_11 stored as values in memory (estimated size 52.4 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_11_piece0 stored as bytes in memory (estimated size 21.9 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_11_piece0 in memory on 192.168.216.37:63892 (size: 21.9 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 5.0 with 200 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 5.0 (TID 207, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 5.0 (TID 208, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 5.0 (TID 209, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 5.0 (TID 210, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 5.0 (TID 209)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 5.0 (TID 207)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 5.0 (TID 208)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 5.0 (TID 210)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 5.0 (TID 209). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 5.0 (TID 210). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 4.0 in stage 5.0 (TID 211, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 4.0 in stage 5.0 (TID 211)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 5.0 in stage 5.0 (TID 212, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 5.0 (TID 209) in 43 ms on localhost (executor driver) (1/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 5.0 in stage 5.0 (TID 212)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 5.0 (TID 210) in 44 ms on localhost (executor driver) (2/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 5.0 (TID 208). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 6.0 in stage 5.0 (TID 213, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 5.0 (TID 208) in 53 ms on localhost (executor driver) (3/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 6.0 in stage 5.0 (TID 213)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 5.0 (TID 207). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 7.0 in stage 5.0 (TID 214, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 5.0 (TID 207) in 64 ms on localhost (executor driver) (4/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 7.0 in stage 5.0 (TID 214)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 5.0 (TID 211). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 8.0 in stage 5.0 (TID 215, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 5.0 (TID 211) in 56 ms on localhost (executor driver) (5/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 8.0 in stage 5.0 (TID 215)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 5.0 (TID 212). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 9.0 in stage 5.0 (TID 216, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 9.0 in stage 5.0 (TID 216)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:54,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 5.0 (TID 212) in 62 ms on localhost (executor driver) (6/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 5.0 (TID 213). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 10.0 in stage 5.0 (TID 217, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 5.0 (TID 213) in 60 ms on localhost (executor driver) (7/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 10.0 in stage 5.0 (TID 217)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 5.0 (TID 214). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 11.0 in stage 5.0 (TID 218, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 5.0 (TID 214) in 58 ms on localhost (executor driver) (8/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 11.0 in stage 5.0 (TID 218)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 5.0 (TID 216). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 12.0 in stage 5.0 (TID 219, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 12.0 in stage 5.0 (TID 219)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 5.0 (TID 216) in 47 ms on localhost (executor driver) (9/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 5.0 (TID 215). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 13.0 in stage 5.0 (TID 220, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 5.0 (TID 215) in 72 ms on localhost (executor driver) (10/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 13.0 in stage 5.0 (TID 220)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,180][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_5_piece0 on 192.168.216.37:63892 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_6_piece0 on 192.168.216.37:63892 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_10_piece0 on 192.168.216.37:63892 in memory (size: 13.1 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 151
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 102
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 5.0 (TID 217). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 14.0 in stage 5.0 (TID 221, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 5.0 (TID 217) in 211 ms on localhost (executor driver) (11/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 14.0 in stage 5.0 (TID 221)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,226][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 5.0 (TID 218). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 15.0 in stage 5.0 (TID 222, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 5.0 (TID 218) in 242 ms on localhost (executor driver) (12/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 15.0 in stage 5.0 (TID 222)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 5.0 (TID 219). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 5.0 (TID 220). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 12 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 16.0 in stage 5.0 (TID 223, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 5.0 (TID 219) in 220 ms on localhost (executor driver) (13/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 17.0 in stage 5.0 (TID 224, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 16.0 in stage 5.0 (TID 223)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 5.0 (TID 220) in 204 ms on localhost (executor driver) (14/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 17.0 in stage 5.0 (TID 224)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 5.0 (TID 221). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 5.0 (TID 221) in 171 ms on localhost (executor driver) (15/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 18.0 in stage 5.0 (TID 225, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 18.0 in stage 5.0 (TID 225)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 5.0 (TID 223). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 19.0 in stage 5.0 (TID 226, localhost, executor driver, partition 19, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 5.0 (TID 223) in 144 ms on localhost (executor driver) (16/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 19.0 in stage 5.0 (TID 226)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 5.0 (TID 222). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 5.0 (TID 224). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 20.0 in stage 5.0 (TID 227, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 20.0 in stage 5.0 (TID 227)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 21.0 in stage 5.0 (TID 228, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 5.0 (TID 222) in 206 ms on localhost (executor driver) (17/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 21.0 in stage 5.0 (TID 228)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 5.0 (TID 224) in 188 ms on localhost (executor driver) (18/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 5.0 (TID 226). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,493][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 22.0 in stage 5.0 (TID 229, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,493][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 5.0 (TID 226) in 87 ms on localhost (executor driver) (19/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,493][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 22.0 in stage 5.0 (TID 229)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,500][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,501][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 5.0 (TID 225). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 23.0 in stage 5.0 (TID 230, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 5.0 (TID 225) in 119 ms on localhost (executor driver) (20/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 23.0 in stage 5.0 (TID 230)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,504][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 5.0 (TID 227). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 24.0 in stage 5.0 (TID 231, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 5.0 (TID 227) in 56 ms on localhost (executor driver) (21/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 24.0 in stage 5.0 (TID 231)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,506][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,506][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 5.0 (TID 228). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 25.0 in stage 5.0 (TID 232, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 5.0 (TID 228) in 62 ms on localhost (executor driver) (22/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 25.0 in stage 5.0 (TID 232)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,516][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 5.0 (TID 229). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 26.0 in stage 5.0 (TID 233, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 5.0 (TID 229) in 85 ms on localhost (executor driver) (23/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 26.0 in stage 5.0 (TID 233)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 5.0 (TID 231). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 27.0 in stage 5.0 (TID 234, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 27.0 in stage 5.0 (TID 234)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 5.0 (TID 231) in 82 ms on localhost (executor driver) (24/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 5.0 (TID 232). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 5.0 (TID 230). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 28.0 in stage 5.0 (TID 235, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 28.0 in stage 5.0 (TID 235)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 29.0 in stage 5.0 (TID 236, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 5.0 (TID 230) in 100 ms on localhost (executor driver) (25/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 5.0 (TID 232) in 89 ms on localhost (executor driver) (26/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 29.0 in stage 5.0 (TID 236)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,657][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,657][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,657][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,657][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 5.0 (TID 236). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 5.0 (TID 235). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 5.0 (TID 233). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 30.0 in stage 5.0 (TID 237, localhost, executor driver, partition 30, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 5.0 (TID 236) in 58 ms on localhost (executor driver) (27/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 30.0 in stage 5.0 (TID 237)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 31.0 in stage 5.0 (TID 238, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 5.0 (TID 235) in 59 ms on localhost (executor driver) (28/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 31.0 in stage 5.0 (TID 238)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 32.0 in stage 5.0 (TID 239, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 5.0 (TID 233) in 82 ms on localhost (executor driver) (29/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 32.0 in stage 5.0 (TID 239)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,662][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,665][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,665][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,667][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,667][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,669][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 5.0 (TID 234). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 33.0 in stage 5.0 (TID 240, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 5.0 (TID 234) in 102 ms on localhost (executor driver) (30/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 33.0 in stage 5.0 (TID 240)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 5.0 (TID 239). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 5.0 (TID 238). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 34.0 in stage 5.0 (TID 241, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 5.0 (TID 239) in 122 ms on localhost (executor driver) (31/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 34.0 in stage 5.0 (TID 241)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 5.0 (TID 238) in 122 ms on localhost (executor driver) (32/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 35.0 in stage 5.0 (TID 242, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 35.0 in stage 5.0 (TID 242)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 5.0 (TID 240). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 36.0 in stage 5.0 (TID 243, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 5.0 (TID 240) in 114 ms on localhost (executor driver) (33/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 36.0 in stage 5.0 (TID 243)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 5.0 (TID 237). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 37.0 in stage 5.0 (TID 244, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 5.0 (TID 237) in 145 ms on localhost (executor driver) (34/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 37.0 in stage 5.0 (TID 244)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 5.0 (TID 242). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 5.0 (TID 241). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 38.0 in stage 5.0 (TID 245, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 39.0 in stage 5.0 (TID 246, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 38.0 in stage 5.0 (TID 245)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 5.0 (TID 241) in 60 ms on localhost (executor driver) (35/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 39.0 in stage 5.0 (TID 246)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 5.0 (TID 242) in 58 ms on localhost (executor driver) (36/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 5.0 (TID 243). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 40.0 in stage 5.0 (TID 247, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 5.0 (TID 243) in 53 ms on localhost (executor driver) (37/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 40.0 in stage 5.0 (TID 247)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 5.0 (TID 244). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 41.0 in stage 5.0 (TID 248, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 41.0 in stage 5.0 (TID 248)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 5.0 (TID 244) in 60 ms on localhost (executor driver) (38/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 5.0 (TID 245). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 42.0 in stage 5.0 (TID 249, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 5.0 (TID 245) in 53 ms on localhost (executor driver) (39/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 5.0 (TID 246). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 43.0 in stage 5.0 (TID 250, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 42.0 in stage 5.0 (TID 249)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 5.0 (TID 246) in 53 ms on localhost (executor driver) (40/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 43.0 in stage 5.0 (TID 250)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,896][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 5.0 (TID 247). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 44.0 in stage 5.0 (TID 251, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 5.0 (TID 247) in 56 ms on localhost (executor driver) (41/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 44.0 in stage 5.0 (TID 251)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 5.0 (TID 248). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 45.0 in stage 5.0 (TID 252, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 5.0 (TID 248) in 61 ms on localhost (executor driver) (42/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 45.0 in stage 5.0 (TID 252)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 5.0 (TID 249). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 46.0 in stage 5.0 (TID 253, localhost, executor driver, partition 46, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 5.0 (TID 249) in 74 ms on localhost (executor driver) (43/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 46.0 in stage 5.0 (TID 253)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 5.0 (TID 252). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 47.0 in stage 5.0 (TID 254, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 5.0 (TID 252) in 51 ms on localhost (executor driver) (44/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 47.0 in stage 5.0 (TID 254)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 5.0 (TID 250). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 48.0 in stage 5.0 (TID 255, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 5.0 (TID 250) in 87 ms on localhost (executor driver) (45/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 48.0 in stage 5.0 (TID 255)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 5.0 (TID 251). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 49.0 in stage 5.0 (TID 256, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 5.0 (TID 251) in 82 ms on localhost (executor driver) (46/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 49.0 in stage 5.0 (TID 256)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:55,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 5.0 (TID 254). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 50.0 in stage 5.0 (TID 257, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 5.0 (TID 253). 3737 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 5.0 (TID 254) in 64 ms on localhost (executor driver) (47/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 50.0 in stage 5.0 (TID 257)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 51.0 in stage 5.0 (TID 258, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 5.0 (TID 253) in 71 ms on localhost (executor driver) (48/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 51.0 in stage 5.0 (TID 258)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,058][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 5.0 (TID 255). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 52.0 in stage 5.0 (TID 259, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 52.0 in stage 5.0 (TID 259)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 5.0 (TID 255) in 82 ms on localhost (executor driver) (49/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 5.0 (TID 256). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 53.0 in stage 5.0 (TID 260, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 5.0 (TID 256) in 119 ms on localhost (executor driver) (50/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 53.0 in stage 5.0 (TID 260)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 5.0 (TID 259). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 54.0 in stage 5.0 (TID 261, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 5.0 (TID 259) in 83 ms on localhost (executor driver) (51/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 54.0 in stage 5.0 (TID 261)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 5.0 (TID 258). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 55.0 in stage 5.0 (TID 262, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 5.0 (TID 258) in 110 ms on localhost (executor driver) (52/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 55.0 in stage 5.0 (TID 262)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,206][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 5.0 (TID 257). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 56.0 in stage 5.0 (TID 263, localhost, executor driver, partition 56, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 5.0 (TID 257) in 176 ms on localhost (executor driver) (53/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 56.0 in stage 5.0 (TID 263)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,222][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 5.0 (TID 260). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 57.0 in stage 5.0 (TID 264, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 57.0 in stage 5.0 (TID 264)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 5.0 (TID 260) in 117 ms on localhost (executor driver) (54/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 5.0 (TID 262). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 58.0 in stage 5.0 (TID 265, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 5.0 (TID 262) in 98 ms on localhost (executor driver) (55/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 58.0 in stage 5.0 (TID 265)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 5.0 (TID 261). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 59.0 in stage 5.0 (TID 266, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 59.0 in stage 5.0 (TID 266)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 5.0 (TID 261) in 103 ms on localhost (executor driver) (56/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 5.0 (TID 263). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 60.0 in stage 5.0 (TID 267, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 60.0 in stage 5.0 (TID 267)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 5.0 (TID 263) in 53 ms on localhost (executor driver) (57/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 5.0 (TID 264). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 61.0 in stage 5.0 (TID 268, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 5.0 (TID 264) in 47 ms on localhost (executor driver) (58/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 61.0 in stage 5.0 (TID 268)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,296][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 5.0 (TID 266). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 62.0 in stage 5.0 (TID 269, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 62.0 in stage 5.0 (TID 269)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 5.0 (TID 266) in 51 ms on localhost (executor driver) (59/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,306][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 5.0 (TID 265). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 63.0 in stage 5.0 (TID 270, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 5.0 (TID 265) in 65 ms on localhost (executor driver) (60/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 63.0 in stage 5.0 (TID 270)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 5.0 (TID 267). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 64.0 in stage 5.0 (TID 271, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 5.0 (TID 267) in 50 ms on localhost (executor driver) (61/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 64.0 in stage 5.0 (TID 271)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 5.0 (TID 268). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 65.0 in stage 5.0 (TID 272, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 65.0 in stage 5.0 (TID 272)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 5.0 (TID 268) in 59 ms on localhost (executor driver) (62/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 5.0 (TID 269). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 66.0 in stage 5.0 (TID 273, localhost, executor driver, partition 66, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 66.0 in stage 5.0 (TID 273)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 5.0 (TID 269) in 53 ms on localhost (executor driver) (63/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,363][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,363][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,363][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 5.0 (TID 271). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 67.0 in stage 5.0 (TID 274, localhost, executor driver, partition 67, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 5.0 (TID 270). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 5.0 (TID 271) in 56 ms on localhost (executor driver) (64/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 67.0 in stage 5.0 (TID 274)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 68.0 in stage 5.0 (TID 275, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 5.0 (TID 270) in 64 ms on localhost (executor driver) (65/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 68.0 in stage 5.0 (TID 275)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 5.0 (TID 272). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 69.0 in stage 5.0 (TID 276, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 5.0 (TID 272) in 69 ms on localhost (executor driver) (66/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 69.0 in stage 5.0 (TID 276)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 5.0 (TID 273). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 70.0 in stage 5.0 (TID 277, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 5.0 (TID 273) in 97 ms on localhost (executor driver) (67/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 70.0 in stage 5.0 (TID 277)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 5.0 (TID 275). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 71.0 in stage 5.0 (TID 278, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 5.0 (TID 275) in 128 ms on localhost (executor driver) (68/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 71.0 in stage 5.0 (TID 278)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,506][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 5.0 (TID 277). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 72.0 in stage 5.0 (TID 279, localhost, executor driver, partition 72, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 72.0 in stage 5.0 (TID 279)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 5.0 (TID 277) in 61 ms on localhost (executor driver) (69/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 5.0 (TID 276). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 73.0 in stage 5.0 (TID 280, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 5.0 (TID 276) in 264 ms on localhost (executor driver) (70/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 73.0 in stage 5.0 (TID 280)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 5.0 (TID 274). 3785 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,669][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 5.0 (TID 279). 3782 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,669][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 74.0 in stage 5.0 (TID 281, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 74.0 in stage 5.0 (TID 281)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 75.0 in stage 5.0 (TID 282, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 5.0 (TID 274) in 303 ms on localhost (executor driver) (71/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 75.0 in stage 5.0 (TID 282)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 5.0 (TID 279) in 164 ms on localhost (executor driver) (72/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 5.0 (TID 278). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 76.0 in stage 5.0 (TID 283, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 76.0 in stage 5.0 (TID 283)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 5.0 (TID 278) in 192 ms on localhost (executor driver) (73/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,705][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 5.0 (TID 280). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 77.0 in stage 5.0 (TID 284, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 77.0 in stage 5.0 (TID 284)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 5.0 (TID 282). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 5.0 (TID 280) in 55 ms on localhost (executor driver) (74/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 78.0 in stage 5.0 (TID 285, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 5.0 (TID 282) in 45 ms on localhost (executor driver) (75/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 78.0 in stage 5.0 (TID 285)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,717][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,717][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 5.0 (TID 281). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 79.0 in stage 5.0 (TID 286, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 79.0 in stage 5.0 (TID 286)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 5.0 (TID 281) in 79 ms on localhost (executor driver) (76/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 5.0 (TID 283). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 80.0 in stage 5.0 (TID 287, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 80.0 in stage 5.0 (TID 287)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 5.0 (TID 283) in 71 ms on localhost (executor driver) (77/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,763][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,776][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,776][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 5.0 (TID 285). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 5.0 (TID 284). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 81.0 in stage 5.0 (TID 288, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 81.0 in stage 5.0 (TID 288)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 82.0 in stage 5.0 (TID 289, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 5.0 (TID 285) in 64 ms on localhost (executor driver) (78/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 82.0 in stage 5.0 (TID 289)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 5.0 (TID 284) in 66 ms on localhost (executor driver) (79/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 5.0 (TID 287). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 83.0 in stage 5.0 (TID 290, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 5.0 (TID 287) in 73 ms on localhost (executor driver) (80/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 83.0 in stage 5.0 (TID 290)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 5.0 (TID 286). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 84.0 in stage 5.0 (TID 291, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 5.0 (TID 286) in 113 ms on localhost (executor driver) (81/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 84.0 in stage 5.0 (TID 291)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 5.0 (TID 288). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 85.0 in stage 5.0 (TID 292, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 5.0 (TID 288) in 101 ms on localhost (executor driver) (82/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 85.0 in stage 5.0 (TID 292)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 5.0 (TID 289). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 86.0 in stage 5.0 (TID 293, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 5.0 (TID 289) in 122 ms on localhost (executor driver) (83/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 86.0 in stage 5.0 (TID 293)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 5.0 (TID 291). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,901][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 87.0 in stage 5.0 (TID 294, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 5.0 (TID 291) in 42 ms on localhost (executor driver) (84/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 87.0 in stage 5.0 (TID 294)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 5.0 (TID 290). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 88.0 in stage 5.0 (TID 295, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 5.0 (TID 290) in 74 ms on localhost (executor driver) (85/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 88.0 in stage 5.0 (TID 295)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 5.0 (TID 292). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 89.0 in stage 5.0 (TID 296, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 89.0 in stage 5.0 (TID 296)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 5.0 (TID 292) in 60 ms on localhost (executor driver) (86/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 5.0 (TID 293). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 90.0 in stage 5.0 (TID 297, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 5.0 (TID 293) in 66 ms on localhost (executor driver) (87/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 90.0 in stage 5.0 (TID 297)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 5.0 (TID 295). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 91.0 in stage 5.0 (TID 298, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 5.0 (TID 295) in 63 ms on localhost (executor driver) (88/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 91.0 in stage 5.0 (TID 298)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 5.0 (TID 294). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 92.0 in stage 5.0 (TID 299, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 5.0 (TID 294) in 78 ms on localhost (executor driver) (89/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 92.0 in stage 5.0 (TID 299)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:56,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,030][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 5.0 (TID 297). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 93.0 in stage 5.0 (TID 300, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 5.0 (TID 297) in 71 ms on localhost (executor driver) (90/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 93.0 in stage 5.0 (TID 300)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 5.0 (TID 298). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 94.0 in stage 5.0 (TID 301, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 5.0 (TID 298) in 76 ms on localhost (executor driver) (91/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 94.0 in stage 5.0 (TID 301)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 5.0 (TID 299). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 5.0 (TID 296). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 95.0 in stage 5.0 (TID 302, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 96.0 in stage 5.0 (TID 303, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 95.0 in stage 5.0 (TID 302)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 5.0 (TID 296) in 118 ms on localhost (executor driver) (92/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 5.0 (TID 299) in 77 ms on localhost (executor driver) (93/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 96.0 in stage 5.0 (TID 303)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 5.0 (TID 301). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 5.0 (TID 302). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 5.0 (TID 300). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 97.0 in stage 5.0 (TID 304, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 98.0 in stage 5.0 (TID 305, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 97.0 in stage 5.0 (TID 304)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 5.0 (TID 301) in 62 ms on localhost (executor driver) (94/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 98.0 in stage 5.0 (TID 305)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 5.0 (TID 302) in 51 ms on localhost (executor driver) (95/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 99.0 in stage 5.0 (TID 306, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 5.0 (TID 300) in 74 ms on localhost (executor driver) (96/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 99.0 in stage 5.0 (TID 306)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 5.0 (TID 303). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 100.0 in stage 5.0 (TID 307, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 5.0 (TID 303) in 61 ms on localhost (executor driver) (97/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 100.0 in stage 5.0 (TID 307)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 5.0 (TID 305). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 101.0 in stage 5.0 (TID 308, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 5.0 (TID 305) in 52 ms on localhost (executor driver) (98/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 101.0 in stage 5.0 (TID 308)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 5.0 (TID 306). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 102.0 in stage 5.0 (TID 309, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 5.0 (TID 306) in 64 ms on localhost (executor driver) (99/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 102.0 in stage 5.0 (TID 309)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 5.0 (TID 304). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,183][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 103.0 in stage 5.0 (TID 310, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,183][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 5.0 (TID 304) in 78 ms on localhost (executor driver) (100/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,184][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 103.0 in stage 5.0 (TID 310)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 5.0 (TID 307). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 104.0 in stage 5.0 (TID 311, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 104.0 in stage 5.0 (TID 311)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 5.0 (TID 307) in 72 ms on localhost (executor driver) (101/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,222][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 5.0 (TID 309). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 105.0 in stage 5.0 (TID 312, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 105.0 in stage 5.0 (TID 312)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 5.0 (TID 309) in 56 ms on localhost (executor driver) (102/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 5.0 (TID 308). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 106.0 in stage 5.0 (TID 313, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,232][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 106.0 in stage 5.0 (TID 313)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,232][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 5.0 (TID 308) in 76 ms on localhost (executor driver) (103/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 5.0 (TID 310). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 107.0 in stage 5.0 (TID 314, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 107.0 in stage 5.0 (TID 314)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 5.0 (TID 310) in 53 ms on localhost (executor driver) (104/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,235][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 5.0 (TID 311). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 108.0 in stage 5.0 (TID 315, localhost, executor driver, partition 108, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 5.0 (TID 311) in 64 ms on localhost (executor driver) (105/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 108.0 in stage 5.0 (TID 315)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 5.0 (TID 314). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 109.0 in stage 5.0 (TID 316, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 5.0 (TID 314) in 55 ms on localhost (executor driver) (106/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 109.0 in stage 5.0 (TID 316)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 5.0 (TID 313). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 110.0 in stage 5.0 (TID 317, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 5.0 (TID 313) in 62 ms on localhost (executor driver) (107/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 110.0 in stage 5.0 (TID 317)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 5.0 (TID 312). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 111.0 in stage 5.0 (TID 318, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 5.0 (TID 312) in 80 ms on localhost (executor driver) (108/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 111.0 in stage 5.0 (TID 318)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 5.0 (TID 315). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 112.0 in stage 5.0 (TID 319, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 112.0 in stage 5.0 (TID 319)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 5.0 (TID 315) in 72 ms on localhost (executor driver) (109/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,333][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 5.0 (TID 316). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 5.0 (TID 317). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 113.0 in stage 5.0 (TID 320, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 5.0 (TID 316) in 58 ms on localhost (executor driver) (110/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 113.0 in stage 5.0 (TID 320)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 5.0 (TID 317) in 54 ms on localhost (executor driver) (111/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 114.0 in stage 5.0 (TID 321, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 114.0 in stage 5.0 (TID 321)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 5.0 (TID 318). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 115.0 in stage 5.0 (TID 322, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 115.0 in stage 5.0 (TID 322)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 5.0 (TID 318) in 58 ms on localhost (executor driver) (112/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 5.0 (TID 319). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 116.0 in stage 5.0 (TID 323, localhost, executor driver, partition 116, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 5.0 (TID 319) in 73 ms on localhost (executor driver) (113/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 116.0 in stage 5.0 (TID 323)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,400][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 5.0 (TID 321). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 117.0 in stage 5.0 (TID 324, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 5.0 (TID 321) in 54 ms on localhost (executor driver) (114/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 117.0 in stage 5.0 (TID 324)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 5.0 (TID 320). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 118.0 in stage 5.0 (TID 325, localhost, executor driver, partition 118, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 5.0 (TID 320) in 59 ms on localhost (executor driver) (115/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 118.0 in stage 5.0 (TID 325)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 5.0 (TID 322). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 119.0 in stage 5.0 (TID 326, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 5.0 (TID 322) in 53 ms on localhost (executor driver) (116/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 119.0 in stage 5.0 (TID 326)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 5.0 (TID 326). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 120.0 in stage 5.0 (TID 327, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 5.0 (TID 326) in 46 ms on localhost (executor driver) (117/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 120.0 in stage 5.0 (TID 327)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,478][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 5.0 (TID 323). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 121.0 in stage 5.0 (TID 328, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 5.0 (TID 323) in 85 ms on localhost (executor driver) (118/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,479][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 121.0 in stage 5.0 (TID 328)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 5.0 (TID 324). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 122.0 in stage 5.0 (TID 329, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,500][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 5.0 (TID 324) in 100 ms on localhost (executor driver) (119/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,501][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 122.0 in stage 5.0 (TID 329)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 5.0 (TID 327). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 123.0 in stage 5.0 (TID 330, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 5.0 (TID 327) in 51 ms on localhost (executor driver) (120/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 123.0 in stage 5.0 (TID 330)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,516][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 5.0 (TID 325). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,516][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 124.0 in stage 5.0 (TID 331, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 5.0 (TID 325) in 114 ms on localhost (executor driver) (121/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 124.0 in stage 5.0 (TID 331)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 5.0 (TID 328). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 125.0 in stage 5.0 (TID 332, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 5.0 (TID 328) in 46 ms on localhost (executor driver) (122/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 125.0 in stage 5.0 (TID 332)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 5.0 (TID 329). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,575][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 126.0 in stage 5.0 (TID 333, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 5.0 (TID 329) in 76 ms on localhost (executor driver) (123/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 5.0 (TID 330). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 127.0 in stage 5.0 (TID 334, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 127.0 in stage 5.0 (TID 334)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 5.0 (TID 330) in 69 ms on localhost (executor driver) (124/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 126.0 in stage 5.0 (TID 333)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,582][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,582][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 5.0 (TID 332). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 128.0 in stage 5.0 (TID 335, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 5.0 (TID 332) in 78 ms on localhost (executor driver) (125/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 128.0 in stage 5.0 (TID 335)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,608][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 5.0 (TID 331). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 129.0 in stage 5.0 (TID 336, localhost, executor driver, partition 129, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 5.0 (TID 331) in 94 ms on localhost (executor driver) (126/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 129.0 in stage 5.0 (TID 336)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 5.0 (TID 334). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 130.0 in stage 5.0 (TID 337, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 130.0 in stage 5.0 (TID 337)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 5.0 (TID 334) in 161 ms on localhost (executor driver) (127/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 5.0 (TID 333). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 131.0 in stage 5.0 (TID 338, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 5.0 (TID 333) in 164 ms on localhost (executor driver) (128/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 131.0 in stage 5.0 (TID 338)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,742][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,742][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,742][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 5.0 (TID 335). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 132.0 in stage 5.0 (TID 339, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 132.0 in stage 5.0 (TID 339)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 5.0 (TID 335) in 160 ms on localhost (executor driver) (129/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 5.0 (TID 336). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 5.0 (TID 337). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 133.0 in stage 5.0 (TID 340, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 133.0 in stage 5.0 (TID 340)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 134.0 in stage 5.0 (TID 341, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 5.0 (TID 336) in 173 ms on localhost (executor driver) (130/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 134.0 in stage 5.0 (TID 341)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 5.0 (TID 337) in 46 ms on localhost (executor driver) (131/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 5.0 (TID 338). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 135.0 in stage 5.0 (TID 342, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 5.0 (TID 338) in 64 ms on localhost (executor driver) (132/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 135.0 in stage 5.0 (TID 342)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:57,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,096][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 3
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 6
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 21
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 14
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 18
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 5
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 28
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 4
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 23
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 16
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 8
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 29
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 27
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 15
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 22
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 7
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on 192.168.216.37:63892 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on 192.168.216.37:63892 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 5.0 (TID 339). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 19
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 24
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 136.0 in stage 5.0 (TID 343, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 5.0 (TID 339) in 340 ms on localhost (executor driver) (133/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 136.0 in stage 5.0 (TID 343)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned shuffle 0
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_4_piece0 on 192.168.216.37:63892 in memory (size: 21.9 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 25
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 10
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 26
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on 192.168.216.37:63892 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 11
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 20
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 12
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 13
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 17
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 9
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 5.0 (TID 340). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 137.0 in stage 5.0 (TID 344, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 137.0 in stage 5.0 (TID 344)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 5.0 (TID 340) in 350 ms on localhost (executor driver) (134/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 5.0 (TID 341). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 5.0 (TID 342). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 138.0 in stage 5.0 (TID 345, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 139.0 in stage 5.0 (TID 346, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 138.0 in stage 5.0 (TID 345)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 5.0 (TID 342) in 340 ms on localhost (executor driver) (135/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 139.0 in stage 5.0 (TID 346)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 5.0 (TID 341) in 360 ms on localhost (executor driver) (136/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 5.0 (TID 343). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 140.0 in stage 5.0 (TID 347, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 140.0 in stage 5.0 (TID 347)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 5.0 (TID 343) in 53 ms on localhost (executor driver) (137/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,161][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 5.0 (TID 344). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 141.0 in stage 5.0 (TID 348, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 141.0 in stage 5.0 (TID 348)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 5.0 (TID 344) in 57 ms on localhost (executor driver) (138/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 5.0 (TID 346). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 5.0 (TID 345). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 142.0 in stage 5.0 (TID 349, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 142.0 in stage 5.0 (TID 349)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 143.0 in stage 5.0 (TID 350, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 5.0 (TID 346) in 59 ms on localhost (executor driver) (139/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 143.0 in stage 5.0 (TID 350)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 5.0 (TID 345) in 59 ms on localhost (executor driver) (140/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,206][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 5.0 (TID 347). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 144.0 in stage 5.0 (TID 351, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 5.0 (TID 347) in 55 ms on localhost (executor driver) (141/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 144.0 in stage 5.0 (TID 351)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,226][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 5.0 (TID 348). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 145.0 in stage 5.0 (TID 352, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 145.0 in stage 5.0 (TID 352)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 5.0 (TID 348) in 50 ms on localhost (executor driver) (142/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 5.0 (TID 350). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 146.0 in stage 5.0 (TID 353, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 5.0 (TID 350) in 46 ms on localhost (executor driver) (143/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 146.0 in stage 5.0 (TID 353)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 5.0 (TID 349). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 147.0 in stage 5.0 (TID 354, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 147.0 in stage 5.0 (TID 354)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 5.0 (TID 349) in 55 ms on localhost (executor driver) (144/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,270][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 5.0 (TID 351). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 148.0 in stage 5.0 (TID 355, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 148.0 in stage 5.0 (TID 355)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 5.0 (TID 351) in 64 ms on localhost (executor driver) (145/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 5.0 (TID 352). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 149.0 in stage 5.0 (TID 356, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 5.0 (TID 352) in 55 ms on localhost (executor driver) (146/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 149.0 in stage 5.0 (TID 356)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 5.0 (TID 354). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 5.0 (TID 353). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 150.0 in stage 5.0 (TID 357, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 150.0 in stage 5.0 (TID 357)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 151.0 in stage 5.0 (TID 358, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 5.0 (TID 354) in 46 ms on localhost (executor driver) (147/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 151.0 in stage 5.0 (TID 358)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 5.0 (TID 353) in 55 ms on localhost (executor driver) (148/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 5.0 (TID 355). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 152.0 in stage 5.0 (TID 359, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 152.0 in stage 5.0 (TID 359)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 5.0 (TID 355) in 55 ms on localhost (executor driver) (149/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 5.0 (TID 358). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 5.0 (TID 357). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 153.0 in stage 5.0 (TID 360, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 154.0 in stage 5.0 (TID 361, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 153.0 in stage 5.0 (TID 360)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 5.0 (TID 358) in 48 ms on localhost (executor driver) (150/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 154.0 in stage 5.0 (TID 361)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 5.0 (TID 357) in 49 ms on localhost (executor driver) (151/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 5.0 (TID 356). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 155.0 in stage 5.0 (TID 362, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 5.0 (TID 356) in 66 ms on localhost (executor driver) (152/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 155.0 in stage 5.0 (TID 362)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,363][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 5.0 (TID 359). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 156.0 in stage 5.0 (TID 363, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 5.0 (TID 359) in 56 ms on localhost (executor driver) (153/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 156.0 in stage 5.0 (TID 363)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 5.0 (TID 361). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 5.0 (TID 360). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 157.0 in stage 5.0 (TID 364, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 157.0 in stage 5.0 (TID 364)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 158.0 in stage 5.0 (TID 365, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 5.0 (TID 361) in 51 ms on localhost (executor driver) (154/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 158.0 in stage 5.0 (TID 365)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 5.0 (TID 360) in 52 ms on localhost (executor driver) (155/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,400][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 5.0 (TID 362). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 159.0 in stage 5.0 (TID 366, localhost, executor driver, partition 159, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 5.0 (TID 362) in 56 ms on localhost (executor driver) (156/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 159.0 in stage 5.0 (TID 366)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 5.0 (TID 363). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 5.0 (TID 364). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 160.0 in stage 5.0 (TID 367, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 160.0 in stage 5.0 (TID 367)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 5.0 (TID 363) in 60 ms on localhost (executor driver) (157/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 161.0 in stage 5.0 (TID 368, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 5.0 (TID 364) in 45 ms on localhost (executor driver) (158/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 161.0 in stage 5.0 (TID 368)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 5.0 (TID 365). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 162.0 in stage 5.0 (TID 369, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 5.0 (TID 365) in 54 ms on localhost (executor driver) (159/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 162.0 in stage 5.0 (TID 369)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 5.0 (TID 366). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 163.0 in stage 5.0 (TID 370, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 5.0 (TID 366) in 56 ms on localhost (executor driver) (160/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 163.0 in stage 5.0 (TID 370)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 5.0 (TID 367). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 164.0 in stage 5.0 (TID 371, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 5.0 (TID 367) in 43 ms on localhost (executor driver) (161/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 164.0 in stage 5.0 (TID 371)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,489][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 5.0 (TID 369). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 5.0 (TID 368). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 165.0 in stage 5.0 (TID 372, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 166.0 in stage 5.0 (TID 373, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 165.0 in stage 5.0 (TID 372)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 5.0 (TID 369) in 48 ms on localhost (executor driver) (162/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 166.0 in stage 5.0 (TID 373)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 5.0 (TID 368) in 57 ms on localhost (executor driver) (163/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 5.0 (TID 370). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 167.0 in stage 5.0 (TID 374, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 5.0 (TID 370) in 48 ms on localhost (executor driver) (164/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 167.0 in stage 5.0 (TID 374)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 5.0 (TID 371). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 168.0 in stage 5.0 (TID 375, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 5.0 (TID 371) in 46 ms on localhost (executor driver) (165/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 168.0 in stage 5.0 (TID 375)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 5.0 (TID 372). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 169.0 in stage 5.0 (TID 376, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 5.0 (TID 372) in 40 ms on localhost (executor driver) (166/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 169.0 in stage 5.0 (TID 376)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,542][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 5.0 (TID 374). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 170.0 in stage 5.0 (TID 377, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 5.0 (TID 374) in 70 ms on localhost (executor driver) (167/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 170.0 in stage 5.0 (TID 377)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 5.0 (TID 376). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 5.0 (TID 373). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 171.0 in stage 5.0 (TID 378, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 171.0 in stage 5.0 (TID 378)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 172.0 in stage 5.0 (TID 379, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 5.0 (TID 376) in 57 ms on localhost (executor driver) (168/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 172.0 in stage 5.0 (TID 379)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 5.0 (TID 373) in 97 ms on localhost (executor driver) (169/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 5.0 (TID 375). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 173.0 in stage 5.0 (TID 380, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 5.0 (TID 375) in 85 ms on localhost (executor driver) (170/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 173.0 in stage 5.0 (TID 380)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 5.0 (TID 377). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 174.0 in stage 5.0 (TID 381, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 5.0 (TID 377) in 60 ms on localhost (executor driver) (171/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 5.0 (TID 378). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 175.0 in stage 5.0 (TID 382, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 5.0 (TID 378) in 56 ms on localhost (executor driver) (172/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 175.0 in stage 5.0 (TID 382)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 5.0 (TID 379). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 176.0 in stage 5.0 (TID 383, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 5.0 (TID 379) in 57 ms on localhost (executor driver) (173/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 176.0 in stage 5.0 (TID 383)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,658][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 174.0 in stage 5.0 (TID 381)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 5.0 (TID 380). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 177.0 in stage 5.0 (TID 384, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 5.0 (TID 380) in 77 ms on localhost (executor driver) (174/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 177.0 in stage 5.0 (TID 384)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,705][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 5.0 (TID 381). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 178.0 in stage 5.0 (TID 385, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 178.0 in stage 5.0 (TID 385)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 5.0 (TID 381) in 71 ms on localhost (executor driver) (175/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 5.0 (TID 382). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 179.0 in stage 5.0 (TID 386, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 5.0 (TID 382) in 84 ms on localhost (executor driver) (176/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 179.0 in stage 5.0 (TID 386)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 5.0 (TID 384). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 180.0 in stage 5.0 (TID 387, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 180.0 in stage 5.0 (TID 387)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 5.0 (TID 384) in 50 ms on localhost (executor driver) (177/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 5.0 (TID 383). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 181.0 in stage 5.0 (TID 388, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 5.0 (TID 383) in 92 ms on localhost (executor driver) (178/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 181.0 in stage 5.0 (TID 388)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 5.0 (TID 385). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 182.0 in stage 5.0 (TID 389, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 5.0 (TID 385) in 46 ms on localhost (executor driver) (179/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 182.0 in stage 5.0 (TID 389)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 5.0 (TID 387). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 5.0 (TID 386). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 183.0 in stage 5.0 (TID 390, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 183.0 in stage 5.0 (TID 390)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 184.0 in stage 5.0 (TID 391, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 5.0 (TID 387) in 46 ms on localhost (executor driver) (180/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 5.0 (TID 386) in 56 ms on localhost (executor driver) (181/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 184.0 in stage 5.0 (TID 391)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 5.0 (TID 388). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 185.0 in stage 5.0 (TID 392, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 5.0 (TID 388) in 65 ms on localhost (executor driver) (182/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 185.0 in stage 5.0 (TID 392)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 5.0 (TID 389). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 186.0 in stage 5.0 (TID 393, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 5.0 (TID 389) in 78 ms on localhost (executor driver) (183/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 186.0 in stage 5.0 (TID 393)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 5.0 (TID 390). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 187.0 in stage 5.0 (TID 394, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 187.0 in stage 5.0 (TID 394)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 5.0 (TID 390) in 73 ms on localhost (executor driver) (184/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 5.0 (TID 391). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 5.0 (TID 392). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 188.0 in stage 5.0 (TID 395, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 189.0 in stage 5.0 (TID 396, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 188.0 in stage 5.0 (TID 395)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 5.0 (TID 391) in 75 ms on localhost (executor driver) (185/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 189.0 in stage 5.0 (TID 396)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 5.0 (TID 392) in 53 ms on localhost (executor driver) (186/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 5.0 (TID 393). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 190.0 in stage 5.0 (TID 397, localhost, executor driver, partition 190, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 5.0 (TID 393) in 62 ms on localhost (executor driver) (187/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 190.0 in stage 5.0 (TID 397)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,903][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 5.0 (TID 394). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 191.0 in stage 5.0 (TID 398, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 5.0 (TID 394) in 49 ms on localhost (executor driver) (188/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 191.0 in stage 5.0 (TID 398)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 5.0 (TID 396). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,919][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 192.0 in stage 5.0 (TID 399, localhost, executor driver, partition 192, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 5.0 (TID 396) in 59 ms on localhost (executor driver) (189/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 192.0 in stage 5.0 (TID 399)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,925][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,925][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 2 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 5.0 (TID 395). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 193.0 in stage 5.0 (TID 400, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 5.0 (TID 395) in 70 ms on localhost (executor driver) (190/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 193.0 in stage 5.0 (TID 400)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,969][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 5.0 (TID 397). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 194.0 in stage 5.0 (TID 401, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 5.0 (TID 399). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 5.0 (TID 397) in 71 ms on localhost (executor driver) (191/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 194.0 in stage 5.0 (TID 401)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 195.0 in stage 5.0 (TID 402, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 5.0 (TID 399) in 53 ms on localhost (executor driver) (192/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 195.0 in stage 5.0 (TID 402)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 5.0 (TID 400). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 196.0 in stage 5.0 (TID 403, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 5.0 (TID 400) in 50 ms on localhost (executor driver) (193/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 196.0 in stage 5.0 (TID 403)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 5.0 (TID 398). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 197.0 in stage 5.0 (TID 404, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 5.0 (TID 398) in 73 ms on localhost (executor driver) (194/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 197.0 in stage 5.0 (TID 404)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:58,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 5.0 (TID 402). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 198.0 in stage 5.0 (TID 405, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 5.0 (TID 402) in 62 ms on localhost (executor driver) (195/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 198.0 in stage 5.0 (TID 405)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 5.0 (TID 403). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 199.0 in stage 5.0 (TID 406, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 5.0 (TID 403) in 58 ms on localhost (executor driver) (196/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 199.0 in stage 5.0 (TID 406)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 5.0 (TID 401). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 5.0 (TID 401) in 68 ms on localhost (executor driver) (197/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 2 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 5.0 (TID 404). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 5.0 (TID 404) in 65 ms on localhost (executor driver) (198/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198/2.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 5.0 (TID 406). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 5.0 (TID 406) in 63 ms on localhost (executor driver) (199/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198]
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 5.0 (TID 405). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 5.0 (TID 405) in 84 ms on localhost (executor driver) (200/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 5.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 5 (start at StreamingFile.scala:61) finished in 4.218 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 3 finished: start at StreamingFile.scala:61, took 4.302804 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 4 (start at StreamingFile.scala:61) with 1 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 6 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 6 (MapPartitionsRDD[27] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_12 stored as values in memory (estimated size 8.7 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_12_piece0 in memory on 192.168.216.37:63892 (size: 4.6 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 6.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 6.0 (TID 407, localhost, executor driver, partition 0, PROCESS_LOCAL, 6076 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 6.0 (TID 407)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 6.0 (TID 407). 1087 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 6.0 (TID 407) in 5 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 6.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 6 (start at StreamingFile.scala:61) finished in 0.005 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 4 finished: start at StreamingFile.scala:61, took 0.014530 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 5 (start at StreamingFile.scala:61) with 3 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 7 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 7 (MapPartitionsRDD[27] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_13 stored as values in memory (estimated size 8.7 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_13_piece0 in memory on 192.168.216.37:63892 (size: 4.6 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(1, 2, 3))
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 7.0 with 3 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 7.0 (TID 408, localhost, executor driver, partition 1, PROCESS_LOCAL, 6139 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 7.0 (TID 409, localhost, executor driver, partition 2, PROCESS_LOCAL, 6133 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 7.0 (TID 410, localhost, executor driver, partition 3, PROCESS_LOCAL, 6136 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 7.0 (TID 409)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 7.0 (TID 408)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 7.0 (TID 410)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,173][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 7.0 (TID 409). 1116 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 7.0 (TID 409) in 5 ms on localhost (executor driver) (1/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 7.0 (TID 410). 1120 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 7.0 (TID 410) in 4 ms on localhost (executor driver) (2/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 7.0 (TID 408). 1111 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 7.0 (TID 408) in 8 ms on localhost (executor driver) (3/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 7.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 7 (start at StreamingFile.scala:61) finished in 0.008 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 5 finished: start at StreamingFile.scala:61, took 0.021185 s
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:03:54.004Z",
  "numInputRows" : 200,
  "inputRowsPerSecond" : 99.90009990009992,
  "processedRowsPerSecond" : 38.60258637328701,
  "durationMs" : {
    "addBatch" : 5001,
    "getBatch" : 37,
    "getOffset" : 60,
    "queryPlanning" : 15,
    "triggerExecution" : 5181,
    "walCommit" : 59
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 15
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 200,
    "inputRowsPerSecond" : 99.90009990009992,
    "processedRowsPerSecond" : 38.60258637328701
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[31m[WARN ][0;39m [35m[2017-11-03 08:03:59,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 5237 milliseconds
[34m[INFO ][0;39m [35m[2017-11-03 08:03:59,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:03:59.241Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:04:10,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Log offset set to 2 with 7 new files
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1509703452049,Map(spark.sql.shuffle.partitions -> 200))
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Processing 7 files from 2:2
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<carrier: string, marital_status: string>
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_14 stored as values in memory (estimated size 221.7 KB, free 911.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.3 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_14_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 14 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 7352028 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_15 stored as values in memory (estimated size 220.5 KB, free 911.0 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.0 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_15_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 15 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_16 stored as values in memory (estimated size 220.5 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.7 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,335][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_16_piece0 in memory on 192.168.216.37:63892 (size: 20.7 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 16 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering RDD 30 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 6 (start at StreamingFile.scala:61) with 200 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 9 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 8)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 8)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 8 (MapPartitionsRDD[30] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_17 stored as values in memory (estimated size 27.5 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_17_piece0 stored as bytes in memory (estimated size 13.1 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_17_piece0 in memory on 192.168.216.37:63892 (size: 13.1 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 17 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[30] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 8.0 with 4 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 8.0 (TID 411, localhost, executor driver, partition 0, PROCESS_LOCAL, 5464 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 8.0 (TID 412, localhost, executor driver, partition 1, PROCESS_LOCAL, 5464 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 8.0 (TID 413, localhost, executor driver, partition 2, PROCESS_LOCAL, 5465 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 8.0 (TID 414, localhost, executor driver, partition 3, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 8.0 (TID 412)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 8.0 (TID 413)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 8.0 (TID 411)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 8.0 (TID 414)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.7.csv, range: 0-6975, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.4.csv, range: 0-6819, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.5.csv, range: 0-6897, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.8.csv, range: 0-6714, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.6.csv, range: 0-6934, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.10.csv, range: 0-6789, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.9.csv, range: 0-6858, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 8.0 (TID 414). 2264 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 8.0 (TID 414) in 43 ms on localhost (executor driver) (1/4)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 8.0 (TID 411). 2264 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,434][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 8.0 (TID 411) in 53 ms on localhost (executor driver) (2/4)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 8.0 (TID 412). 2264 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 8.0 (TID 413). 2264 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 8.0 (TID 412) in 59 ms on localhost (executor driver) (3/4)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 8.0 (TID 413) in 59 ms on localhost (executor driver) (4/4)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 8.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ShuffleMapStage 8 (start at StreamingFile.scala:61) finished in 0.060 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 9)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 9 (MapPartitionsRDD[37] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_18 stored as values in memory (estimated size 52.9 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_18_piece0 stored as bytes in memory (estimated size 22.0 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_18_piece0 in memory on 192.168.216.37:63892 (size: 22.0 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 18 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 200 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 9.0 with 200 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 9.0 (TID 415, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 9.0 (TID 416, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 9.0 (TID 417, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 9.0 (TID 418, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 9.0 (TID 416)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 9.0 (TID 415)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 9.0 (TID 417)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 9.0 (TID 418)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/0]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/3]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/2]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 9.0 (TID 415). 3783 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 9.0 (TID 418). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 9.0 (TID 417). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 4.0 in stage 9.0 (TID 419, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 5.0 in stage 9.0 (TID 420, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 4.0 in stage 9.0 (TID 419)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 5.0 in stage 9.0 (TID 420)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 9.0 (TID 415) in 52 ms on localhost (executor driver) (1/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/1]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 9.0 (TID 418) in 52 ms on localhost (executor driver) (2/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 6.0 in stage 9.0 (TID 421, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 9.0 (TID 417) in 54 ms on localhost (executor driver) (3/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 6.0 in stage 9.0 (TID 421)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 9.0 (TID 416). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 7.0 in stage 9.0 (TID 422, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 9.0 (TID 416) in 55 ms on localhost (executor driver) (4/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 7.0 in stage 9.0 (TID 422)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,558][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/5]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,566][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 9.0 (TID 420). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,566][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 8.0 in stage 9.0 (TID 423, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 8.0 in stage 9.0 (TID 423)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 9.0 (TID 420) in 39 ms on localhost (executor driver) (5/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/4]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 9.0 (TID 419). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 9.0 in stage 9.0 (TID 424, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 9.0 (TID 419) in 43 ms on localhost (executor driver) (6/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 9.0 in stage 9.0 (TID 424)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/6]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 9.0 (TID 421). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 10.0 in stage 9.0 (TID 425, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 9.0 (TID 421) in 44 ms on localhost (executor driver) (7/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 10.0 in stage 9.0 (TID 425)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/7]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 9.0 (TID 422). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 11.0 in stage 9.0 (TID 426, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 9.0 (TID 422) in 55 ms on localhost (executor driver) (8/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 11.0 in stage 9.0 (TID 426)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,608][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/8]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,608][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,608][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 9.0 (TID 423). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 12.0 in stage 9.0 (TID 427, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 12.0 in stage 9.0 (TID 427)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 9.0 (TID 423) in 43 ms on localhost (executor driver) (9/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/9]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 9.0 (TID 424). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 13.0 in stage 9.0 (TID 428, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 9.0 (TID 424) in 48 ms on localhost (executor driver) (10/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 13.0 in stage 9.0 (TID 428)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/10]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 9.0 (TID 425). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 14.0 in stage 9.0 (TID 429, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 9.0 (TID 425) in 53 ms on localhost (executor driver) (11/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 14.0 in stage 9.0 (TID 429)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,631][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/11]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 9.0 (TID 426). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 15.0 in stage 9.0 (TID 430, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 9.0 (TID 426) in 48 ms on localhost (executor driver) (12/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 15.0 in stage 9.0 (TID 430)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/12]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 9.0 (TID 427). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 16.0 in stage 9.0 (TID 431, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 16.0 in stage 9.0 (TID 431)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 9.0 (TID 427) in 47 ms on localhost (executor driver) (13/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/13]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 9.0 (TID 428). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 17.0 in stage 9.0 (TID 432, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 9.0 (TID 428) in 56 ms on localhost (executor driver) (14/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 17.0 in stage 9.0 (TID 432)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,677][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/15]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,677][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 9.0 (TID 430). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 18.0 in stage 9.0 (TID 433, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 9.0 (TID 430) in 52 ms on localhost (executor driver) (15/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 18.0 in stage 9.0 (TID 433)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/14]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 9.0 (TID 429). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 19.0 in stage 9.0 (TID 434, localhost, executor driver, partition 19, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 9.0 (TID 429) in 66 ms on localhost (executor driver) (16/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,691][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 19.0 in stage 9.0 (TID 434)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/16]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 9.0 (TID 431). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,714][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 20.0 in stage 9.0 (TID 435, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 9.0 (TID 431) in 60 ms on localhost (executor driver) (17/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 20.0 in stage 9.0 (TID 435)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/18]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 9.0 (TID 433). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 21.0 in stage 9.0 (TID 436, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 21.0 in stage 9.0 (TID 436)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 9.0 (TID 433) in 49 ms on localhost (executor driver) (18/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/19]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/17]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 9.0 (TID 434). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 9.0 (TID 432). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 22.0 in stage 9.0 (TID 437, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 22.0 in stage 9.0 (TID 437)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 23.0 in stage 9.0 (TID 438, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 9.0 (TID 434) in 51 ms on localhost (executor driver) (19/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 23.0 in stage 9.0 (TID 438)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 9.0 (TID 432) in 69 ms on localhost (executor driver) (20/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,743][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/20]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 9.0 (TID 435). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 24.0 in stage 9.0 (TID 439, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 9.0 (TID 435) in 65 ms on localhost (executor driver) (21/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 24.0 in stage 9.0 (TID 439)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/22]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 9.0 (TID 437). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 25.0 in stage 9.0 (TID 440, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 25.0 in stage 9.0 (TID 440)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 9.0 (TID 437) in 68 ms on localhost (executor driver) (22/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/21]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 9.0 (TID 436). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 26.0 in stage 9.0 (TID 441, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 26.0 in stage 9.0 (TID 441)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,810][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 9.0 (TID 436) in 79 ms on localhost (executor driver) (23/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/24]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 9.0 (TID 439). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 27.0 in stage 9.0 (TID 442, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 9.0 (TID 439) in 57 ms on localhost (executor driver) (24/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 27.0 in stage 9.0 (TID 442)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/23]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 9.0 (TID 438). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 28.0 in stage 9.0 (TID 443, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,845][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 9.0 (TID 438) in 104 ms on localhost (executor driver) (25/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 28.0 in stage 9.0 (TID 443)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/25]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 9.0 (TID 440). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 29.0 in stage 9.0 (TID 444, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 9.0 (TID 440) in 53 ms on localhost (executor driver) (26/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 29.0 in stage 9.0 (TID 444)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/26]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 9.0 (TID 441). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 30.0 in stage 9.0 (TID 445, localhost, executor driver, partition 30, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 30.0 in stage 9.0 (TID 445)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 9.0 (TID 441) in 56 ms on localhost (executor driver) (27/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/27]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,903][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 9.0 (TID 442). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,903][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 31.0 in stage 9.0 (TID 446, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 9.0 (TID 442) in 68 ms on localhost (executor driver) (28/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 31.0 in stage 9.0 (TID 446)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,910][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/28]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,928][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 9.0 (TID 443). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 32.0 in stage 9.0 (TID 447, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 9.0 (TID 443) in 84 ms on localhost (executor driver) (29/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 32.0 in stage 9.0 (TID 447)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,932][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/30]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 9.0 (TID 445). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 33.0 in stage 9.0 (TID 448, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 33.0 in stage 9.0 (TID 448)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 9.0 (TID 445) in 91 ms on localhost (executor driver) (30/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/29]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 9.0 (TID 444). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 34.0 in stage 9.0 (TID 449, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 34.0 in stage 9.0 (TID 449)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 9.0 (TID 444) in 98 ms on localhost (executor driver) (31/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/31]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 9.0 (TID 446). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 35.0 in stage 9.0 (TID 450, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 9.0 (TID 446) in 70 ms on localhost (executor driver) (32/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 35.0 in stage 9.0 (TID 450)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,983][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/32]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:12,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 9.0 (TID 447). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 36.0 in stage 9.0 (TID 451, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 36.0 in stage 9.0 (TID 451)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 9.0 (TID 447) in 74 ms on localhost (executor driver) (33/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/33]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 9.0 (TID 448). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 37.0 in stage 9.0 (TID 452, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 9.0 (TID 448) in 69 ms on localhost (executor driver) (34/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 37.0 in stage 9.0 (TID 452)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/34]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/35]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 9.0 (TID 449). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 9.0 (TID 450). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 38.0 in stage 9.0 (TID 453, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 38.0 in stage 9.0 (TID 453)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 39.0 in stage 9.0 (TID 454, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 9.0 (TID 449) in 77 ms on localhost (executor driver) (35/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 9.0 (TID 450) in 64 ms on localhost (executor driver) (36/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 39.0 in stage 9.0 (TID 454)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/36]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 9.0 (TID 451). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 40.0 in stage 9.0 (TID 455, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 9.0 (TID 451) in 55 ms on localhost (executor driver) (37/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 40.0 in stage 9.0 (TID 455)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/37]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 9.0 (TID 452). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 41.0 in stage 9.0 (TID 456, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 9.0 (TID 452) in 55 ms on localhost (executor driver) (38/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 41.0 in stage 9.0 (TID 456)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/39]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/38]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 9.0 (TID 454). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 9.0 (TID 453). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 42.0 in stage 9.0 (TID 457, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 43.0 in stage 9.0 (TID 458, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 9.0 (TID 454) in 55 ms on localhost (executor driver) (39/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 42.0 in stage 9.0 (TID 457)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 43.0 in stage 9.0 (TID 458)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 9.0 (TID 453) in 55 ms on localhost (executor driver) (40/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,099][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/40]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 9.0 (TID 455). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 44.0 in stage 9.0 (TID 459, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 9.0 (TID 455) in 54 ms on localhost (executor driver) (41/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 44.0 in stage 9.0 (TID 459)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/41]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 9.0 (TID 456). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 45.0 in stage 9.0 (TID 460, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 45.0 in stage 9.0 (TID 460)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 9.0 (TID 456) in 55 ms on localhost (executor driver) (42/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/43]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 9.0 (TID 458). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 46.0 in stage 9.0 (TID 461, localhost, executor driver, partition 46, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 9.0 (TID 458) in 51 ms on localhost (executor driver) (43/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 46.0 in stage 9.0 (TID 461)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/42]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 9.0 (TID 457). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 47.0 in stage 9.0 (TID 462, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 9.0 (TID 457) in 82 ms on localhost (executor driver) (44/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 47.0 in stage 9.0 (TID 462)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/45]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 9.0 (TID 460). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 48.0 in stage 9.0 (TID 463, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/44]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 9.0 (TID 459). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 48.0 in stage 9.0 (TID 463)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,232][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 9.0 (TID 460) in 113 ms on localhost (executor driver) (45/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,232][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 49.0 in stage 9.0 (TID 464, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 49.0 in stage 9.0 (TID 464)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 9.0 (TID 459) in 124 ms on localhost (executor driver) (46/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/46]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_13_piece0 on 192.168.216.37:63892 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 229
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 278
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 9.0 (TID 461). 3780 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 50.0 in stage 9.0 (TID 465, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_12_piece0 on 192.168.216.37:63892 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 9.0 (TID 461) in 106 ms on localhost (executor driver) (47/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 50.0 in stage 9.0 (TID 465)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_17_piece0 on 192.168.216.37:63892 in memory (size: 13.1 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/47]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 9.0 (TID 462). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 51.0 in stage 9.0 (TID 466, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 9.0 (TID 462) in 105 ms on localhost (executor driver) (48/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 51.0 in stage 9.0 (TID 466)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/48]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/49]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 9.0 (TID 463). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 9.0 (TID 464). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 52.0 in stage 9.0 (TID 467, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 53.0 in stage 9.0 (TID 468, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 52.0 in stage 9.0 (TID 467)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 53.0 in stage 9.0 (TID 468)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 9.0 (TID 464) in 52 ms on localhost (executor driver) (49/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 9.0 (TID 463) in 54 ms on localhost (executor driver) (50/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/50]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 9.0 (TID 465). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 54.0 in stage 9.0 (TID 469, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 9.0 (TID 465) in 72 ms on localhost (executor driver) (51/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 54.0 in stage 9.0 (TID 469)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/51]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 9.0 (TID 466). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 55.0 in stage 9.0 (TID 470, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 55.0 in stage 9.0 (TID 470)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 9.0 (TID 466) in 73 ms on localhost (executor driver) (52/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/53]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 9.0 (TID 468). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 56.0 in stage 9.0 (TID 471, localhost, executor driver, partition 56, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 56.0 in stage 9.0 (TID 471)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 9.0 (TID 468) in 68 ms on localhost (executor driver) (53/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/52]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 9.0 (TID 467). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 57.0 in stage 9.0 (TID 472, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 9.0 (TID 467) in 71 ms on localhost (executor driver) (54/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 57.0 in stage 9.0 (TID 472)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/54]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 9.0 (TID 469). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 58.0 in stage 9.0 (TID 473, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 58.0 in stage 9.0 (TID 473)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 9.0 (TID 469) in 69 ms on localhost (executor driver) (55/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/57]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 9.0 (TID 472). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 59.0 in stage 9.0 (TID 474, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 59.0 in stage 9.0 (TID 474)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 9.0 (TID 472) in 46 ms on localhost (executor driver) (56/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/55]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 9.0 (TID 470). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 60.0 in stage 9.0 (TID 475, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 60.0 in stage 9.0 (TID 475)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 9.0 (TID 470) in 60 ms on localhost (executor driver) (57/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/56]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 9.0 (TID 471). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 61.0 in stage 9.0 (TID 476, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 61.0 in stage 9.0 (TID 476)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 9.0 (TID 471) in 61 ms on localhost (executor driver) (58/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,434][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/58]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 9.0 (TID 473). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 62.0 in stage 9.0 (TID 477, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 62.0 in stage 9.0 (TID 477)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 9.0 (TID 473) in 58 ms on localhost (executor driver) (59/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/59]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 9.0 (TID 474). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 63.0 in stage 9.0 (TID 478, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 9.0 (TID 474) in 63 ms on localhost (executor driver) (60/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 63.0 in stage 9.0 (TID 478)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,487][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/61]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 9.0 (TID 476). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 64.0 in stage 9.0 (TID 479, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 9.0 (TID 476) in 76 ms on localhost (executor driver) (61/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,488][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 64.0 in stage 9.0 (TID 479)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/62]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 9.0 (TID 477). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 65.0 in stage 9.0 (TID 480, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 9.0 (TID 477) in 65 ms on localhost (executor driver) (62/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 65.0 in stage 9.0 (TID 480)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,509][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/60]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 9.0 (TID 475). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 66.0 in stage 9.0 (TID 481, localhost, executor driver, partition 66, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 9.0 (TID 475) in 104 ms on localhost (executor driver) (63/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 66.0 in stage 9.0 (TID 481)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/63]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 9.0 (TID 478). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 67.0 in stage 9.0 (TID 482, localhost, executor driver, partition 67, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 67.0 in stage 9.0 (TID 482)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 9.0 (TID 478) in 80 ms on localhost (executor driver) (64/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/64]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 9.0 (TID 479). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 68.0 in stage 9.0 (TID 483, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 68.0 in stage 9.0 (TID 483)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 9.0 (TID 479) in 73 ms on localhost (executor driver) (65/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/66]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 9.0 (TID 481). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 69.0 in stage 9.0 (TID 484, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,575][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 69.0 in stage 9.0 (TID 484)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,575][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 9.0 (TID 481) in 65 ms on localhost (executor driver) (66/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/65]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 9.0 (TID 480). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 70.0 in stage 9.0 (TID 485, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 70.0 in stage 9.0 (TID 485)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 9.0 (TID 480) in 84 ms on localhost (executor driver) (67/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/67]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,609][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 9.0 (TID 482). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 71.0 in stage 9.0 (TID 486, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 9.0 (TID 482) in 70 ms on localhost (executor driver) (68/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 71.0 in stage 9.0 (TID 486)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/68]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 9.0 (TID 483). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 72.0 in stage 9.0 (TID 487, localhost, executor driver, partition 72, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 72.0 in stage 9.0 (TID 487)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 9.0 (TID 483) in 55 ms on localhost (executor driver) (69/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/69]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 9.0 (TID 484). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 73.0 in stage 9.0 (TID 488, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 9.0 (TID 484) in 63 ms on localhost (executor driver) (70/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 73.0 in stage 9.0 (TID 488)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/70]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 9.0 (TID 485). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 74.0 in stage 9.0 (TID 489, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/71]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 9.0 (TID 485) in 92 ms on localhost (executor driver) (71/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 74.0 in stage 9.0 (TID 489)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 9.0 (TID 486). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 75.0 in stage 9.0 (TID 490, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 9.0 (TID 486) in 75 ms on localhost (executor driver) (72/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 75.0 in stage 9.0 (TID 490)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/72]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,699][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 9.0 (TID 487). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,699][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 76.0 in stage 9.0 (TID 491, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,700][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 76.0 in stage 9.0 (TID 491)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,700][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 9.0 (TID 487) in 85 ms on localhost (executor driver) (73/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,704][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,704][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/73]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 9.0 (TID 488). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 77.0 in stage 9.0 (TID 492, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 9.0 (TID 488) in 99 ms on localhost (executor driver) (74/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 77.0 in stage 9.0 (TID 492)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,736][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/74]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 9.0 (TID 489). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/75]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 78.0 in stage 9.0 (TID 493, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 78.0 in stage 9.0 (TID 493)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 9.0 (TID 489) in 66 ms on localhost (executor driver) (75/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 9.0 (TID 490). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 79.0 in stage 9.0 (TID 494, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 79.0 in stage 9.0 (TID 494)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 9.0 (TID 490) in 65 ms on localhost (executor driver) (76/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,752][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/76]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 9.0 (TID 491). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 80.0 in stage 9.0 (TID 495, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 9.0 (TID 491) in 56 ms on localhost (executor driver) (77/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 80.0 in stage 9.0 (TID 495)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/77]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 9.0 (TID 492). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 81.0 in stage 9.0 (TID 496, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 81.0 in stage 9.0 (TID 496)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 9.0 (TID 492) in 55 ms on localhost (executor driver) (78/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/79]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 9.0 (TID 494). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 82.0 in stage 9.0 (TID 497, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 82.0 in stage 9.0 (TID 497)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 9.0 (TID 494) in 47 ms on localhost (executor driver) (79/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/78]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 9.0 (TID 493). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 83.0 in stage 9.0 (TID 498, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 9.0 (TID 493) in 54 ms on localhost (executor driver) (80/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 83.0 in stage 9.0 (TID 498)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,822][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,822][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/80]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 9.0 (TID 495). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 84.0 in stage 9.0 (TID 499, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 9.0 (TID 495) in 70 ms on localhost (executor driver) (81/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 84.0 in stage 9.0 (TID 499)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/81]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 9.0 (TID 496). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 85.0 in stage 9.0 (TID 500, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 85.0 in stage 9.0 (TID 500)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 9.0 (TID 496) in 65 ms on localhost (executor driver) (82/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/82]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/83]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 9.0 (TID 497). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 86.0 in stage 9.0 (TID 501, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 9.0 (TID 498). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 86.0 in stage 9.0 (TID 501)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 9.0 (TID 497) in 69 ms on localhost (executor driver) (83/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 87.0 in stage 9.0 (TID 502, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 9.0 (TID 498) in 63 ms on localhost (executor driver) (84/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 87.0 in stage 9.0 (TID 502)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,874][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/84]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 9.0 (TID 499). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 88.0 in stage 9.0 (TID 503, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 9.0 (TID 499) in 53 ms on localhost (executor driver) (85/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 88.0 in stage 9.0 (TID 503)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/85]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 9.0 (TID 500). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 89.0 in stage 9.0 (TID 504, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 89.0 in stage 9.0 (TID 504)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 9.0 (TID 500) in 71 ms on localhost (executor driver) (86/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/86]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,934][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 9.0 (TID 501). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 90.0 in stage 9.0 (TID 505, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 90.0 in stage 9.0 (TID 505)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 9.0 (TID 501) in 72 ms on localhost (executor driver) (87/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/87]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 9.0 (TID 502). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 91.0 in stage 9.0 (TID 506, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 9.0 (TID 502) in 77 ms on localhost (executor driver) (88/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 91.0 in stage 9.0 (TID 506)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/88]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 9.0 (TID 503). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 92.0 in stage 9.0 (TID 507, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 92.0 in stage 9.0 (TID 507)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 9.0 (TID 503) in 104 ms on localhost (executor driver) (89/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/89]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 9.0 (TID 504). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 93.0 in stage 9.0 (TID 508, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 93.0 in stage 9.0 (TID 508)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 9.0 (TID 504) in 66 ms on localhost (executor driver) (90/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,994][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/90]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 9.0 (TID 505). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 94.0 in stage 9.0 (TID 509, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 9.0 (TID 505) in 62 ms on localhost (executor driver) (91/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:13,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 94.0 in stage 9.0 (TID 509)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/91]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 9.0 (TID 506). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 95.0 in stage 9.0 (TID 510, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 9.0 (TID 506) in 79 ms on localhost (executor driver) (92/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 95.0 in stage 9.0 (TID 510)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/92]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 9.0 (TID 507). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 96.0 in stage 9.0 (TID 511, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 9.0 (TID 507) in 58 ms on localhost (executor driver) (93/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 96.0 in stage 9.0 (TID 511)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/93]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 9.0 (TID 508). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 97.0 in stage 9.0 (TID 512, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 97.0 in stage 9.0 (TID 512)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 9.0 (TID 508) in 59 ms on localhost (executor driver) (94/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,051][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,055][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/94]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 9.0 (TID 509). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 98.0 in stage 9.0 (TID 513, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 9.0 (TID 509) in 59 ms on localhost (executor driver) (95/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 98.0 in stage 9.0 (TID 513)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/95]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 9.0 (TID 510). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 99.0 in stage 9.0 (TID 514, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 99.0 in stage 9.0 (TID 514)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 9.0 (TID 510) in 50 ms on localhost (executor driver) (96/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,080][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/96]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 9.0 (TID 511). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 100.0 in stage 9.0 (TID 515, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 100.0 in stage 9.0 (TID 515)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 9.0 (TID 511) in 56 ms on localhost (executor driver) (97/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/97]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 9.0 (TID 512). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 101.0 in stage 9.0 (TID 516, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 9.0 (TID 512) in 59 ms on localhost (executor driver) (98/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 101.0 in stage 9.0 (TID 516)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,110][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/99]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/98]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 9.0 (TID 514). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 9.0 (TID 513). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 102.0 in stage 9.0 (TID 517, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 9.0 (TID 514) in 53 ms on localhost (executor driver) (99/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 103.0 in stage 9.0 (TID 518, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 102.0 in stage 9.0 (TID 517)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 9.0 (TID 513) in 64 ms on localhost (executor driver) (100/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 103.0 in stage 9.0 (TID 518)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,150][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/100]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 9.0 (TID 515). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 104.0 in stage 9.0 (TID 519, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 104.0 in stage 9.0 (TID 519)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 9.0 (TID 515) in 61 ms on localhost (executor driver) (101/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,172][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/103]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 9.0 (TID 518). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 105.0 in stage 9.0 (TID 520, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 9.0 (TID 518) in 57 ms on localhost (executor driver) (102/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 105.0 in stage 9.0 (TID 520)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,180][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,186][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/101]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 9.0 (TID 516). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 106.0 in stage 9.0 (TID 521, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 9.0 (TID 516) in 83 ms on localhost (executor driver) (103/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 106.0 in stage 9.0 (TID 521)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/104]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 9.0 (TID 519). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 107.0 in stage 9.0 (TID 522, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 107.0 in stage 9.0 (TID 522)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 9.0 (TID 519) in 48 ms on localhost (executor driver) (104/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/102]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 9.0 (TID 517). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 108.0 in stage 9.0 (TID 523, localhost, executor driver, partition 108, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 9.0 (TID 517) in 83 ms on localhost (executor driver) (105/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 108.0 in stage 9.0 (TID 523)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/105]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 9.0 (TID 520). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 109.0 in stage 9.0 (TID 524, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 109.0 in stage 9.0 (TID 524)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 9.0 (TID 520) in 65 ms on localhost (executor driver) (106/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/107]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 9.0 (TID 522). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 110.0 in stage 9.0 (TID 525, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 9.0 (TID 522) in 57 ms on localhost (executor driver) (107/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 110.0 in stage 9.0 (TID 525)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/106]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 9.0 (TID 521). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 111.0 in stage 9.0 (TID 526, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 111.0 in stage 9.0 (TID 526)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 9.0 (TID 521) in 125 ms on localhost (executor driver) (108/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,313][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/108]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 9.0 (TID 523). 3783 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 112.0 in stage 9.0 (TID 527, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 9.0 (TID 523) in 114 ms on localhost (executor driver) (109/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 112.0 in stage 9.0 (TID 527)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/112]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 9.0 (TID 527). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 113.0 in stage 9.0 (TID 528, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 113.0 in stage 9.0 (TID 528)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 9.0 (TID 527) in 66 ms on localhost (executor driver) (110/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/111]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 9.0 (TID 526). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 114.0 in stage 9.0 (TID 529, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 9.0 (TID 526) in 70 ms on localhost (executor driver) (111/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 114.0 in stage 9.0 (TID 529)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,384][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/109]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/110]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 9.0 (TID 524). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 9.0 (TID 525). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 115.0 in stage 9.0 (TID 530, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 115.0 in stage 9.0 (TID 530)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 116.0 in stage 9.0 (TID 531, localhost, executor driver, partition 116, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 9.0 (TID 524) in 146 ms on localhost (executor driver) (112/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 116.0 in stage 9.0 (TID 531)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 9.0 (TID 525) in 134 ms on localhost (executor driver) (113/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/115]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/114]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/116]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 9.0 (TID 530). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 9.0 (TID 531). 3737 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 117.0 in stage 9.0 (TID 532, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 9.0 (TID 529). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 118.0 in stage 9.0 (TID 533, localhost, executor driver, partition 118, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 117.0 in stage 9.0 (TID 532)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 119.0 in stage 9.0 (TID 534, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 118.0 in stage 9.0 (TID 533)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 9.0 (TID 530) in 59 ms on localhost (executor driver) (114/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 119.0 in stage 9.0 (TID 534)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 9.0 (TID 529) in 62 ms on localhost (executor driver) (115/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 9.0 (TID 531) in 58 ms on localhost (executor driver) (116/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/113]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 9.0 (TID 528). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 120.0 in stage 9.0 (TID 535, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 120.0 in stage 9.0 (TID 535)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 9.0 (TID 528) in 87 ms on localhost (executor driver) (117/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/118]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 9.0 (TID 533). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/117]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 121.0 in stage 9.0 (TID 536, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 9.0 (TID 532). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 121.0 in stage 9.0 (TID 536)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 122.0 in stage 9.0 (TID 537, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 122.0 in stage 9.0 (TID 537)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 9.0 (TID 533) in 78 ms on localhost (executor driver) (118/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 9.0 (TID 532) in 78 ms on localhost (executor driver) (119/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/120]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,535][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 9.0 (TID 535). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 123.0 in stage 9.0 (TID 538, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 123.0 in stage 9.0 (TID 538)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 9.0 (TID 535) in 71 ms on localhost (executor driver) (120/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,540][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/119]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 9.0 (TID 534). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 124.0 in stage 9.0 (TID 539, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 9.0 (TID 534) in 107 ms on localhost (executor driver) (121/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 124.0 in stage 9.0 (TID 539)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/121]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 9.0 (TID 536). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/122]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 125.0 in stage 9.0 (TID 540, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 125.0 in stage 9.0 (TID 540)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 9.0 (TID 536) in 67 ms on localhost (executor driver) (122/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 9.0 (TID 537). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 126.0 in stage 9.0 (TID 541, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 9.0 (TID 537) in 67 ms on localhost (executor driver) (123/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 126.0 in stage 9.0 (TID 541)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/123]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 9.0 (TID 538). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 127.0 in stage 9.0 (TID 542, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 127.0 in stage 9.0 (TID 542)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 9.0 (TID 538) in 55 ms on localhost (executor driver) (124/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/124]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 9.0 (TID 539). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 128.0 in stage 9.0 (TID 543, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 9.0 (TID 539) in 63 ms on localhost (executor driver) (125/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 128.0 in stage 9.0 (TID 543)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/127]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 9.0 (TID 542). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 129.0 in stage 9.0 (TID 544, localhost, executor driver, partition 129, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 9.0 (TID 542) in 39 ms on localhost (executor driver) (126/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 129.0 in stage 9.0 (TID 544)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/126]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/125]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 9.0 (TID 541). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 9.0 (TID 540). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 130.0 in stage 9.0 (TID 545, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 131.0 in stage 9.0 (TID 546, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 130.0 in stage 9.0 (TID 545)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 9.0 (TID 541) in 47 ms on localhost (executor driver) (127/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 131.0 in stage 9.0 (TID 546)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 9.0 (TID 540) in 49 ms on localhost (executor driver) (128/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/128]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 9.0 (TID 543). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,678][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 132.0 in stage 9.0 (TID 547, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,678][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 9.0 (TID 543) in 65 ms on localhost (executor driver) (129/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,678][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 132.0 in stage 9.0 (TID 547)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/131]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/129]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 9.0 (TID 546). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 9.0 (TID 544). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 133.0 in stage 9.0 (TID 548, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 133.0 in stage 9.0 (TID 548)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 134.0 in stage 9.0 (TID 549, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 9.0 (TID 546) in 51 ms on localhost (executor driver) (130/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 134.0 in stage 9.0 (TID 549)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 9.0 (TID 544) in 58 ms on localhost (executor driver) (131/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/130]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 9.0 (TID 545). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 135.0 in stage 9.0 (TID 550, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,704][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 135.0 in stage 9.0 (TID 550)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,704][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 9.0 (TID 545) in 70 ms on localhost (executor driver) (132/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/133]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,734][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/134]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,734][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 9.0 (TID 548). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,734][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 9.0 (TID 549). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,734][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 136.0 in stage 9.0 (TID 551, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 136.0 in stage 9.0 (TID 551)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 137.0 in stage 9.0 (TID 552, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 137.0 in stage 9.0 (TID 552)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 9.0 (TID 548) in 50 ms on localhost (executor driver) (133/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 9.0 (TID 549) in 51 ms on localhost (executor driver) (134/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/132]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 9.0 (TID 547). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 138.0 in stage 9.0 (TID 553, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 138.0 in stage 9.0 (TID 553)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 9.0 (TID 547) in 72 ms on localhost (executor driver) (135/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/135]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 9.0 (TID 550). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 139.0 in stage 9.0 (TID 554, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 9.0 (TID 550) in 62 ms on localhost (executor driver) (136/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 139.0 in stage 9.0 (TID 554)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/136]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/137]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 9.0 (TID 551). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 9.0 (TID 552). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 140.0 in stage 9.0 (TID 555, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 140.0 in stage 9.0 (TID 555)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 141.0 in stage 9.0 (TID 556, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 9.0 (TID 551) in 59 ms on localhost (executor driver) (137/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 141.0 in stage 9.0 (TID 556)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 9.0 (TID 552) in 58 ms on localhost (executor driver) (138/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,797][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,797][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/138]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 9.0 (TID 553). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 142.0 in stage 9.0 (TID 557, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 9.0 (TID 553) in 59 ms on localhost (executor driver) (139/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 142.0 in stage 9.0 (TID 557)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,822][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/139]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 9.0 (TID 554). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 143.0 in stage 9.0 (TID 558, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 9.0 (TID 554) in 59 ms on localhost (executor driver) (140/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 143.0 in stage 9.0 (TID 558)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/140]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 9.0 (TID 555). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 144.0 in stage 9.0 (TID 559, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 9.0 (TID 555) in 51 ms on localhost (executor driver) (141/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 144.0 in stage 9.0 (TID 559)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/141]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 9.0 (TID 556). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 145.0 in stage 9.0 (TID 560, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 145.0 in stage 9.0 (TID 560)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 9.0 (TID 556) in 59 ms on localhost (executor driver) (142/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/142]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 9.0 (TID 557). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 146.0 in stage 9.0 (TID 561, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 146.0 in stage 9.0 (TID 561)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 9.0 (TID 557) in 63 ms on localhost (executor driver) (143/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,874][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,874][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/143]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 9.0 (TID 558). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 147.0 in stage 9.0 (TID 562, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 147.0 in stage 9.0 (TID 562)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 9.0 (TID 558) in 62 ms on localhost (executor driver) (144/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/144]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 9.0 (TID 559). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 148.0 in stage 9.0 (TID 563, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 148.0 in stage 9.0 (TID 563)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 9.0 (TID 559) in 47 ms on localhost (executor driver) (145/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,893][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,911][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/145]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 9.0 (TID 560). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 149.0 in stage 9.0 (TID 564, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 9.0 (TID 560) in 61 ms on localhost (executor driver) (146/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 149.0 in stage 9.0 (TID 564)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,925][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/146]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 9.0 (TID 561). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,926][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 150.0 in stage 9.0 (TID 565, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 150.0 in stage 9.0 (TID 565)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 9.0 (TID 561) in 56 ms on localhost (executor driver) (147/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,929][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,930][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/147]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 9.0 (TID 562). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/148]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 151.0 in stage 9.0 (TID 566, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 9.0 (TID 563). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 151.0 in stage 9.0 (TID 566)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 9.0 (TID 562) in 57 ms on localhost (executor driver) (148/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 152.0 in stage 9.0 (TID 567, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 152.0 in stage 9.0 (TID 567)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 9.0 (TID 563) in 53 ms on localhost (executor driver) (149/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,967][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/150]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 9.0 (TID 565). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 153.0 in stage 9.0 (TID 568, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 153.0 in stage 9.0 (TID 568)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 9.0 (TID 565) in 63 ms on localhost (executor driver) (150/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,993][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/149]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 9.0 (TID 564). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 154.0 in stage 9.0 (TID 569, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 154.0 in stage 9.0 (TID 569)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:14,998][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 9.0 (TID 564) in 86 ms on localhost (executor driver) (151/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/151]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 9.0 (TID 566). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 155.0 in stage 9.0 (TID 570, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 155.0 in stage 9.0 (TID 570)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 9.0 (TID 566) in 71 ms on localhost (executor driver) (152/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/152]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 9.0 (TID 567). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 156.0 in stage 9.0 (TID 571, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 156.0 in stage 9.0 (TID 571)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 9.0 (TID 567) in 104 ms on localhost (executor driver) (153/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/154]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 9.0 (TID 569). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 157.0 in stage 9.0 (TID 572, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 157.0 in stage 9.0 (TID 572)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 9.0 (TID 569) in 52 ms on localhost (executor driver) (154/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,051][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,054][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/153]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 9.0 (TID 568). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 158.0 in stage 9.0 (TID 573, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 158.0 in stage 9.0 (TID 573)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 9.0 (TID 568) in 69 ms on localhost (executor driver) (155/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,079][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/155]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 9.0 (TID 570). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 159.0 in stage 9.0 (TID 574, localhost, executor driver, partition 159, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 159.0 in stage 9.0 (TID 574)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 9.0 (TID 570) in 69 ms on localhost (executor driver) (156/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/157]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 9.0 (TID 572). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 160.0 in stage 9.0 (TID 575, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 9.0 (TID 572) in 49 ms on localhost (executor driver) (157/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 160.0 in stage 9.0 (TID 575)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,102][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,102][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,102][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/158]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 9.0 (TID 573). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 161.0 in stage 9.0 (TID 576, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 9.0 (TID 573) in 50 ms on localhost (executor driver) (158/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 161.0 in stage 9.0 (TID 576)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/156]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 9.0 (TID 571). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 162.0 in stage 9.0 (TID 577, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 9.0 (TID 571) in 79 ms on localhost (executor driver) (159/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 162.0 in stage 9.0 (TID 577)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/159]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 9.0 (TID 574). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 163.0 in stage 9.0 (TID 578, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 9.0 (TID 574) in 52 ms on localhost (executor driver) (160/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 163.0 in stage 9.0 (TID 578)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/160]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 9.0 (TID 575). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 164.0 in stage 9.0 (TID 579, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 9.0 (TID 575) in 51 ms on localhost (executor driver) (161/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 164.0 in stage 9.0 (TID 579)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/161]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 9.0 (TID 576). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 165.0 in stage 9.0 (TID 580, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 9.0 (TID 576) in 49 ms on localhost (executor driver) (162/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,156][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 165.0 in stage 9.0 (TID 580)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,157][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/162]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 9.0 (TID 577). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 166.0 in stage 9.0 (TID 581, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 166.0 in stage 9.0 (TID 581)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 9.0 (TID 577) in 50 ms on localhost (executor driver) (163/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,179][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/163]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 9.0 (TID 578). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 167.0 in stage 9.0 (TID 582, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 9.0 (TID 578) in 55 ms on localhost (executor driver) (164/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 167.0 in stage 9.0 (TID 582)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/165]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 9.0 (TID 580). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 168.0 in stage 9.0 (TID 583, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 168.0 in stage 9.0 (TID 583)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 9.0 (TID 580) in 56 ms on localhost (executor driver) (165/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/164]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 9.0 (TID 579). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 169.0 in stage 9.0 (TID 584, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 9.0 (TID 579) in 67 ms on localhost (executor driver) (166/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 169.0 in stage 9.0 (TID 584)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,219][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,219][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/166]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 9.0 (TID 581). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 170.0 in stage 9.0 (TID 585, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 170.0 in stage 9.0 (TID 585)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 9.0 (TID 581) in 55 ms on localhost (executor driver) (167/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/167]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 9.0 (TID 582). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 171.0 in stage 9.0 (TID 586, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 9.0 (TID 582) in 61 ms on localhost (executor driver) (168/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 171.0 in stage 9.0 (TID 586)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/169]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 9.0 (TID 584). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 172.0 in stage 9.0 (TID 587, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 9.0 (TID 584) in 59 ms on localhost (executor driver) (169/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 172.0 in stage 9.0 (TID 587)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/168]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/170]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 9.0 (TID 583). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 9.0 (TID 585). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 173.0 in stage 9.0 (TID 588, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 9.0 (TID 583) in 74 ms on localhost (executor driver) (170/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 173.0 in stage 9.0 (TID 588)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 174.0 in stage 9.0 (TID 589, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 9.0 (TID 585) in 56 ms on localhost (executor driver) (171/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 174.0 in stage 9.0 (TID 589)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/172]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 9.0 (TID 587). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/173]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 175.0 in stage 9.0 (TID 590, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 9.0 (TID 587) in 49 ms on localhost (executor driver) (172/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 9.0 (TID 588). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 175.0 in stage 9.0 (TID 590)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 176.0 in stage 9.0 (TID 591, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 176.0 in stage 9.0 (TID 591)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 9.0 (TID 588) in 40 ms on localhost (executor driver) (173/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/174]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 9.0 (TID 589). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 177.0 in stage 9.0 (TID 592, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/171]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 9.0 (TID 589) in 63 ms on localhost (executor driver) (174/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 177.0 in stage 9.0 (TID 592)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 9.0 (TID 586). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 178.0 in stage 9.0 (TID 593, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 9.0 (TID 586) in 102 ms on localhost (executor driver) (175/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 178.0 in stage 9.0 (TID 593)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/176]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 9.0 (TID 591). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 179.0 in stage 9.0 (TID 594, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 9.0 (TID 591) in 54 ms on localhost (executor driver) (176/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 179.0 in stage 9.0 (TID 594)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/175]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 9.0 (TID 590). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 180.0 in stage 9.0 (TID 595, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 9.0 (TID 590) in 66 ms on localhost (executor driver) (177/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 180.0 in stage 9.0 (TID 595)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/178]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 9.0 (TID 593). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 181.0 in stage 9.0 (TID 596, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 181.0 in stage 9.0 (TID 596)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 9.0 (TID 593) in 54 ms on localhost (executor driver) (178/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/177]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 9.0 (TID 592). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 182.0 in stage 9.0 (TID 597, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 9.0 (TID 592) in 65 ms on localhost (executor driver) (179/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,413][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 182.0 in stage 9.0 (TID 597)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/180]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 9.0 (TID 595). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 183.0 in stage 9.0 (TID 598, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 183.0 in stage 9.0 (TID 598)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 9.0 (TID 595) in 55 ms on localhost (executor driver) (180/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/181]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 9.0 (TID 596). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 184.0 in stage 9.0 (TID 599, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 184.0 in stage 9.0 (TID 599)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 9.0 (TID 596) in 68 ms on localhost (executor driver) (181/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/182]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/179]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 9.0 (TID 597). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 9.0 (TID 594). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 185.0 in stage 9.0 (TID 600, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 9.0 (TID 597) in 71 ms on localhost (executor driver) (182/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 185.0 in stage 9.0 (TID 600)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 186.0 in stage 9.0 (TID 601, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,485][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 9.0 (TID 594) in 106 ms on localhost (executor driver) (183/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 186.0 in stage 9.0 (TID 601)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,500][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/183]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,502][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 9.0 (TID 598). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 187.0 in stage 9.0 (TID 602, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 187.0 in stage 9.0 (TID 602)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 9.0 (TID 598) in 59 ms on localhost (executor driver) (184/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/184]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 9.0 (TID 599). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 188.0 in stage 9.0 (TID 603, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 188.0 in stage 9.0 (TID 603)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 9.0 (TID 599) in 48 ms on localhost (executor driver) (185/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,547][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/186]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 9.0 (TID 601). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,548][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 189.0 in stage 9.0 (TID 604, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 189.0 in stage 9.0 (TID 604)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 9.0 (TID 601) in 64 ms on localhost (executor driver) (186/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,552][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/185]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 9.0 (TID 600). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 190.0 in stage 9.0 (TID 605, localhost, executor driver, partition 190, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 190.0 in stage 9.0 (TID 605)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 9.0 (TID 600) in 81 ms on localhost (executor driver) (187/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/188]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 9.0 (TID 603). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 191.0 in stage 9.0 (TID 606, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 191.0 in stage 9.0 (TID 606)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 9.0 (TID 603) in 54 ms on localhost (executor driver) (188/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/189]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 9.0 (TID 604). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 192.0 in stage 9.0 (TID 607, localhost, executor driver, partition 192, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 192.0 in stage 9.0 (TID 607)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 9.0 (TID 604) in 55 ms on localhost (executor driver) (189/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/187]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 9.0 (TID 602). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 193.0 in stage 9.0 (TID 608, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 193.0 in stage 9.0 (TID 608)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 9.0 (TID 602) in 118 ms on localhost (executor driver) (190/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/190]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 9.0 (TID 605). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 194.0 in stage 9.0 (TID 609, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 194.0 in stage 9.0 (TID 609)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 9.0 (TID 605) in 61 ms on localhost (executor driver) (191/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/191]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 9.0 (TID 606). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 195.0 in stage 9.0 (TID 610, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 9.0 (TID 606) in 67 ms on localhost (executor driver) (192/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,639][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 195.0 in stage 9.0 (TID 610)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/192]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 9.0 (TID 607). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 196.0 in stage 9.0 (TID 611, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 9.0 (TID 607) in 59 ms on localhost (executor driver) (193/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 196.0 in stage 9.0 (TID 611)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,665][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/193]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 9.0 (TID 608). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 197.0 in stage 9.0 (TID 612, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 9.0 (TID 608) in 60 ms on localhost (executor driver) (194/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 197.0 in stage 9.0 (TID 612)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/195]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 9.0 (TID 610). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 198.0 in stage 9.0 (TID 613, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 9.0 (TID 610) in 48 ms on localhost (executor driver) (195/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 198.0 in stage 9.0 (TID 613)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/194]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,699][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 9.0 (TID 609). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,699][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 199.0 in stage 9.0 (TID 614, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,699][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 9.0 (TID 609) in 73 ms on localhost (executor driver) (196/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,700][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 199.0 in stage 9.0 (TID 614)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,705][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/196]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 9.0 (TID 611). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 9.0 (TID 611) in 47 ms on localhost (executor driver) (197/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,710][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 2 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 3 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199/3.delta
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/197]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/198]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 9.0 (TID 612). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 9.0 (TID 612) in 71 ms on localhost (executor driver) (198/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 9.0 (TID 613). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,752][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 9.0 (TID 613) in 65 ms on localhost (executor driver) (199/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,763][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 3 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80/state/0/199]
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 9.0 (TID 614). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 9.0 (TID 614) in 65 ms on localhost (executor driver) (200/200)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 9.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 9 (start at StreamingFile.scala:61) finished in 3.289 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 6 finished: start at StreamingFile.scala:61, took 3.391541 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 7 (start at StreamingFile.scala:61) with 1 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 10 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 10 (MapPartitionsRDD[41] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_19 stored as values in memory (estimated size 8.7 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.6 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_19_piece0 in memory on 192.168.216.37:63892 (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 19 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[41] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 10.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 10.0 (TID 615, localhost, executor driver, partition 0, PROCESS_LOCAL, 6076 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 10.0 (TID 615)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 10.0 (TID 615). 1087 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 10.0 (TID 615) in 4 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 10.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 10 (start at StreamingFile.scala:61) finished in 0.005 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 7 finished: start at StreamingFile.scala:61, took 0.012257 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 8 (start at StreamingFile.scala:61) with 3 output partitions
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 11 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 11 (MapPartitionsRDD[41] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_20 stored as values in memory (estimated size 8.7 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_20_piece0 in memory on 192.168.216.37:63892 (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 20 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 3 missing tasks from ResultStage 11 (MapPartitionsRDD[41] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(1, 2, 3))
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 11.0 with 3 tasks
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 11.0 (TID 616, localhost, executor driver, partition 1, PROCESS_LOCAL, 6139 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 11.0 (TID 617, localhost, executor driver, partition 2, PROCESS_LOCAL, 6133 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 11.0 (TID 618, localhost, executor driver, partition 3, PROCESS_LOCAL, 6136 bytes)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 11.0 (TID 617)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 11.0 (TID 616)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 11.0 (TID 618)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 11.0 (TID 616). 1111 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 11.0 (TID 617). 1114 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 11.0 (TID 616) in 6 ms on localhost (executor driver) (1/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 11.0 (TID 617) in 5 ms on localhost (executor driver) (2/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 11.0 (TID 618). 1120 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 11.0 (TID 618) in 7 ms on localhost (executor driver) (3/3)
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 11.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 11 (start at StreamingFile.scala:61) finished in 0.008 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 8 finished: start at StreamingFile.scala:61, took 0.014582 s
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:12.002Z",
  "numInputRows" : 700,
  "inputRowsPerSecond" : 350.35035035035037,
  "processedRowsPerSecond" : 183.15018315018315,
  "durationMs" : {
    "addBatch" : 3684,
    "getBatch" : 17,
    "getOffset" : 46,
    "queryPlanning" : 25,
    "triggerExecution" : 3821,
    "walCommit" : 41
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 15
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 2
    },
    "numInputRows" : 700,
    "inputRowsPerSecond" : 350.35035035035037,
    "processedRowsPerSecond" : 183.15018315018315
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[31m[WARN ][0;39m [35m[2017-11-03 08:04:15,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 3877 milliseconds
[34m[INFO ][0;39m [35m[2017-11-03 08:04:15,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:15.880Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 2
    },
    "endOffset" : {
      "logOffset" : 2
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:04:26,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:26.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 9,
    "triggerExecution" : 13
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 2
    },
    "endOffset" : {
      "logOffset" : 2
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:04:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:38.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 2
    },
    "endOffset" : {
      "logOffset" : 2
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:04:48,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 2
    },
    "endOffset" : {
      "logOffset" : 2
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:04:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "7d2a611d-52e5-41ef-bc4b-86e8687960cf",
  "runId" : "c65b161d-c88c-47f1-9c92-568c90dbff87",
  "name" : null,
  "timestamp" : "2017-11-03T10:04:58.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 2
    },
    "endOffset" : {
      "logOffset" : 2
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@72977986"
  }
}
[34m[INFO ][0;39m [35m[2017-11-03 08:05:03,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Invoking stop() from shutdown hook
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.216.37:4040
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-16f462c7-5456-4395-b467-0187f2f2bf80
[34m[INFO ][0;39m [35m[2017-11-03 08:05:04,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/spark-0481e42b-f5d0-43de-a506-8b5e44dc96fa
[34m[INFO ][0;39m [35m[2017-11-05 15:55:16,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running Spark version 2.2.0
[31m[WARN ][0;39m [35m[2017-11-05 15:55:17,404][0;39m [33m[][0;39m [35m[org.apache.hadoop.util.NativeCodeLoader-><clinit>][0;39m | Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[34m[INFO ][0;39m [35m[2017-11-05 15:55:17,742][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitted application: Spark Structured Streaming Job
[34m[INFO ][0;39m [35m[2017-11-05 15:55:17,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing view acls to: flavio.clesio
[34m[INFO ][0;39m [35m[2017-11-05 15:55:17,789][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing modify acls to: flavio.clesio
[34m[INFO ][0;39m [35m[2017-11-05 15:55:17,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing view acls groups to: 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:17,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Changing modify acls groups to: 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:17,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(flavio.clesio); groups with view permissions: Set(); users  with modify permissions: Set(flavio.clesio); groups with modify permissions: Set()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 55781.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManagerMasterEndpoint up
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/blockmgr-37db318f-360e-49ee-ae93-93aa0972276b
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MemoryStore started with capacity 912.3 MB
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,382][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:19,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Bound SparkUI to 0.0.0.0, and started at http://192.168.1.33:4040
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55782.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Server created on 192.168.1.33:55782
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering BlockManager BlockManagerId(driver, 192.168.1.33, 55782, None)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering block manager 192.168.1.33:55782 with 912.3 MB RAM, BlockManagerId(driver, 192.168.1.33, 55782, None)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registered BlockManager BlockManagerId(driver, 192.168.1.33, 55782, None)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Initialized BlockManager: BlockManagerId(driver, 192.168.1.33, 55782, None)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,494][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/spark-warehouse/').
[34m[INFO ][0;39m [35m[2017-11-05 15:55:20,495][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Warehouse path is 'file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/spark-warehouse/'.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:23,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registered StateStoreCoordinator endpoint
[34m[INFO ][0;39m [35m[2017-11-05 15:55:25,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parsing command: user_records
[34m[INFO ][0;39m [35m[2017-11-05 15:55:26,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parsing command: 
        SELECT carrier, marital_status, COUNT(1) as num_users
        FROM user_records
        GROUP BY carrier, marital_status
      
[34m[INFO ][0;39m [35m[2017-11-05 15:55:26,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting [id = 43b2ac4a-2ddd-45e0-a80b-646801594e28, runId = eaf727d7-0e91-4b6a-a51b-98dcd528c284]. Use /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648 to store the query checkpoint.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:26,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Set the compact interval to 10 [defaultCompactInterval: 10]
[31m[WARN ][0;39m [35m[2017-11-05 15:55:26,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | 'latestFirst' is true. New files will be processed first, which may affect the watermark
value. In addition, 'maxFileAge' will be ignored.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:26,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | maxFilesPerBatch = None, maxFileAgeMs = 604800000
[34m[INFO ][0;39m [35m[2017-11-05 15:55:26,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting new streaming query.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Log offset set to 0 with 1 new files
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1509904527152,Map(spark.sql.shuffle.partitions -> 200))
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Processing 1 files from 0:0
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<carrier: string, marital_status: string>
[34m[INFO ][0;39m [35m[2017-11-05 15:55:27,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:28,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 0
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,490][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 714.265522 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 81.603514 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 71.869747 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 221.6 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.7 KB, free 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on 192.168.1.33:55782 (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:29,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 0 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 220.5 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.8 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on 192.168.1.33:55782 (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 1 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 220.5 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,506][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on 192.168.1.33:55782 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 2 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering RDD 2 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 0 (start at StreamingFile.scala:61) with 200 output partitions
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 0)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 0)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 26.9 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.1 KB, free 911.6 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on 192.168.1.33:55782 (size: 13.1 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 3 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:30,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 156.20255 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 27.933425 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,123][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 11.966276 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 23.091271 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 12.361255 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.1.csv, range: 0-6730, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2305 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 749 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ShuffleMapStage 0 (start at StreamingFile.scala:61) finished in 0.774 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 1)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,471][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[9] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 52.3 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 21.9 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on 192.168.1.33:55782 (size: 21.9 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 200 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 1.0 with 200 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 4.0 in stage 1.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,581][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 4.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | State Store maintenance task started
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 8 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 8 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 8 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_3_piece0 on 192.168.1.33:55782 in memory (size: 13.1 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 1.0 (TID 4). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 3). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 2). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 5.0 in stage 1.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 6.0 in stage 1.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 5.0 in stage 1.0 (TID 5)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 3) in 235 ms on localhost (executor driver) (1/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 6.0 in stage 1.0 (TID 6)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 7.0 in stage 1.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 1.0 (TID 4) in 236 ms on localhost (executor driver) (2/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 2) in 237 ms on localhost (executor driver) (3/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 7.0 in stage 1.0 (TID 7)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 1). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 8.0 in stage 1.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 1) in 268 ms on localhost (executor driver) (4/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 8.0 in stage 1.0 (TID 8)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 1.0 (TID 5). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 9.0 in stage 1.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 9.0 in stage 1.0 (TID 9)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 1.0 (TID 5) in 76 ms on localhost (executor driver) (5/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 1.0 (TID 7). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 10.0 in stage 1.0 (TID 10, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 1.0 (TID 7) in 74 ms on localhost (executor driver) (6/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 10.0 in stage 1.0 (TID 10)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,894][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,907][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,915][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 1.0 (TID 6). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 11.0 in stage 1.0 (TID 11, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 1.0 (TID 6) in 128 ms on localhost (executor driver) (7/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 11.0 in stage 1.0 (TID 11)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,959][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:31,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 1.0 (TID 10). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 1.0 (TID 8). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 12.0 in stage 1.0 (TID 12, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 12.0 in stage 1.0 (TID 12)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 13.0 in stage 1.0 (TID 13, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 1.0 (TID 10) in 264 ms on localhost (executor driver) (8/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 1.0 (TID 8) in 309 ms on localhost (executor driver) (9/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 13.0 in stage 1.0 (TID 13)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 1.0 (TID 9). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 14.0 in stage 1.0 (TID 14, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 1.0 (TID 9) in 269 ms on localhost (executor driver) (10/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 14.0 in stage 1.0 (TID 14)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 1.0 (TID 11). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 15.0 in stage 1.0 (TID 15, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 15.0 in stage 1.0 (TID 15)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 1.0 (TID 11) in 255 ms on localhost (executor driver) (11/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,206][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,226][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,226][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 1.0 (TID 13). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 16.0 in stage 1.0 (TID 16, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 16.0 in stage 1.0 (TID 16)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 1.0 (TID 13) in 78 ms on localhost (executor driver) (12/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 1.0 (TID 12). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 17.0 in stage 1.0 (TID 17, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,232][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 1.0 (TID 12) in 83 ms on localhost (executor driver) (13/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 17.0 in stage 1.0 (TID 17)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,243][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 1.0 (TID 15). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 18.0 in stage 1.0 (TID 18, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 1.0 (TID 15) in 63 ms on localhost (executor driver) (14/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 18.0 in stage 1.0 (TID 18)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,287][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 1.0 (TID 14). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 20.0 in stage 1.0 (TID 19, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 1.0 (TID 14) in 148 ms on localhost (executor driver) (15/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 20.0 in stage 1.0 (TID 19)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,310][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 1.0 (TID 17). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,333][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 21.0 in stage 1.0 (TID 20, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,334][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 1.0 (TID 17) in 103 ms on localhost (executor driver) (16/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,335][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 21.0 in stage 1.0 (TID 20)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 1.0 (TID 19). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 22.0 in stage 1.0 (TID 21, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 1.0 (TID 19) in 78 ms on localhost (executor driver) (17/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 22.0 in stage 1.0 (TID 21)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,383][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 1.0 (TID 16). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,385][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 23.0 in stage 1.0 (TID 22, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 23.0 in stage 1.0 (TID 22)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 1.0 (TID 16) in 158 ms on localhost (executor driver) (18/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 1.0 (TID 18). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 24.0 in stage 1.0 (TID 23, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 1.0 (TID 18) in 150 ms on localhost (executor driver) (19/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 24.0 in stage 1.0 (TID 23)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,421][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,422][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,423][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,435][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 1.0 (TID 22). 3781 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 25.0 in stage 1.0 (TID 24, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 1.0 (TID 22) in 139 ms on localhost (executor driver) (20/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 25.0 in stage 1.0 (TID 24)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 1.0 (TID 20). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 26.0 in stage 1.0 (TID 25, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 26.0 in stage 1.0 (TID 25)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 1.0 (TID 20) in 198 ms on localhost (executor driver) (21/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,541][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 1.0 (TID 21). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 27.0 in stage 1.0 (TID 26, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 1.0 (TID 21) in 177 ms on localhost (executor driver) (22/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 27.0 in stage 1.0 (TID 26)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,566][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,584][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 1.0 (TID 23). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 28.0 in stage 1.0 (TID 27, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 1.0 (TID 23) in 185 ms on localhost (executor driver) (23/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 28.0 in stage 1.0 (TID 27)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 1.0 (TID 25). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 1.0 (TID 24). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 29.0 in stage 1.0 (TID 28, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 1.0 (TID 25) in 84 ms on localhost (executor driver) (24/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 29.0 in stage 1.0 (TID 28)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 1.0 (TID 24) in 92 ms on localhost (executor driver) (25/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 31.0 in stage 1.0 (TID 29, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 31.0 in stage 1.0 (TID 29)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,629][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 1.0 (TID 27). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 32.0 in stage 1.0 (TID 30, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 1.0 (TID 27) in 67 ms on localhost (executor driver) (26/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,656][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 32.0 in stage 1.0 (TID 30)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,662][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,665][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 1.0 (TID 26). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 33.0 in stage 1.0 (TID 31, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 1.0 (TID 26) in 133 ms on localhost (executor driver) (27/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,689][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 33.0 in stage 1.0 (TID 31)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,733][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 1.0 (TID 30). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,734][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 34.0 in stage 1.0 (TID 32, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 1.0 (TID 30) in 79 ms on localhost (executor driver) (28/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 34.0 in stage 1.0 (TID 32)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,752][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,753][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,754][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 1.0 (TID 29). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 35.0 in stage 1.0 (TID 33, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 1.0 (TID 29) in 147 ms on localhost (executor driver) (29/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,763][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 1.0 (TID 28). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 36.0 in stage 1.0 (TID 34, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 1.0 (TID 28) in 152 ms on localhost (executor driver) (30/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 36.0 in stage 1.0 (TID 34)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 35.0 in stage 1.0 (TID 33)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 1.0 (TID 31). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,775][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 37.0 in stage 1.0 (TID 35, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,775][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 1.0 (TID 31) in 87 ms on localhost (executor driver) (31/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,789][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 37.0 in stage 1.0 (TID 35)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 1.0 (TID 34). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,819][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 38.0 in stage 1.0 (TID 36, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 1.0 (TID 34) in 55 ms on localhost (executor driver) (32/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 38.0 in stage 1.0 (TID 36)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,872][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,874][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 1.0 (TID 36). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 39.0 in stage 1.0 (TID 37, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,874][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 39.0 in stage 1.0 (TID 37)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 1.0 (TID 36) in 56 ms on localhost (executor driver) (33/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 1.0 (TID 33). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 40.0 in stage 1.0 (TID 38, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 40.0 in stage 1.0 (TID 38)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 1.0 (TID 33) in 118 ms on localhost (executor driver) (34/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,910][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 1.0 (TID 32). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 41.0 in stage 1.0 (TID 39, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 1.0 (TID 32) in 179 ms on localhost (executor driver) (35/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 41.0 in stage 1.0 (TID 39)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,924][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,924][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,925][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,931][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,933][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 1.0 (TID 35). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,935][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 42.0 in stage 1.0 (TID 40, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 42.0 in stage 1.0 (TID 40)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 1.0 (TID 35) in 161 ms on localhost (executor driver) (36/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 1.0 (TID 37). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 43.0 in stage 1.0 (TID 41, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 1.0 (TID 37) in 117 ms on localhost (executor driver) (37/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 43.0 in stage 1.0 (TID 41)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:32,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 1.0 (TID 38). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 44.0 in stage 1.0 (TID 42, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 1.0 (TID 38) in 131 ms on localhost (executor driver) (38/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 44.0 in stage 1.0 (TID 42)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 1.0 (TID 39). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 45.0 in stage 1.0 (TID 43, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 1.0 (TID 39) in 108 ms on localhost (executor driver) (39/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 45.0 in stage 1.0 (TID 43)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,030][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,030][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 1.0 (TID 40). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 47.0 in stage 1.0 (TID 44, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 1.0 (TID 40) in 111 ms on localhost (executor driver) (40/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 47.0 in stage 1.0 (TID 44)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,058][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 1.0 (TID 41). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 48.0 in stage 1.0 (TID 45, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 1.0 (TID 41) in 71 ms on localhost (executor driver) (41/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 48.0 in stage 1.0 (TID 45)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 1.0 (TID 42). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 49.0 in stage 1.0 (TID 46, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,071][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 1.0 (TID 42) in 62 ms on localhost (executor driver) (42/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 49.0 in stage 1.0 (TID 46)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,077][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,078][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,079][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,079][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,096][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 1.0 (TID 43). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 50.0 in stage 1.0 (TID 47, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 1.0 (TID 43) in 79 ms on localhost (executor driver) (43/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 50.0 in stage 1.0 (TID 47)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,108][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,112][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 1.0 (TID 44). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 51.0 in stage 1.0 (TID 48, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 1.0 (TID 44) in 68 ms on localhost (executor driver) (44/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,113][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 51.0 in stage 1.0 (TID 48)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 1.0 (TID 45). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 52.0 in stage 1.0 (TID 49, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 52.0 in stage 1.0 (TID 49)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 1.0 (TID 45) in 70 ms on localhost (executor driver) (45/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,139][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,140][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 1.0 (TID 46). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 53.0 in stage 1.0 (TID 50, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 1.0 (TID 46) in 73 ms on localhost (executor driver) (46/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 53.0 in stage 1.0 (TID 50)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,184][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,185][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 1.0 (TID 47). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 54.0 in stage 1.0 (TID 51, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 1.0 (TID 47) in 92 ms on localhost (executor driver) (47/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 54.0 in stage 1.0 (TID 51)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 1.0 (TID 48). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 55.0 in stage 1.0 (TID 52, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 55.0 in stage 1.0 (TID 52)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 1.0 (TID 48) in 80 ms on localhost (executor driver) (48/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 1.0 (TID 50). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 57.0 in stage 1.0 (TID 53, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 1.0 (TID 50) in 72 ms on localhost (executor driver) (49/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 57.0 in stage 1.0 (TID 53)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,222][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 1.0 (TID 49). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,222][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 58.0 in stage 1.0 (TID 54, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 1.0 (TID 49) in 96 ms on localhost (executor driver) (50/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 58.0 in stage 1.0 (TID 54)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 1.0 (TID 53). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 59.0 in stage 1.0 (TID 55, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 1.0 (TID 53) in 54 ms on localhost (executor driver) (51/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 59.0 in stage 1.0 (TID 55)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 1.0 (TID 54). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 60.0 in stage 1.0 (TID 56, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 1.0 (TID 54) in 51 ms on localhost (executor driver) (52/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 60.0 in stage 1.0 (TID 56)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,286][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,300][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 1.0 (TID 51). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 61.0 in stage 1.0 (TID 57, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 1.0 (TID 51) in 114 ms on localhost (executor driver) (53/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 61.0 in stage 1.0 (TID 57)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 1.0 (TID 52). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 62.0 in stage 1.0 (TID 58, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 1.0 (TID 52) in 132 ms on localhost (executor driver) (54/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 62.0 in stage 1.0 (TID 58)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,337][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 1.0 (TID 57). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 63.0 in stage 1.0 (TID 59, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 1.0 (TID 57) in 60 ms on localhost (executor driver) (55/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 63.0 in stage 1.0 (TID 59)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,363][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 1.0 (TID 56). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 64.0 in stage 1.0 (TID 60, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 1.0 (TID 56) in 92 ms on localhost (executor driver) (56/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,366][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 64.0 in stage 1.0 (TID 60)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,368][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 1.0 (TID 58). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 65.0 in stage 1.0 (TID 61, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 1.0 (TID 58) in 58 ms on localhost (executor driver) (57/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 65.0 in stage 1.0 (TID 61)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 14 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,423][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 1.0 (TID 59). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 68.0 in stage 1.0 (TID 62, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 1.0 (TID 59) in 103 ms on localhost (executor driver) (58/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 68.0 in stage 1.0 (TID 62)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 1.0 (TID 55). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 69.0 in stage 1.0 (TID 63, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 1.0 (TID 55) in 241 ms on localhost (executor driver) (59/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,508][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 69.0 in stage 1.0 (TID 63)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 1.0 (TID 60). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 70.0 in stage 1.0 (TID 64, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 1.0 (TID 60) in 149 ms on localhost (executor driver) (60/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 70.0 in stage 1.0 (TID 64)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 1.0 (TID 61). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 71.0 in stage 1.0 (TID 65, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 71.0 in stage 1.0 (TID 65)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 1.0 (TID 61) in 142 ms on localhost (executor driver) (61/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,583][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 1.0 (TID 63). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 73.0 in stage 1.0 (TID 66, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 73.0 in stage 1.0 (TID 66)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 1.0 (TID 63) in 81 ms on localhost (executor driver) (62/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,587][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 1.0 (TID 62). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 74.0 in stage 1.0 (TID 67, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,588][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 1.0 (TID 64). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 1.0 (TID 62) in 128 ms on localhost (executor driver) (63/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 74.0 in stage 1.0 (TID 67)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 75.0 in stage 1.0 (TID 68, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 1.0 (TID 64) in 78 ms on localhost (executor driver) (64/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 75.0 in stage 1.0 (TID 68)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 1.0 (TID 65). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 76.0 in stage 1.0 (TID 69, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 76.0 in stage 1.0 (TID 69)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 1.0 (TID 65) in 86 ms on localhost (executor driver) (65/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 1.0 (TID 66). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,669][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 77.0 in stage 1.0 (TID 70, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 77.0 in stage 1.0 (TID 70)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 1.0 (TID 66) in 84 ms on localhost (executor driver) (66/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,677][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,740][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 1.0 (TID 69). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 78.0 in stage 1.0 (TID 71, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 1.0 (TID 69) in 134 ms on localhost (executor driver) (67/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,741][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 78.0 in stage 1.0 (TID 71)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,752][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 1.0 (TID 68). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 79.0 in stage 1.0 (TID 72, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 1.0 (TID 67). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 1.0 (TID 68) in 171 ms on localhost (executor driver) (68/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 80.0 in stage 1.0 (TID 73, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 1.0 (TID 67) in 174 ms on localhost (executor driver) (69/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 79.0 in stage 1.0 (TID 72)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 80.0 in stage 1.0 (TID 73)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,775][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,790][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 1.0 (TID 70). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 81.0 in stage 1.0 (TID 74, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 1.0 (TID 70) in 122 ms on localhost (executor driver) (70/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 81.0 in stage 1.0 (TID 74)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 1.0 (TID 71). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,802][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 82.0 in stage 1.0 (TID 75, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 82.0 in stage 1.0 (TID 75)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 1.0 (TID 71) in 63 ms on localhost (executor driver) (71/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,811][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 1.0 (TID 73). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 83.0 in stage 1.0 (TID 76, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 1.0 (TID 73) in 72 ms on localhost (executor driver) (72/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 83.0 in stage 1.0 (TID 76)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 1.0 (TID 72). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 84.0 in stage 1.0 (TID 77, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 1.0 (TID 72) in 75 ms on localhost (executor driver) (73/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 84.0 in stage 1.0 (TID 77)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,858][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 1.0 (TID 75). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 85.0 in stage 1.0 (TID 78, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 85.0 in stage 1.0 (TID 78)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,860][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 1.0 (TID 75) in 58 ms on localhost (executor driver) (74/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:33,997][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 1.0 (TID 74). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 86.0 in stage 1.0 (TID 79, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 1.0 (TID 74) in 218 ms on localhost (executor driver) (75/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 86.0 in stage 1.0 (TID 79)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 1.0 (TID 76). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 87.0 in stage 1.0 (TID 80, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 1.0 (TID 76) in 185 ms on localhost (executor driver) (76/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 87.0 in stage 1.0 (TID 80)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 1.0 (TID 78). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 88.0 in stage 1.0 (TID 81, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 88.0 in stage 1.0 (TID 81)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 1.0 (TID 78) in 180 ms on localhost (executor driver) (77/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,058][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 1.0 (TID 77). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 89.0 in stage 1.0 (TID 82, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 89.0 in stage 1.0 (TID 82)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,059][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 1.0 (TID 77) in 225 ms on localhost (executor driver) (78/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,066][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,079][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,080][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,080][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 1.0 (TID 79). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 90.0 in stage 1.0 (TID 83, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 90.0 in stage 1.0 (TID 83)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 1.0 (TID 79) in 73 ms on localhost (executor driver) (79/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 1.0 (TID 81). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 91.0 in stage 1.0 (TID 84, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 1.0 (TID 81) in 51 ms on localhost (executor driver) (80/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,089][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 91.0 in stage 1.0 (TID 84)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,094][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 1.0 (TID 82). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 92.0 in stage 1.0 (TID 85, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 92.0 in stage 1.0 (TID 85)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 1.0 (TID 82) in 59 ms on localhost (executor driver) (81/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 1.0 (TID 80). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 93.0 in stage 1.0 (TID 86, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 1.0 (TID 80) in 103 ms on localhost (executor driver) (82/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 93.0 in stage 1.0 (TID 86)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 1.0 (TID 83). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 94.0 in stage 1.0 (TID 87, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 94.0 in stage 1.0 (TID 87)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 1.0 (TID 83) in 57 ms on localhost (executor driver) (83/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,177][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 1.0 (TID 85). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 95.0 in stage 1.0 (TID 88, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 1.0 (TID 85) in 61 ms on localhost (executor driver) (84/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,178][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 95.0 in stage 1.0 (TID 88)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,187][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 1.0 (TID 86). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,188][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 96.0 in stage 1.0 (TID 89, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 96.0 in stage 1.0 (TID 89)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 1.0 (TID 86) in 71 ms on localhost (executor driver) (85/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 1.0 (TID 84). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 97.0 in stage 1.0 (TID 90, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 1.0 (TID 84) in 125 ms on localhost (executor driver) (86/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 97.0 in stage 1.0 (TID 90)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 1.0 (TID 87). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 98.0 in stage 1.0 (TID 91, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 1.0 (TID 87) in 94 ms on localhost (executor driver) (87/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,231][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 98.0 in stage 1.0 (TID 91)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,238][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,242][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 1.0 (TID 88). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 99.0 in stage 1.0 (TID 92, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 1.0 (TID 88) in 68 ms on localhost (executor driver) (88/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 99.0 in stage 1.0 (TID 92)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 1.0 (TID 89). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 100.0 in stage 1.0 (TID 93, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 1.0 (TID 89) in 76 ms on localhost (executor driver) (89/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 100.0 in stage 1.0 (TID 93)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 1.0 (TID 90). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,281][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 101.0 in stage 1.0 (TID 94, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 101.0 in stage 1.0 (TID 94)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 1.0 (TID 90) in 73 ms on localhost (executor driver) (90/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,293][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,294][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 1.0 (TID 91). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 102.0 in stage 1.0 (TID 95, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 1.0 (TID 91) in 64 ms on localhost (executor driver) (91/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,295][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 102.0 in stage 1.0 (TID 95)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,302][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,310][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,332][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 1.0 (TID 93). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,333][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 103.0 in stage 1.0 (TID 96, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,333][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 103.0 in stage 1.0 (TID 96)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,333][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 1.0 (TID 93) in 69 ms on localhost (executor driver) (92/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 1.0 (TID 92). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 104.0 in stage 1.0 (TID 97, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 1.0 (TID 92) in 100 ms on localhost (executor driver) (93/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 104.0 in stage 1.0 (TID 97)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 1.0 (TID 95). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 1.0 (TID 94). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 105.0 in stage 1.0 (TID 98, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 105.0 in stage 1.0 (TID 98)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 106.0 in stage 1.0 (TID 99, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 1.0 (TID 95) in 59 ms on localhost (executor driver) (94/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 106.0 in stage 1.0 (TID 99)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 1.0 (TID 94) in 75 ms on localhost (executor driver) (95/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 1.0 (TID 97). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 107.0 in stage 1.0 (TID 100, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 107.0 in stage 1.0 (TID 100)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 1.0 (TID 97) in 65 ms on localhost (executor driver) (96/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 1.0 (TID 96). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 109.0 in stage 1.0 (TID 101, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,432][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 1.0 (TID 96) in 100 ms on localhost (executor driver) (97/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,432][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 109.0 in stage 1.0 (TID 101)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,438][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,438][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 1.0 (TID 98). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 110.0 in stage 1.0 (TID 102, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 1.0 (TID 98) in 100 ms on localhost (executor driver) (98/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 110.0 in stage 1.0 (TID 102)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 1.0 (TID 100). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 111.0 in stage 1.0 (TID 103, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 1.0 (TID 100) in 54 ms on localhost (executor driver) (99/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 111.0 in stage 1.0 (TID 103)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,490][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,491][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,493][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 1.0 (TID 99). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,494][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 112.0 in stage 1.0 (TID 104, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,494][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 1.0 (TID 99) in 140 ms on localhost (executor driver) (100/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,495][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 112.0 in stage 1.0 (TID 104)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,500][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 1.0 (TID 101). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 113.0 in stage 1.0 (TID 105, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 113.0 in stage 1.0 (TID 105)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,531][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 1.0 (TID 101) in 100 ms on localhost (executor driver) (101/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,539][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 1.0 (TID 102). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 114.0 in stage 1.0 (TID 106, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 114.0 in stage 1.0 (TID 106)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,557][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 1.0 (TID 102) in 105 ms on localhost (executor driver) (102/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,567][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,568][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 1.0 (TID 104). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 115.0 in stage 1.0 (TID 107, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 115.0 in stage 1.0 (TID 107)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 1.0 (TID 104) in 78 ms on localhost (executor driver) (103/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,577][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,589][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 1.0 (TID 105). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,591][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 117.0 in stage 1.0 (TID 108, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 1.0 (TID 105) in 62 ms on localhost (executor driver) (104/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 117.0 in stage 1.0 (TID 108)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 1.0 (TID 106). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 119.0 in stage 1.0 (TID 109, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 1.0 (TID 106) in 55 ms on localhost (executor driver) (105/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 119.0 in stage 1.0 (TID 109)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 1.0 (TID 107). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 120.0 in stage 1.0 (TID 110, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 120.0 in stage 1.0 (TID 110)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 1.0 (TID 107) in 56 ms on localhost (executor driver) (106/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,631][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,631][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,632][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,641][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 1.0 (TID 108). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 121.0 in stage 1.0 (TID 111, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 121.0 in stage 1.0 (TID 111)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,642][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 1.0 (TID 108) in 51 ms on localhost (executor driver) (107/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,643][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 1.0 (TID 103). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 122.0 in stage 1.0 (TID 112, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 122.0 in stage 1.0 (TID 112)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 1.0 (TID 103) in 211 ms on localhost (executor driver) (108/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,678][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,690][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,710][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 1.0 (TID 109). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 123.0 in stage 1.0 (TID 113, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 1.0 (TID 109) in 101 ms on localhost (executor driver) (109/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 123.0 in stage 1.0 (TID 113)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 1.0 (TID 110). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 124.0 in stage 1.0 (TID 114, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 1.0 (TID 110) in 95 ms on localhost (executor driver) (110/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 124.0 in stage 1.0 (TID 114)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,726][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 1.0 (TID 112). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 125.0 in stage 1.0 (TID 115, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 1.0 (TID 112) in 60 ms on localhost (executor driver) (111/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 125.0 in stage 1.0 (TID 115)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,739][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 1.0 (TID 111). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 126.0 in stage 1.0 (TID 116, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 1.0 (TID 111) in 106 ms on localhost (executor driver) (112/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 126.0 in stage 1.0 (TID 116)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,756][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 1.0 (TID 114). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 127.0 in stage 1.0 (TID 117, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 127.0 in stage 1.0 (TID 117)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 1.0 (TID 114) in 55 ms on localhost (executor driver) (113/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 1.0 (TID 115). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 128.0 in stage 1.0 (TID 118, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 1.0 (TID 115) in 51 ms on localhost (executor driver) (114/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 128.0 in stage 1.0 (TID 118)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 1.0 (TID 113). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 130.0 in stage 1.0 (TID 119, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 1.0 (TID 113) in 81 ms on localhost (executor driver) (115/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 130.0 in stage 1.0 (TID 119)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,798][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,803][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,804][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,805][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,806][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,823][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 1.0 (TID 116). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 131.0 in stage 1.0 (TID 120, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 1.0 (TID 116) in 77 ms on localhost (executor driver) (116/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 131.0 in stage 1.0 (TID 120)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 1.0 (TID 117). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 132.0 in stage 1.0 (TID 121, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 1.0 (TID 117) in 64 ms on localhost (executor driver) (117/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 132.0 in stage 1.0 (TID 121)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,840][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,844][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 1.0 (TID 118). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 133.0 in stage 1.0 (TID 122, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 1.0 (TID 118) in 74 ms on localhost (executor driver) (118/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 133.0 in stage 1.0 (TID 122)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,863][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 1.0 (TID 119). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,873][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 134.0 in stage 1.0 (TID 123, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,873][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 1.0 (TID 119) in 80 ms on localhost (executor driver) (119/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 134.0 in stage 1.0 (TID 123)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 1.0 (TID 121). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 135.0 in stage 1.0 (TID 124, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 1.0 (TID 121) in 56 ms on localhost (executor driver) (120/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,892][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 135.0 in stage 1.0 (TID 124)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,898][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,899][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 1.0 (TID 120). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 136.0 in stage 1.0 (TID 125, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 1.0 (TID 120) in 75 ms on localhost (executor driver) (121/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 136.0 in stage 1.0 (TID 125)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,900][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:34,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 1.0 (TID 124). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 137.0 in stage 1.0 (TID 126, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 1.0 (TID 124) in 115 ms on localhost (executor driver) (122/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 137.0 in stage 1.0 (TID 126)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 1.0 (TID 122). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 138.0 in stage 1.0 (TID 127, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 1.0 (TID 122) in 153 ms on localhost (executor driver) (123/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 138.0 in stage 1.0 (TID 127)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 1.0 (TID 123). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 139.0 in stage 1.0 (TID 128, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 139.0 in stage 1.0 (TID 128)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 1.0 (TID 123) in 151 ms on localhost (executor driver) (124/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,029][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,056][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,057][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 1.0 (TID 125). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 140.0 in stage 1.0 (TID 129, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 1.0 (TID 125) in 162 ms on localhost (executor driver) (125/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 140.0 in stage 1.0 (TID 129)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 1.0 (TID 126). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 141.0 in stage 1.0 (TID 130, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 141.0 in stage 1.0 (TID 130)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,072][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 1.0 (TID 126) in 63 ms on localhost (executor driver) (126/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,075][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,075][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,077][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 1.0 (TID 128). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,078][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 142.0 in stage 1.0 (TID 131, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,079][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 1.0 (TID 128) in 57 ms on localhost (executor driver) (127/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,080][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 142.0 in stage 1.0 (TID 131)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,083][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 1.0 (TID 127). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 143.0 in stage 1.0 (TID 132, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 143.0 in stage 1.0 (TID 132)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 1.0 (TID 127) in 77 ms on localhost (executor driver) (128/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 1.0 (TID 130). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 144.0 in stage 1.0 (TID 133, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 144.0 in stage 1.0 (TID 133)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 1.0 (TID 130) in 61 ms on localhost (executor driver) (129/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 1.0 (TID 129). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 145.0 in stage 1.0 (TID 134, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 145.0 in stage 1.0 (TID 134)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 1.0 (TID 129) in 73 ms on localhost (executor driver) (130/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,141][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,142][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 1.0 (TID 131). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 1.0 (TID 132). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 146.0 in stage 1.0 (TID 135, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 147.0 in stage 1.0 (TID 136, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 1.0 (TID 131) in 67 ms on localhost (executor driver) (131/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 146.0 in stage 1.0 (TID 135)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 147.0 in stage 1.0 (TID 136)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 1.0 (TID 132) in 61 ms on localhost (executor driver) (132/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,153][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,155][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,180][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,206][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 1.0 (TID 133). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,206][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 1.0 (TID 134). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 148.0 in stage 1.0 (TID 137, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 148.0 in stage 1.0 (TID 137)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 149.0 in stage 1.0 (TID 138, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 1.0 (TID 134) in 74 ms on localhost (executor driver) (133/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 149.0 in stage 1.0 (TID 138)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 1.0 (TID 133) in 80 ms on localhost (executor driver) (134/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 1.0 (TID 135). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 150.0 in stage 1.0 (TID 139, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 1.0 (TID 135) in 74 ms on localhost (executor driver) (135/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,219][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 150.0 in stage 1.0 (TID 139)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 1.0 (TID 137). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 1.0 (TID 138). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 151.0 in stage 1.0 (TID 140, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 151.0 in stage 1.0 (TID 140)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 152.0 in stage 1.0 (TID 141, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 1.0 (TID 137) in 47 ms on localhost (executor driver) (136/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 1.0 (TID 138) in 46 ms on localhost (executor driver) (137/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 152.0 in stage 1.0 (TID 141)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 1.0 (TID 136). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 153.0 in stage 1.0 (TID 142, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 153.0 in stage 1.0 (TID 142)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 1.0 (TID 136) in 114 ms on localhost (executor driver) (138/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,265][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,281][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 1.0 (TID 139). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 154.0 in stage 1.0 (TID 143, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 1.0 (TID 139) in 67 ms on localhost (executor driver) (139/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 154.0 in stage 1.0 (TID 143)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,291][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,292][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,303][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 1.0 (TID 142). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 155.0 in stage 1.0 (TID 144, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 1.0 (TID 142) in 63 ms on localhost (executor driver) (140/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 155.0 in stage 1.0 (TID 144)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 1.0 (TID 141). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 156.0 in stage 1.0 (TID 145, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 1.0 (TID 141) in 75 ms on localhost (executor driver) (141/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 156.0 in stage 1.0 (TID 145)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,327][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 1.0 (TID 140). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 157.0 in stage 1.0 (TID 146, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 1.0 (TID 140) in 79 ms on localhost (executor driver) (142/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 157.0 in stage 1.0 (TID 146)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 1.0 (TID 144). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 158.0 in stage 1.0 (TID 147, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 158.0 in stage 1.0 (TID 147)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,373][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 1.0 (TID 144) in 53 ms on localhost (executor driver) (143/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,378][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,379][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 1.0 (TID 146). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 160.0 in stage 1.0 (TID 148, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 1.0 (TID 146) in 68 ms on localhost (executor driver) (144/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 160.0 in stage 1.0 (TID 148)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 1.0 (TID 145). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 161.0 in stage 1.0 (TID 149, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 1.0 (TID 145) in 75 ms on localhost (executor driver) (145/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 161.0 in stage 1.0 (TID 149)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,408][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 1.0 (TID 143). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 162.0 in stage 1.0 (TID 150, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 1.0 (TID 143) in 127 ms on localhost (executor driver) (146/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 162.0 in stage 1.0 (TID 150)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,448][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,449][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 1.0 (TID 147). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 163.0 in stage 1.0 (TID 151, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 163.0 in stage 1.0 (TID 151)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 1.0 (TID 147) in 78 ms on localhost (executor driver) (147/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 1.0 (TID 149). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 164.0 in stage 1.0 (TID 152, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 1.0 (TID 149) in 64 ms on localhost (executor driver) (148/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 164.0 in stage 1.0 (TID 152)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,473][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,474][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,487][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,504][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 1.0 (TID 150). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 165.0 in stage 1.0 (TID 153, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 1.0 (TID 150) in 95 ms on localhost (executor driver) (149/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 165.0 in stage 1.0 (TID 153)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 1.0 (TID 148). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 166.0 in stage 1.0 (TID 154, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 1.0 (TID 148) in 114 ms on localhost (executor driver) (150/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 166.0 in stage 1.0 (TID 154)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 1.0 (TID 151). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 167.0 in stage 1.0 (TID 155, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 1.0 (TID 151) in 70 ms on localhost (executor driver) (151/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,520][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 167.0 in stage 1.0 (TID 155)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 1.0 (TID 152). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 168.0 in stage 1.0 (TID 156, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 168.0 in stage 1.0 (TID 156)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 1.0 (TID 152) in 82 ms on localhost (executor driver) (152/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,559][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,559][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,562][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,569][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 1.0 (TID 153). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,585][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 169.0 in stage 1.0 (TID 157, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 1.0 (TID 153) in 82 ms on localhost (executor driver) (153/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,586][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 169.0 in stage 1.0 (TID 157)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,592][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 1.0 (TID 155). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 170.0 in stage 1.0 (TID 158, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 170.0 in stage 1.0 (TID 158)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 1.0 (TID 155) in 77 ms on localhost (executor driver) (154/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 1.0 (TID 154). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,597][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 171.0 in stage 1.0 (TID 159, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 1.0 (TID 154) in 85 ms on localhost (executor driver) (155/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 171.0 in stage 1.0 (TID 159)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 1.0 (TID 158). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 172.0 in stage 1.0 (TID 160, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 172.0 in stage 1.0 (TID 160)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 1.0 (TID 158) in 53 ms on localhost (executor driver) (156/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 1.0 (TID 157). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 173.0 in stage 1.0 (TID 161, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 173.0 in stage 1.0 (TID 161)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 1.0 (TID 157) in 79 ms on localhost (executor driver) (157/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,659][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 1.0 (TID 159). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,660][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 174.0 in stage 1.0 (TID 162, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 1.0 (TID 159) in 64 ms on localhost (executor driver) (158/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,661][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 174.0 in stage 1.0 (TID 162)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,662][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 1.0 (TID 156). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 175.0 in stage 1.0 (TID 163, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,663][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 1.0 (TID 156) in 118 ms on localhost (executor driver) (159/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,664][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 175.0 in stage 1.0 (TID 163)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,667][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,668][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,671][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 1.0 (TID 160). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,710][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 1.0 (TID 161). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,710][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 176.0 in stage 1.0 (TID 164, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,710][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 176.0 in stage 1.0 (TID 164)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,710][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 177.0 in stage 1.0 (TID 165, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 1.0 (TID 160) in 64 ms on localhost (executor driver) (160/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 177.0 in stage 1.0 (TID 165)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 1.0 (TID 161) in 62 ms on localhost (executor driver) (161/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 1.0 (TID 162). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,721][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 1.0 (TID 163). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 178.0 in stage 1.0 (TID 166, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 178.0 in stage 1.0 (TID 166)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,722][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 179.0 in stage 1.0 (TID 167, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 1.0 (TID 162) in 63 ms on localhost (executor driver) (162/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 179.0 in stage 1.0 (TID 167)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,723][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 1.0 (TID 163) in 60 ms on localhost (executor driver) (163/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,727][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,728][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,728][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,728][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,748][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,750][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 1.0 (TID 164). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 1.0 (TID 166). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 1.0 (TID 165). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 180.0 in stage 1.0 (TID 168, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 180.0 in stage 1.0 (TID 168)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 181.0 in stage 1.0 (TID 169, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 182.0 in stage 1.0 (TID 170, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 1.0 (TID 164) in 75 ms on localhost (executor driver) (164/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 1.0 (TID 165) in 75 ms on localhost (executor driver) (165/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 1.0 (TID 166) in 63 ms on localhost (executor driver) (166/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 182.0 in stage 1.0 (TID 170)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,789][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,797][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 181.0 in stage 1.0 (TID 169)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,797][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,841][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 1.0 (TID 168). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 183.0 in stage 1.0 (TID 171, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 183.0 in stage 1.0 (TID 171)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 1.0 (TID 168) in 61 ms on localhost (executor driver) (167/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 1.0 (TID 167). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 184.0 in stage 1.0 (TID 172, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,846][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 1.0 (TID 167) in 124 ms on localhost (executor driver) (168/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 184.0 in stage 1.0 (TID 172)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,865][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 1.0 (TID 170). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,866][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 185.0 in stage 1.0 (TID 173, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 185.0 in stage 1.0 (TID 173)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 1.0 (TID 170) in 83 ms on localhost (executor driver) (169/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,872][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,872][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 1.0 (TID 169). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 186.0 in stage 1.0 (TID 174, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 1.0 (TID 169) in 123 ms on localhost (executor driver) (170/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,908][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 186.0 in stage 1.0 (TID 174)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 1.0 (TID 172). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,915][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 187.0 in stage 1.0 (TID 175, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,915][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 187.0 in stage 1.0 (TID 175)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,915][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 1.0 (TID 172) in 69 ms on localhost (executor driver) (171/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 1.0 (TID 171). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 188.0 in stage 1.0 (TID 176, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 188.0 in stage 1.0 (TID 176)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 1.0 (TID 171) in 76 ms on localhost (executor driver) (172/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,923][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 1.0 (TID 173). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 189.0 in stage 1.0 (TID 177, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 189.0 in stage 1.0 (TID 177)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 1.0 (TID 173) in 85 ms on localhost (executor driver) (173/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,958][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 1.0 (TID 175). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 191.0 in stage 1.0 (TID 178, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 191.0 in stage 1.0 (TID 178)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 1.0 (TID 175) in 50 ms on localhost (executor driver) (174/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 1.0 (TID 174). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 193.0 in stage 1.0 (TID 179, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 1.0 (TID 174) in 61 ms on localhost (executor driver) (175/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,966][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 193.0 in stage 1.0 (TID 179)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,968][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 1.0 (TID 176). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,971][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 194.0 in stage 1.0 (TID 180, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 194.0 in stage 1.0 (TID 180)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 1.0 (TID 176) in 55 ms on localhost (executor driver) (176/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,972][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:35,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 1.0 (TID 178). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 1.0 (TID 179). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 195.0 in stage 1.0 (TID 181, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 1.0 (TID 180). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 195.0 in stage 1.0 (TID 181)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 196.0 in stage 1.0 (TID 182, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 196.0 in stage 1.0 (TID 182)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 197.0 in stage 1.0 (TID 183, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 1.0 (TID 178) in 57 ms on localhost (executor driver) (177/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 197.0 in stage 1.0 (TID 183)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 1.0 (TID 179) in 55 ms on localhost (executor driver) (178/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 1.0 (TID 180) in 51 ms on localhost (executor driver) (179/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 1.0 (TID 177). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 198.0 in stage 1.0 (TID 184, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 1.0 (TID 177) in 84 ms on localhost (executor driver) (180/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 198.0 in stage 1.0 (TID 184)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,080][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 1.0 (TID 184). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 199.0 in stage 1.0 (TID 185, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 1.0 (TID 184) in 48 ms on localhost (executor driver) (181/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,082][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 199.0 in stage 1.0 (TID 185)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 1.0 (TID 181). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,084][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 1.0 (TID 183). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 186, localhost, executor driver, partition 0, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 19.0 in stage 1.0 (TID 187, localhost, executor driver, partition 19, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 1.0 (TID 181) in 66 ms on localhost (executor driver) (182/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 19.0 in stage 1.0 (TID 187)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,086][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 1.0 (TID 183) in 65 ms on localhost (executor driver) (183/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 186)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,085][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,093][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,109][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,111][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 1.0 (TID 182). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 30.0 in stage 1.0 (TID 188, localhost, executor driver, partition 30, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 1.0 (TID 182) in 100 ms on localhost (executor driver) (184/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 30.0 in stage 1.0 (TID 188)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,125][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 1.0 (TID 185). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 46.0 in stage 1.0 (TID 189, localhost, executor driver, partition 46, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 1.0 (TID 185) in 45 ms on localhost (executor driver) (185/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 46.0 in stage 1.0 (TID 189)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,211][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 1.0 (TID 188). 3781 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 56.0 in stage 1.0 (TID 190, localhost, executor driver, partition 56, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 56.0 in stage 1.0 (TID 190)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 1.0 (TID 188) in 110 ms on localhost (executor driver) (186/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,234][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 1.0 (TID 187). 3781 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 66.0 in stage 1.0 (TID 191, localhost, executor driver, partition 66, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 1.0 (TID 187) in 156 ms on localhost (executor driver) (187/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,241][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 66.0 in stage 1.0 (TID 191)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,255][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 186). 3783 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 67.0 in stage 1.0 (TID 192, localhost, executor driver, partition 67, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 186) in 177 ms on localhost (executor driver) (188/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 67.0 in stage 1.0 (TID 192)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 1.0 (TID 190). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 72.0 in stage 1.0 (TID 193, localhost, executor driver, partition 72, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 1.0 (TID 190) in 55 ms on localhost (executor driver) (189/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 72.0 in stage 1.0 (TID 193)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,290][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,297][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,306][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,308][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 1.0 (TID 189). 3779 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,309][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 108.0 in stage 1.0 (TID 194, localhost, executor driver, partition 108, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,310][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 1.0 (TID 189) in 184 ms on localhost (executor driver) (190/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 108.0 in stage 1.0 (TID 194)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,313][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 1.0 (TID 192). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 116.0 in stage 1.0 (TID 195, localhost, executor driver, partition 116, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 116.0 in stage 1.0 (TID 195)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 1.0 (TID 192) in 53 ms on localhost (executor driver) (191/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,318][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,334][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,335][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 1.0 (TID 193). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 118.0 in stage 1.0 (TID 196, localhost, executor driver, partition 118, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 118.0 in stage 1.0 (TID 196)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 1.0 (TID 193) in 53 ms on localhost (executor driver) (192/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 1.0 (TID 191). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 129.0 in stage 1.0 (TID 197, localhost, executor driver, partition 129, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 129.0 in stage 1.0 (TID 197)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,355][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 1.0 (TID 191) in 114 ms on localhost (executor driver) (193/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,362][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 1.0 (TID 195). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,364][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 159.0 in stage 1.0 (TID 198, localhost, executor driver, partition 159, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 1.0 (TID 195) in 51 ms on localhost (executor driver) (194/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,365][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 159.0 in stage 1.0 (TID 198)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,366][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 1.0 (TID 194). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,366][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 190.0 in stage 1.0 (TID 199, localhost, executor driver, partition 190, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 1.0 (TID 194) in 58 ms on localhost (executor driver) (195/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,367][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 190.0 in stage 1.0 (TID 199)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,372][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 1.0 (TID 196). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 192.0 in stage 1.0 (TID 200, localhost, executor driver, partition 192, ANY, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,387][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 1.0 (TID 196) in 51 ms on localhost (executor driver) (196/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 192.0 in stage 1.0 (TID 200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,392][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 0 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,393][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 1.0 (TID 197). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 1.0 (TID 197) in 72 ms on localhost (executor driver) (197/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,437][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 1 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192/1.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,438][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 1.0 (TID 198). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 1.0 (TID 198) in 76 ms on localhost (executor driver) (198/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,441][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 1.0 (TID 199). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,443][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 1.0 (TID 199) in 77 ms on localhost (executor driver) (199/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 1 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192]
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 1.0 (TID 200). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 1.0 (TID 200) in 72 ms on localhost (executor driver) (200/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 1 (start at StreamingFile.scala:61) finished in 4.884 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 0 finished: start at StreamingFile.scala:61, took 5.909983 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 10.123749 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 1 (start at StreamingFile.scala:61) with 1 output partitions
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 2 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,607][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 2 (MapPartitionsRDD[13] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.7 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on 192.168.1.33:55782 (size: 4.6 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 2.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 201, localhost, executor driver, partition 0, PROCESS_LOCAL, 6076 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 201)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,640][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 10.395489 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Code generated in 24.517678 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 201). 1087 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 201) in 54 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,675][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 2 (start at StreamingFile.scala:61) finished in 0.056 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,676][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 1 finished: start at StreamingFile.scala:61, took 0.071876 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 2 (start at StreamingFile.scala:61) with 3 output partitions
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,680][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,681][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[13] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_6 stored as values in memory (estimated size 8.7 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_6_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.5 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_6_piece0 in memory on 192.168.1.33:55782 (size: 4.6 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,685][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 6 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(1, 2, 3))
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 3.0 with 3 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,686][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 202, localhost, executor driver, partition 1, PROCESS_LOCAL, 6130 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 203, localhost, executor driver, partition 2, PROCESS_LOCAL, 6124 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 204, localhost, executor driver, partition 3, PROCESS_LOCAL, 6127 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 202)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 204)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 203)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 204). 1120 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 204) in 5 ms on localhost (executor driver) (1/3)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 202). 1116 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 202) in 7 ms on localhost (executor driver) (2/3)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 203). 1114 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 203) in 7 ms on localhost (executor driver) (3/3)
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,695][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 3 (start at StreamingFile.scala:61) finished in 0.009 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,696][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 2 finished: start at StreamingFile.scala:61, took 0.016866 s
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:55:26.982Z",
  "numInputRows" : 100,
  "processedRowsPerSecond" : 10.275380189066997,
  "durationMs" : {
    "addBatch" : 8963,
    "getBatch" : 221,
    "getOffset" : 161,
    "queryPlanning" : 256,
    "triggerExecution" : 9731,
    "walCommit" : 102
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 15
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : null,
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 100,
    "processedRowsPerSecond" : 10.275380189066997
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[31m[WARN ][0;39m [35m[2017-11-05 15:55:36,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 9829 milliseconds
[34m[INFO ][0;39m [35m[2017-11-05 15:55:36,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:55:36.812Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:55:48,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:55:48.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:56:00,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:00.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 0
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Log offset set to 1 with 1 new files
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1509904566079,Map(spark.sql.shuffle.partitions -> 200))
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Processing 1 files from 1:1
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pruning directories with: 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Post-Scan Filters: 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Output Data Schema: struct<carrier: string, marital_status: string>
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Pushed Filters: 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_7 stored as values in memory (estimated size 221.6 KB, free 911.3 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_7_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.3 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,331][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_7_piece0 in memory on 192.168.1.33:55782 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 7 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,433][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_8 stored as values in memory (estimated size 220.5 KB, free 911.0 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 911.0 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_8_piece0 in memory on 192.168.1.33:55782 (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 8 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_9 stored as values in memory (estimated size 220.5 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,744][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.7 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_9_piece0 in memory on 192.168.1.33:55782 (size: 20.7 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 9 from start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,763][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Registering RDD 16 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,763][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 3 (start at StreamingFile.scala:61) with 200 output partitions
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 5 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 4)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 4)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 4 (MapPartitionsRDD[16] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_10 stored as values in memory (estimated size 26.9 KB, free 910.8 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.1 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,772][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_10_piece0 in memory on 192.168.1.33:55782 (size: 13.1 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 10 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,776][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[16] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 4.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 4.0 (TID 205, localhost, executor driver, partition 0, PROCESS_LOCAL, 5330 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 4.0 (TID 205)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Reading File path: file:///Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/user-record.2.csv, range: 0-6869, partition values: [empty row]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 4.0 (TID 205). 2262 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 4.0 (TID 205) in 51 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 4.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ShuffleMapStage 4 (start at StreamingFile.scala:61) finished in 0.053 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 5)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 5 (MapPartitionsRDD[23] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_11 stored as values in memory (estimated size 52.3 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_11_piece0 stored as bytes in memory (estimated size 21.9 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_11_piece0 in memory on 192.168.1.33:55782 (size: 21.9 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 11 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 200 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,879][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 5.0 with 200 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 5.0 (TID 206, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 5.0 (TID 207, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 5.0 (TID 208, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 5.0 (TID 209, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 3.0 in stage 5.0 (TID 209)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 5.0 (TID 208)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 5.0 (TID 207)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 5.0 (TID 206)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,902][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=3), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=2), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=1), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=0), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=2),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/2]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,973][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=1),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/1]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 5.0 (TID 208). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 5.0 (TID 207). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 4.0 in stage 5.0 (TID 210, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 5.0 in stage 5.0 (TID 211, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 4.0 in stage 5.0 (TID 210)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 5.0 (TID 208) in 87 ms on localhost (executor driver) (1/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 5.0 in stage 5.0 (TID 211)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 5.0 (TID 207) in 87 ms on localhost (executor driver) (2/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=5), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=4), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,979][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,980][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=0),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/0]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=3),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/3]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 5.0 (TID 206). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 5.0 (TID 209). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 6.0 in stage 5.0 (TID 212, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 6.0 in stage 5.0 (TID 212)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 7.0 in stage 5.0 (TID 213, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 5.0 (TID 209) in 93 ms on localhost (executor driver) (3/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,983][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 5.0 (TID 206) in 97 ms on localhost (executor driver) (4/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 7.0 in stage 5.0 (TID 213)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,989][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=6), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,990][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,994][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=7), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:06,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=4),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/4]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 5.0 (TID 210). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=5),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/5]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 8.0 in stage 5.0 (TID 214, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 8.0 in stage 5.0 (TID 214)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 4.0 in stage 5.0 (TID 210) in 60 ms on localhost (executor driver) (5/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 5.0 (TID 211). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 9.0 in stage 5.0 (TID 215, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 9.0 in stage 5.0 (TID 215)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 5.0 in stage 5.0 (TID 211) in 60 ms on localhost (executor driver) (6/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=8), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=6),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/6]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=9), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 5.0 (TID 212). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 10.0 in stage 5.0 (TID 216, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 102
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,146][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 6.0 in stage 5.0 (TID 212) in 165 ms on localhost (executor driver) (7/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 10.0 in stage 5.0 (TID 216)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,147][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_5_piece0 on 192.168.1.33:55782 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,148][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_6_piece0 on 192.168.1.33:55782 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 151
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,149][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_10_piece0 on 192.168.1.33:55782 in memory (size: 13.1 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,151][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,152][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=10), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,154][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,158][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=7),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/7]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 5.0 (TID 213). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,159][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 11.0 in stage 5.0 (TID 217, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 11.0 in stage 5.0 (TID 217)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,160][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 7.0 in stage 5.0 (TID 213) in 178 ms on localhost (executor driver) (8/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=11), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,166][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,167][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,176][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,189][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=8),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/8]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 5.0 (TID 214). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 12.0 in stage 5.0 (TID 218, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 8.0 in stage 5.0 (TID 214) in 158 ms on localhost (executor driver) (9/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 12.0 in stage 5.0 (TID 218)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=9),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/9]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 5.0 (TID 215). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 13.0 in stage 5.0 (TID 219, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=10),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/10]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 9.0 in stage 5.0 (TID 215) in 160 ms on localhost (executor driver) (10/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 13.0 in stage 5.0 (TID 219)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 5.0 (TID 216). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 14.0 in stage 5.0 (TID 220, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 14.0 in stage 5.0 (TID 220)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 10.0 in stage 5.0 (TID 216) in 152 ms on localhost (executor driver) (11/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=12), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=13), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=14), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,232][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,233][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=11),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/11]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 5.0 (TID 217). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 15.0 in stage 5.0 (TID 221, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 15.0 in stage 5.0 (TID 221)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 11.0 in stage 5.0 (TID 217) in 88 ms on localhost (executor driver) (12/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=15), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=12),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/12]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,256][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=13),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/13]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 5.0 (TID 218). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 5.0 (TID 219). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,257][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 16.0 in stage 5.0 (TID 222, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 16.0 in stage 5.0 (TID 222)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 17.0 in stage 5.0 (TID 223, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 12.0 in stage 5.0 (TID 218) in 68 ms on localhost (executor driver) (13/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 17.0 in stage 5.0 (TID 223)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,258][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 13.0 in stage 5.0 (TID 219) in 65 ms on localhost (executor driver) (14/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=16), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=17), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,263][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=14),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/14]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 5.0 (TID 220). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 18.0 in stage 5.0 (TID 224, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 14.0 in stage 5.0 (TID 220) in 74 ms on localhost (executor driver) (15/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 18.0 in stage 5.0 (TID 224)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=18), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,282][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,283][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=16),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/16]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,306][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 5.0 (TID 222). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 19.0 in stage 5.0 (TID 225, localhost, executor driver, partition 19, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 16.0 in stage 5.0 (TID 222) in 50 ms on localhost (executor driver) (16/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 19.0 in stage 5.0 (TID 225)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=15),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/15]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,312][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 5.0 (TID 221). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 20.0 in stage 5.0 (TID 226, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 15.0 in stage 5.0 (TID 221) in 68 ms on localhost (executor driver) (17/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 20.0 in stage 5.0 (TID 226)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=19), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=20), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=17),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/17]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,324][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 5.0 (TID 223). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 21.0 in stage 5.0 (TID 227, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 17.0 in stage 5.0 (TID 223) in 71 ms on localhost (executor driver) (18/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,334][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 21.0 in stage 5.0 (TID 227)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=18),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/18]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 5.0 (TID 224). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 22.0 in stage 5.0 (TID 228, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 18.0 in stage 5.0 (TID 224) in 72 ms on localhost (executor driver) (19/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 22.0 in stage 5.0 (TID 228)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=21), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=22), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=20),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/20]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 5.0 (TID 226). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 23.0 in stage 5.0 (TID 229, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 20.0 in stage 5.0 (TID 226) in 76 ms on localhost (executor driver) (20/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 23.0 in stage 5.0 (TID 229)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=23), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=22),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/22]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 5.0 (TID 228). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 24.0 in stage 5.0 (TID 230, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 22.0 in stage 5.0 (TID 228) in 72 ms on localhost (executor driver) (21/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,412][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 24.0 in stage 5.0 (TID 230)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=19),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/19]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 5.0 (TID 225). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 25.0 in stage 5.0 (TID 231, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 19.0 in stage 5.0 (TID 225) in 109 ms on localhost (executor driver) (22/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 25.0 in stage 5.0 (TID 231)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=24), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,417][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,418][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=25), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=21),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/21]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 5.0 (TID 227). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 26.0 in stage 5.0 (TID 232, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 21.0 in stage 5.0 (TID 227) in 136 ms on localhost (executor driver) (23/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 26.0 in stage 5.0 (TID 232)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=26), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=23),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/23]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,475][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 5.0 (TID 229). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 27.0 in stage 5.0 (TID 233, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 23.0 in stage 5.0 (TID 229) in 88 ms on localhost (executor driver) (24/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,477][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 27.0 in stage 5.0 (TID 233)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=27), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=24),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/24]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,525][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 5.0 (TID 230). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 28.0 in stage 5.0 (TID 234, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 24.0 in stage 5.0 (TID 230) in 115 ms on localhost (executor driver) (25/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=25),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/25]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 5.0 (TID 231). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 29.0 in stage 5.0 (TID 235, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 28.0 in stage 5.0 (TID 234)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 25.0 in stage 5.0 (TID 231) in 113 ms on localhost (executor driver) (26/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 29.0 in stage 5.0 (TID 235)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,532][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=29), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=28), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=26),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/26]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 5.0 (TID 232). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 30.0 in stage 5.0 (TID 236, localhost, executor driver, partition 30, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 26.0 in stage 5.0 (TID 232) in 84 ms on localhost (executor driver) (27/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 30.0 in stage 5.0 (TID 236)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=30), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,551][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,561][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,563][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=27),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/27]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 5.0 (TID 233). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 31.0 in stage 5.0 (TID 237, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,565][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 27.0 in stage 5.0 (TID 233) in 88 ms on localhost (executor driver) (28/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,566][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 31.0 in stage 5.0 (TID 237)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,570][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=31), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,579][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,580][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,598][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=29),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/29]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 5.0 (TID 235). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 32.0 in stage 5.0 (TID 238, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 29.0 in stage 5.0 (TID 235) in 71 ms on localhost (executor driver) (29/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 32.0 in stage 5.0 (TID 238)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=32), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,604][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,608][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=28),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/28]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 5.0 (TID 234). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=30),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/30]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 33.0 in stage 5.0 (TID 239, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 5.0 (TID 236). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,611][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 28.0 in stage 5.0 (TID 234) in 85 ms on localhost (executor driver) (30/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 33.0 in stage 5.0 (TID 239)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 34.0 in stage 5.0 (TID 240, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,612][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 30.0 in stage 5.0 (TID 236) in 67 ms on localhost (executor driver) (31/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,613][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 34.0 in stage 5.0 (TID 240)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,615][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=33), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,616][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,617][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=34), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=31),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/31]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 5.0 (TID 237). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 35.0 in stage 5.0 (TID 241, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 31.0 in stage 5.0 (TID 237) in 90 ms on localhost (executor driver) (32/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 35.0 in stage 5.0 (TID 241)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,670][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=35), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,673][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,674][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,694][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,704][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,752][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=32),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/32]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 5.0 (TID 238). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=33),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/33]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,755][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 36.0 in stage 5.0 (TID 242, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,756][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 32.0 in stage 5.0 (TID 238) in 157 ms on localhost (executor driver) (33/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,756][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 5.0 (TID 239). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,757][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 36.0 in stage 5.0 (TID 242)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,758][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 37.0 in stage 5.0 (TID 243, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 33.0 in stage 5.0 (TID 239) in 148 ms on localhost (executor driver) (34/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 37.0 in stage 5.0 (TID 243)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=34),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/34]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 5.0 (TID 240). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,763][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 38.0 in stage 5.0 (TID 244, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 34.0 in stage 5.0 (TID 240) in 152 ms on localhost (executor driver) (35/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 38.0 in stage 5.0 (TID 244)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=36), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,768][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=37), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=38), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,782][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=35),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/35]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 5.0 (TID 241). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 39.0 in stage 5.0 (TID 245, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 35.0 in stage 5.0 (TID 241) in 207 ms on localhost (executor driver) (36/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,862][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 39.0 in stage 5.0 (TID 245)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,870][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=39), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,871][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,872][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=37),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/37]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=36),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/36]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=38),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/38]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 5.0 (TID 243). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 5.0 (TID 242). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 5.0 (TID 244). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 40.0 in stage 5.0 (TID 246, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 40.0 in stage 5.0 (TID 246)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 41.0 in stage 5.0 (TID 247, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 37.0 in stage 5.0 (TID 243) in 199 ms on localhost (executor driver) (37/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 41.0 in stage 5.0 (TID 247)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 36.0 in stage 5.0 (TID 242) in 202 ms on localhost (executor driver) (38/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 42.0 in stage 5.0 (TID 248, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 38.0 in stage 5.0 (TID 244) in 194 ms on localhost (executor driver) (39/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 42.0 in stage 5.0 (TID 248)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=41), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,960][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=40), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,963][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=42), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,964][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,965][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,983][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,994][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,995][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,996][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=39),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/39]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:07,999][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 5.0 (TID 245). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 43.0 in stage 5.0 (TID 249, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,000][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 39.0 in stage 5.0 (TID 245) in 139 ms on localhost (executor driver) (40/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,001][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 43.0 in stage 5.0 (TID 249)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=43), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=42),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/42]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 5.0 (TID 248). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 44.0 in stage 5.0 (TID 250, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 44.0 in stage 5.0 (TID 250)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,017][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 42.0 in stage 5.0 (TID 248) in 60 ms on localhost (executor driver) (41/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=41),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/41]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,019][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 5.0 (TID 247). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 45.0 in stage 5.0 (TID 251, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 41.0 in stage 5.0 (TID 247) in 64 ms on localhost (executor driver) (42/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 45.0 in stage 5.0 (TID 251)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=44), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=45), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=40),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/40]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 5.0 (TID 246). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,027][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 46.0 in stage 5.0 (TID 252, localhost, executor driver, partition 46, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 40.0 in stage 5.0 (TID 246) in 73 ms on localhost (executor driver) (43/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,028][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 46.0 in stage 5.0 (TID 252)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,032][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=46), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,047][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,103][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=45),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/45]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 5.0 (TID 251). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=43),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/43]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 47.0 in stage 5.0 (TID 253, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 45.0 in stage 5.0 (TID 251) in 100 ms on localhost (executor driver) (44/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 47.0 in stage 5.0 (TID 253)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 5.0 (TID 249). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 48.0 in stage 5.0 (TID 254, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 43.0 in stage 5.0 (TID 249) in 121 ms on localhost (executor driver) (45/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,122][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 48.0 in stage 5.0 (TID 254)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=46),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/46]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=47), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 5.0 (TID 252). 3780 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 49.0 in stage 5.0 (TID 255, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=48), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 46.0 in stage 5.0 (TID 252) in 102 ms on localhost (executor driver) (46/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 49.0 in stage 5.0 (TID 255)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,133][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=44),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/44]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=49), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 5.0 (TID 250). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,135][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,136][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 50.0 in stage 5.0 (TID 256, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 44.0 in stage 5.0 (TID 250) in 120 ms on localhost (executor driver) (47/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,137][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 50.0 in stage 5.0 (TID 256)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=50), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,161][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,184][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,184][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=48),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/48]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,194][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 5.0 (TID 254). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 51.0 in stage 5.0 (TID 257, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 51.0 in stage 5.0 (TID 257)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,195][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 48.0 in stage 5.0 (TID 254) in 74 ms on localhost (executor driver) (48/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=51), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=49),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/49]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 5.0 (TID 255). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=50),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/50]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 52.0 in stage 5.0 (TID 258, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 49.0 in stage 5.0 (TID 255) in 88 ms on localhost (executor driver) (49/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 5.0 (TID 256). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 52.0 in stage 5.0 (TID 258)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,219][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 53.0 in stage 5.0 (TID 259, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 50.0 in stage 5.0 (TID 256) in 84 ms on localhost (executor driver) (50/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 53.0 in stage 5.0 (TID 259)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,223][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=52), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=53), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=51),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/51]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=47),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/47]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 5.0 (TID 253). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 5.0 (TID 257). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 54.0 in stage 5.0 (TID 260, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 54.0 in stage 5.0 (TID 260)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 55.0 in stage 5.0 (TID 261, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 51.0 in stage 5.0 (TID 257) in 54 ms on localhost (executor driver) (51/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 47.0 in stage 5.0 (TID 253) in 132 ms on localhost (executor driver) (52/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,250][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 55.0 in stage 5.0 (TID 261)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=54), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,253][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,260][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=55), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,261][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,262][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,264][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,278][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=53),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/53]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,298][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 5.0 (TID 259). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 56.0 in stage 5.0 (TID 262, localhost, executor driver, partition 56, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 53.0 in stage 5.0 (TID 259) in 80 ms on localhost (executor driver) (53/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,299][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 56.0 in stage 5.0 (TID 262)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,304][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=56), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,305][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,306][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,310][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=54),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/54]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 5.0 (TID 260). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 57.0 in stage 5.0 (TID 263, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 57.0 in stage 5.0 (TID 263)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,311][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 54.0 in stage 5.0 (TID 260) in 63 ms on localhost (executor driver) (54/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=57), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=52),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/52]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 5.0 (TID 258). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 58.0 in stage 5.0 (TID 264, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 52.0 in stage 5.0 (TID 258) in 105 ms on localhost (executor driver) (55/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 58.0 in stage 5.0 (TID 264)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,325][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=58), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,326][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=55),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/55]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 5.0 (TID 261). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 59.0 in stage 5.0 (TID 265, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 55.0 in stage 5.0 (TID 261) in 93 ms on localhost (executor driver) (56/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 59.0 in stage 5.0 (TID 265)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=59), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,356][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=57),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/57]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 5.0 (TID 263). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 60.0 in stage 5.0 (TID 266, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,376][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 57.0 in stage 5.0 (TID 263) in 65 ms on localhost (executor driver) (57/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,377][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 60.0 in stage 5.0 (TID 266)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,380][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=60), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,381][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=58),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/58]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=56),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/56]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 5.0 (TID 264). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 5.0 (TID 262). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 61.0 in stage 5.0 (TID 267, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 62.0 in stage 5.0 (TID 268, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 61.0 in stage 5.0 (TID 267)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 58.0 in stage 5.0 (TID 264) in 69 ms on localhost (executor driver) (58/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 62.0 in stage 5.0 (TID 268)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 56.0 in stage 5.0 (TID 262) in 92 ms on localhost (executor driver) (59/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=62), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,394][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=61), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=59),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/59]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,399][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,400][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 5.0 (TID 265). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,400][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 63.0 in stage 5.0 (TID 269, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 59.0 in stage 5.0 (TID 265) in 60 ms on localhost (executor driver) (60/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 63.0 in stage 5.0 (TID 269)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=63), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=60),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/60]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 5.0 (TID 266). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 64.0 in stage 5.0 (TID 270, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 64.0 in stage 5.0 (TID 270)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,426][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 60.0 in stage 5.0 (TID 266) in 50 ms on localhost (executor driver) (61/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,427][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=64), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,451][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=61),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/61]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 5.0 (TID 267). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 65.0 in stage 5.0 (TID 271, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 65.0 in stage 5.0 (TID 271)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 61.0 in stage 5.0 (TID 267) in 62 ms on localhost (executor driver) (62/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=65), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=63),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/63]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,458][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 5.0 (TID 269). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 66.0 in stage 5.0 (TID 272, localhost, executor driver, partition 66, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 66.0 in stage 5.0 (TID 272)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,459][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 63.0 in stage 5.0 (TID 269) in 59 ms on localhost (executor driver) (63/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,460][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=62),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/62]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 5.0 (TID 268). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 67.0 in stage 5.0 (TID 273, localhost, executor driver, partition 67, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 62.0 in stage 5.0 (TID 268) in 72 ms on localhost (executor driver) (64/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 67.0 in stage 5.0 (TID 273)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=66), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=67), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,468][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,510][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=64),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/64]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 5.0 (TID 270). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,516][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 68.0 in stage 5.0 (TID 274, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 64.0 in stage 5.0 (TID 270) in 92 ms on localhost (executor driver) (65/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,517][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 68.0 in stage 5.0 (TID 274)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=68), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,524][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=65),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/65]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=66),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/66]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 5.0 (TID 272). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 5.0 (TID 271). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 69.0 in stage 5.0 (TID 275, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 70.0 in stage 5.0 (TID 276, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 69.0 in stage 5.0 (TID 275)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 66.0 in stage 5.0 (TID 272) in 70 ms on localhost (executor driver) (66/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,529][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 70.0 in stage 5.0 (TID 276)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,530][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 65.0 in stage 5.0 (TID 271) in 78 ms on localhost (executor driver) (67/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=70), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=69), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,533][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,534][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,573][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=67),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/67]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 5.0 (TID 273). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 71.0 in stage 5.0 (TID 277, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 71.0 in stage 5.0 (TID 277)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,574][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 67.0 in stage 5.0 (TID 273) in 113 ms on localhost (executor driver) (68/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=71), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,578][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,590][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=70),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/70]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=68),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/68]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 5.0 (TID 276). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,594][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 5.0 (TID 274). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 72.0 in stage 5.0 (TID 278, localhost, executor driver, partition 72, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 72.0 in stage 5.0 (TID 278)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 73.0 in stage 5.0 (TID 279, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 68.0 in stage 5.0 (TID 274) in 80 ms on localhost (executor driver) (69/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 73.0 in stage 5.0 (TID 279)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 70.0 in stage 5.0 (TID 276) in 67 ms on localhost (executor driver) (70/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=73), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,602][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=72), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,603][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=71),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/71]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=69),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/69]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,621][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 5.0 (TID 277). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 74.0 in stage 5.0 (TID 280, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 74.0 in stage 5.0 (TID 280)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,622][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 71.0 in stage 5.0 (TID 277) in 48 ms on localhost (executor driver) (71/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=74), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 5.0 (TID 275). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 75.0 in stage 5.0 (TID 281, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 69.0 in stage 5.0 (TID 275) in 100 ms on localhost (executor driver) (72/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 75.0 in stage 5.0 (TID 281)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,633][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=75), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,634][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=73),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/73]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 5.0 (TID 279). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 76.0 in stage 5.0 (TID 282, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 76.0 in stage 5.0 (TID 282)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 73.0 in stage 5.0 (TID 279) in 53 ms on localhost (executor driver) (73/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,648][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=72),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/72]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 5.0 (TID 278). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 77.0 in stage 5.0 (TID 283, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 72.0 in stage 5.0 (TID 278) in 55 ms on localhost (executor driver) (74/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 77.0 in stage 5.0 (TID 283)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=76), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,652][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,653][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=77), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,654][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,672][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,682][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=74),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/74]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 5.0 (TID 280). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,683][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 78.0 in stage 5.0 (TID 284, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 78.0 in stage 5.0 (TID 284)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,684][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 74.0 in stage 5.0 (TID 280) in 62 ms on localhost (executor driver) (75/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=78), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=75),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/75]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,697][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 5.0 (TID 281). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 79.0 in stage 5.0 (TID 285, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 79.0 in stage 5.0 (TID 285)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,698][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 75.0 in stage 5.0 (TID 281) in 71 ms on localhost (executor driver) (76/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=79), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=76),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/76]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 5.0 (TID 282). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 80.0 in stage 5.0 (TID 286, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 80.0 in stage 5.0 (TID 286)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 76.0 in stage 5.0 (TID 282) in 60 ms on localhost (executor driver) (77/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=80), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=77),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/77]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,711][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 5.0 (TID 283). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,712][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 81.0 in stage 5.0 (TID 287, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 77.0 in stage 5.0 (TID 283) in 64 ms on localhost (executor driver) (78/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,713][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 81.0 in stage 5.0 (TID 287)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,717][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,717][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=81), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,717][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,718][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,749][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,751][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=79),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/79]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,759][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=80),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/80]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 5.0 (TID 285). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 5.0 (TID 286). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,760][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 82.0 in stage 5.0 (TID 288, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 83.0 in stage 5.0 (TID 289, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 82.0 in stage 5.0 (TID 288)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,761][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 80.0 in stage 5.0 (TID 286) in 54 ms on localhost (executor driver) (79/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 83.0 in stage 5.0 (TID 289)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,762][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 79.0 in stage 5.0 (TID 285) in 64 ms on localhost (executor driver) (80/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=82), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=83), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,766][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=81),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/81]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 5.0 (TID 287). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 84.0 in stage 5.0 (TID 290, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 81.0 in stage 5.0 (TID 287) in 69 ms on localhost (executor driver) (81/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,781][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 84.0 in stage 5.0 (TID 290)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,785][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=84), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,786][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,787][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,788][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,820][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=78),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/78]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 5.0 (TID 284). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 85.0 in stage 5.0 (TID 291, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,821][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 85.0 in stage 5.0 (TID 291)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,822][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 78.0 in stage 5.0 (TID 284) in 137 ms on localhost (executor driver) (82/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,824][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=82),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/82]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 5.0 (TID 288). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 86.0 in stage 5.0 (TID 292, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=85), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,825][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=83),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/83]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 86.0 in stage 5.0 (TID 292)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 82.0 in stage 5.0 (TID 288) in 66 ms on localhost (executor driver) (83/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,826][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 5.0 (TID 289). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 87.0 in stage 5.0 (TID 293, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,827][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 83.0 in stage 5.0 (TID 289) in 67 ms on localhost (executor driver) (84/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,828][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 87.0 in stage 5.0 (TID 293)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=86), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=87), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,859][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,864][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,867][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=84),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/84]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 5.0 (TID 290). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,868][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 88.0 in stage 5.0 (TID 294, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,869][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 84.0 in stage 5.0 (TID 290) in 88 ms on localhost (executor driver) (85/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 88.0 in stage 5.0 (TID 294)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,875][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=86),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/86]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 5.0 (TID 292). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=87),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/87]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 89.0 in stage 5.0 (TID 295, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 5.0 (TID 293). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 86.0 in stage 5.0 (TID 292) in 51 ms on localhost (executor driver) (86/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 89.0 in stage 5.0 (TID 295)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 90.0 in stage 5.0 (TID 296, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,877][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 87.0 in stage 5.0 (TID 293) in 50 ms on localhost (executor driver) (87/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,878][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 90.0 in stage 5.0 (TID 296)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=89), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=88), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,881][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,882][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=90), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,882][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=85),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/85]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,883][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 5.0 (TID 291). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 91.0 in stage 5.0 (TID 297, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 85.0 in stage 5.0 (TID 291) in 70 ms on localhost (executor driver) (88/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 91.0 in stage 5.0 (TID 297)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=91), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,896][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,896][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,920][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=91),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/91]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=89),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/89]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 5.0 (TID 297). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=90),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/90]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 5.0 (TID 295). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 92.0 in stage 5.0 (TID 298, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=88),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/88]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,949][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 5.0 (TID 296). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 93.0 in stage 5.0 (TID 299, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 92.0 in stage 5.0 (TID 298)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 5.0 (TID 294). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 91.0 in stage 5.0 (TID 297) in 61 ms on localhost (executor driver) (89/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 89.0 in stage 5.0 (TID 295) in 75 ms on localhost (executor driver) (90/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 94.0 in stage 5.0 (TID 300, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 94.0 in stage 5.0 (TID 300)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 95.0 in stage 5.0 (TID 301, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 90.0 in stage 5.0 (TID 296) in 76 ms on localhost (executor driver) (91/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 88.0 in stage 5.0 (TID 294) in 85 ms on localhost (executor driver) (92/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 93.0 in stage 5.0 (TID 299)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,955][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=92), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=94), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,956][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,957][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 95.0 in stage 5.0 (TID 301)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,970][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=93), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,974][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=95), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:08,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 35 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,033][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=94),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/94]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 5.0 (TID 300). 3781 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 96.0 in stage 5.0 (TID 302, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 94.0 in stage 5.0 (TID 300) in 83 ms on localhost (executor driver) (93/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 96.0 in stage 5.0 (TID 302)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=96), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=92),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/92]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 5.0 (TID 298). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 97.0 in stage 5.0 (TID 303, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 92.0 in stage 5.0 (TID 298) in 104 ms on localhost (executor driver) (94/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 97.0 in stage 5.0 (TID 303)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,048][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=97), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,058][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=95),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/95]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=93),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/93]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,060][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 5.0 (TID 301). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,061][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 98.0 in stage 5.0 (TID 304, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 98.0 in stage 5.0 (TID 304)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 95.0 in stage 5.0 (TID 301) in 110 ms on localhost (executor driver) (95/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 5.0 (TID 299). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 99.0 in stage 5.0 (TID 305, localhost, executor driver, partition 99, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 93.0 in stage 5.0 (TID 299) in 113 ms on localhost (executor driver) (96/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 99.0 in stage 5.0 (TID 305)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=98), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,065][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=99), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,073][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,087][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,090][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=96),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/96]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 5.0 (TID 302). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,091][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 100.0 in stage 5.0 (TID 306, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 100.0 in stage 5.0 (TID 306)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,092][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 96.0 in stage 5.0 (TID 302) in 58 ms on localhost (executor driver) (97/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,095][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=100), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,096][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,096][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=97),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/97]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 5.0 (TID 303). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 101.0 in stage 5.0 (TID 307, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,098][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 97.0 in stage 5.0 (TID 303) in 54 ms on localhost (executor driver) (98/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 101.0 in stage 5.0 (TID 307)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=99),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/99]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,114][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=98),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/98]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 5.0 (TID 305). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,115][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=101), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 5.0 (TID 304). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 102.0 in stage 5.0 (TID 308, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 102.0 in stage 5.0 (TID 308)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 103.0 in stage 5.0 (TID 309, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 99.0 in stage 5.0 (TID 305) in 55 ms on localhost (executor driver) (99/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,117][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 103.0 in stage 5.0 (TID 309)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 98.0 in stage 5.0 (TID 304) in 57 ms on localhost (executor driver) (100/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,126][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=103), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,129][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,130][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=102), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,131][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,132][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=100),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/100]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,170][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 5.0 (TID 306). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 104.0 in stage 5.0 (TID 310, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 104.0 in stage 5.0 (TID 310)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,171][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 100.0 in stage 5.0 (TID 306) in 80 ms on localhost (executor driver) (101/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,174][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=104), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,175][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,181][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,190][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=103),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/103]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 5.0 (TID 309). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=102),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/102]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,191][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 105.0 in stage 5.0 (TID 311, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 5.0 (TID 308). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 103.0 in stage 5.0 (TID 309) in 75 ms on localhost (executor driver) (102/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,192][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 105.0 in stage 5.0 (TID 311)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 106.0 in stage 5.0 (TID 312, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 102.0 in stage 5.0 (TID 308) in 77 ms on localhost (executor driver) (103/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,193][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 106.0 in stage 5.0 (TID 312)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=105), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,196][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=106), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,198][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=101),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/101]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,204][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 5.0 (TID 307). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 107.0 in stage 5.0 (TID 313, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 101.0 in stage 5.0 (TID 307) in 107 ms on localhost (executor driver) (104/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,205][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 107.0 in stage 5.0 (TID 313)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,209][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=107), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,210][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,228][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=104),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/104]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,229][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 5.0 (TID 310). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 108.0 in stage 5.0 (TID 314, localhost, executor driver, partition 108, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 108.0 in stage 5.0 (TID 314)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 104.0 in stage 5.0 (TID 310) in 59 ms on localhost (executor driver) (105/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,230][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,236][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=108), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,237][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=105),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/105]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,246][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 5.0 (TID 311). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 109.0 in stage 5.0 (TID 315, localhost, executor driver, partition 109, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 109.0 in stage 5.0 (TID 315)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 105.0 in stage 5.0 (TID 311) in 56 ms on localhost (executor driver) (106/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=109), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,252][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,266][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=106),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/106]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=107),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/107]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 5.0 (TID 312). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 5.0 (TID 313). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,267][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 110.0 in stage 5.0 (TID 316, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 111.0 in stage 5.0 (TID 317, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 110.0 in stage 5.0 (TID 316)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 106.0 in stage 5.0 (TID 312) in 75 ms on localhost (executor driver) (107/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,268][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 111.0 in stage 5.0 (TID 317)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,269][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 107.0 in stage 5.0 (TID 313) in 64 ms on localhost (executor driver) (108/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,271][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=110), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=111), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,272][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,273][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,301][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=109),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/109]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,315][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 5.0 (TID 315). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,316][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 112.0 in stage 5.0 (TID 318, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 109.0 in stage 5.0 (TID 315) in 70 ms on localhost (executor driver) (109/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,317][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 112.0 in stage 5.0 (TID 318)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=112), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,338][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,339][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=108),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/108]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 5.0 (TID 314). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,340][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 113.0 in stage 5.0 (TID 319, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 113.0 in stage 5.0 (TID 319)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 108.0 in stage 5.0 (TID 314) in 111 ms on localhost (executor driver) (110/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=113), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=110),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/110]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 5.0 (TID 316). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 114.0 in stage 5.0 (TID 320, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 110.0 in stage 5.0 (TID 316) in 83 ms on localhost (executor driver) (111/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 114.0 in stage 5.0 (TID 320)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=114), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,359][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=112),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/112]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,360][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 5.0 (TID 318). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 115.0 in stage 5.0 (TID 321, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 112.0 in stage 5.0 (TID 318) in 45 ms on localhost (executor driver) (112/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 115.0 in stage 5.0 (TID 321)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,361][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,366][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=115), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,386][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,389][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=111),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/111]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 5.0 (TID 317). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,390][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 116.0 in stage 5.0 (TID 322, localhost, executor driver, partition 116, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 111.0 in stage 5.0 (TID 317) in 123 ms on localhost (executor driver) (113/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,391][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 116.0 in stage 5.0 (TID 322)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,395][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=116), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,402][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=113),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/113]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 5.0 (TID 319). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,406][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 117.0 in stage 5.0 (TID 323, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 113.0 in stage 5.0 (TID 319) in 67 ms on localhost (executor driver) (114/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,407][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 117.0 in stage 5.0 (TID 323)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=117), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,411][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=115),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/115]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 5.0 (TID 321). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 118.0 in stage 5.0 (TID 324, localhost, executor driver, partition 118, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 115.0 in stage 5.0 (TID 321) in 56 ms on localhost (executor driver) (115/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,416][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 118.0 in stage 5.0 (TID 324)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,419][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=118), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,420][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=114),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/114]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 5.0 (TID 320). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,430][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 119.0 in stage 5.0 (TID 325, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 114.0 in stage 5.0 (TID 320) in 81 ms on localhost (executor driver) (116/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,431][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 119.0 in stage 5.0 (TID 325)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,438][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=119), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,450][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,472][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,480][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=116),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/116]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 5.0 (TID 322). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 120.0 in stage 5.0 (TID 326, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 116.0 in stage 5.0 (TID 322) in 92 ms on localhost (executor driver) (117/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,482][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 120.0 in stage 5.0 (TID 326)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,486][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=120), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,487][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,487][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=119),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/119]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 5.0 (TID 325). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,500][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 121.0 in stage 5.0 (TID 327, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,501][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 119.0 in stage 5.0 (TID 325) in 71 ms on localhost (executor driver) (118/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,501][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 121.0 in stage 5.0 (TID 327)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,503][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=117),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/117]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,504][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 5.0 (TID 323). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,504][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 122.0 in stage 5.0 (TID 328, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 117.0 in stage 5.0 (TID 323) in 99 ms on localhost (executor driver) (119/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 122.0 in stage 5.0 (TID 328)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,505][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,507][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=121), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,511][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,512][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=122), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,515][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,516][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,523][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=118),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/118]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,526][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 5.0 (TID 324). 3740 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 123.0 in stage 5.0 (TID 329, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 118.0 in stage 5.0 (TID 324) in 112 ms on localhost (executor driver) (120/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,527][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 123.0 in stage 5.0 (TID 329)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,528][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,536][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=123), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,537][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,538][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=120),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/120]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 5.0 (TID 326). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,555][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 124.0 in stage 5.0 (TID 330, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 120.0 in stage 5.0 (TID 326) in 75 ms on localhost (executor driver) (121/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,556][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 124.0 in stage 5.0 (TID 330)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,559][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=124), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,560][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,599][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=121),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/121]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,600][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 5.0 (TID 327). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 125.0 in stage 5.0 (TID 331, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 121.0 in stage 5.0 (TID 327) in 102 ms on localhost (executor driver) (122/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,601][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 125.0 in stage 5.0 (TID 331)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,605][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=125), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,606][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=122),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/122]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,637][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 5.0 (TID 328). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 126.0 in stage 5.0 (TID 332, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,638][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 122.0 in stage 5.0 (TID 328) in 134 ms on localhost (executor driver) (123/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 126.0 in stage 5.0 (TID 332)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,645][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=125),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/125]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 5.0 (TID 331). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,646][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 127.0 in stage 5.0 (TID 333, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 125.0 in stage 5.0 (TID 331) in 47 ms on localhost (executor driver) (124/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 127.0 in stage 5.0 (TID 333)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,649][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=126), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=127), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,650][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,651][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,679][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 29 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=123),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/123]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 5.0 (TID 329). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,708][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 128.0 in stage 5.0 (TID 334, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 128.0 in stage 5.0 (TID 334)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,709][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 123.0 in stage 5.0 (TID 329) in 182 ms on localhost (executor driver) (125/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,715][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=128), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,716][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,717][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,729][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,730][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=124),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/124]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 5.0 (TID 330). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 129.0 in stage 5.0 (TID 335, localhost, executor driver, partition 129, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 129.0 in stage 5.0 (TID 335)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 124.0 in stage 5.0 (TID 330) in 177 ms on localhost (executor driver) (126/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,735][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=126),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/126]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 5.0 (TID 332). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 130.0 in stage 5.0 (TID 336, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 126.0 in stage 5.0 (TID 332) in 100 ms on localhost (executor driver) (127/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 130.0 in stage 5.0 (TID 336)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,745][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=129), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,746][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=130), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,747][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,769][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=127),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/127]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 5.0 (TID 333). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,770][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 131.0 in stage 5.0 (TID 337, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 131.0 in stage 5.0 (TID 337)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,771][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 127.0 in stage 5.0 (TID 333) in 125 ms on localhost (executor driver) (128/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,773][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=128),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/128]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,774][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,775][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 5.0 (TID 334). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,775][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 132.0 in stage 5.0 (TID 338, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,776][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 132.0 in stage 5.0 (TID 338)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,776][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 128.0 in stage 5.0 (TID 334) in 68 ms on localhost (executor driver) (129/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=131), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=132), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,780][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=129),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/129]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,793][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 5.0 (TID 335). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 133.0 in stage 5.0 (TID 339, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=130),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/130]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 129.0 in stage 5.0 (TID 335) in 62 ms on localhost (executor driver) (130/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 133.0 in stage 5.0 (TID 339)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,795][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 5.0 (TID 336). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 134.0 in stage 5.0 (TID 340, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 130.0 in stage 5.0 (TID 336) in 58 ms on localhost (executor driver) (131/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,796][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 134.0 in stage 5.0 (TID 340)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,799][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=134), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,800][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=133), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,801][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=131),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/131]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 5.0 (TID 337). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 135.0 in stage 5.0 (TID 341, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 131.0 in stage 5.0 (TID 337) in 69 ms on localhost (executor driver) (132/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 135.0 in stage 5.0 (TID 341)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,842][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=135), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,843][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=134),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/134]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 5.0 (TID 340). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 136.0 in stage 5.0 (TID 342, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 136.0 in stage 5.0 (TID 342)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 134.0 in stage 5.0 (TID 340) in 54 ms on localhost (executor driver) (133/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,850][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=132),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/132]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,851][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 5.0 (TID 338). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 137.0 in stage 5.0 (TID 343, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 132.0 in stage 5.0 (TID 338) in 77 ms on localhost (executor driver) (134/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 137.0 in stage 5.0 (TID 343)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=136), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,855][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=137), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,856][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,857][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,876][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,880][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,916][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=137),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/137]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 5.0 (TID 343). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,917][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 138.0 in stage 5.0 (TID 344, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 138.0 in stage 5.0 (TID 344)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 137.0 in stage 5.0 (TID 343) in 67 ms on localhost (executor driver) (135/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,921][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=138), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,922][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=135),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/135]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,937][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 5.0 (TID 341). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 139.0 in stage 5.0 (TID 345, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 139.0 in stage 5.0 (TID 345)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,938][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 135.0 in stage 5.0 (TID 341) in 100 ms on localhost (executor driver) (136/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,942][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=139), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,943][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=133),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/133]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=136),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/136]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,946][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 5.0 (TID 339). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 5.0 (TID 342). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 140.0 in stage 5.0 (TID 346, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 133.0 in stage 5.0 (TID 339) in 153 ms on localhost (executor driver) (137/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,947][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 140.0 in stage 5.0 (TID 346)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 141.0 in stage 5.0 (TID 347, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 141.0 in stage 5.0 (TID 347)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,948][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 136.0 in stage 5.0 (TID 342) in 100 ms on localhost (executor driver) (138/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=140), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,951][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=141), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:09,952][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,034][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=139),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/139]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,035][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 5.0 (TID 345). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 142.0 in stage 5.0 (TID 348, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 139.0 in stage 5.0 (TID 345) in 99 ms on localhost (executor driver) (139/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,036][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 142.0 in stage 5.0 (TID 348)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=140),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/140]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 5.0 (TID 346). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 143.0 in stage 5.0 (TID 349, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,040][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 140.0 in stage 5.0 (TID 346) in 94 ms on localhost (executor driver) (140/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 143.0 in stage 5.0 (TID 349)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=142), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=141),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/141]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,041][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=138),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/138]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 5.0 (TID 347). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 5.0 (TID 344). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,042][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 144.0 in stage 5.0 (TID 350, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 141.0 in stage 5.0 (TID 347) in 97 ms on localhost (executor driver) (141/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 144.0 in stage 5.0 (TID 350)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=143), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,045][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 145.0 in stage 5.0 (TID 351, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 138.0 in stage 5.0 (TID 344) in 129 ms on localhost (executor driver) (142/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 145.0 in stage 5.0 (TID 351)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,046][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=145), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,049][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,050][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,051][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=144), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,052][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,081][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=145),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/145]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=143),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/143]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 5.0 (TID 351). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,100][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 5.0 (TID 349). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 146.0 in stage 5.0 (TID 352, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 146.0 in stage 5.0 (TID 352)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 147.0 in stage 5.0 (TID 353, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 145.0 in stage 5.0 (TID 351) in 56 ms on localhost (executor driver) (143/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,101][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 147.0 in stage 5.0 (TID 353)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,102][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 143.0 in stage 5.0 (TID 349) in 62 ms on localhost (executor driver) (144/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,104][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=147), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=146), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,105][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,107][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,118][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=142),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/142]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 5.0 (TID 348). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=144),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/144]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,119][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 148.0 in stage 5.0 (TID 354, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 142.0 in stage 5.0 (TID 348) in 84 ms on localhost (executor driver) (145/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,120][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 5.0 (TID 350). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,121][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 148.0 in stage 5.0 (TID 354)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,124][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 149.0 in stage 5.0 (TID 355, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,127][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 144.0 in stage 5.0 (TID 350) in 84 ms on localhost (executor driver) (146/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,128][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 149.0 in stage 5.0 (TID 355)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,134][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=148), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,138][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,143][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,144][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=149), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,145][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,163][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=147),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/147]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,164][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 5.0 (TID 353). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 150.0 in stage 5.0 (TID 356, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 150.0 in stage 5.0 (TID 356)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,165][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 147.0 in stage 5.0 (TID 353) in 64 ms on localhost (executor driver) (147/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,168][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=150), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,169][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,179][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,180][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,182][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,212][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=146),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/146]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 5.0 (TID 352). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,213][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 151.0 in stage 5.0 (TID 357, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 151.0 in stage 5.0 (TID 357)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,214][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 146.0 in stage 5.0 (TID 352) in 113 ms on localhost (executor driver) (148/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,215][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=148),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/148]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 5.0 (TID 354). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,216][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 152.0 in stage 5.0 (TID 358, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 148.0 in stage 5.0 (TID 354) in 98 ms on localhost (executor driver) (149/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 152.0 in stage 5.0 (TID 358)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,217][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=151), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=149),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/149]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,218][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,219][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 5.0 (TID 355). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,219][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 153.0 in stage 5.0 (TID 359, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 149.0 in stage 5.0 (TID 355) in 97 ms on localhost (executor driver) (150/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,220][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 153.0 in stage 5.0 (TID 359)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=152), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,221][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,222][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=153), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,224][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,225][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,227][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,254][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=151),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/151]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=153),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/153]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,274][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 5.0 (TID 357). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 5.0 (TID 359). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 154.0 in stage 5.0 (TID 360, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,275][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 155.0 in stage 5.0 (TID 361, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 151.0 in stage 5.0 (TID 357) in 63 ms on localhost (executor driver) (151/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 155.0 in stage 5.0 (TID 361)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 154.0 in stage 5.0 (TID 360)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,276][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 153.0 in stage 5.0 (TID 359) in 57 ms on localhost (executor driver) (152/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,277][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=150),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/150]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,279][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=154), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=155), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,280][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 5.0 (TID 356). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 156.0 in stage 5.0 (TID 362, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,284][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 150.0 in stage 5.0 (TID 356) in 119 ms on localhost (executor driver) (153/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,285][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 156.0 in stage 5.0 (TID 362)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,288][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=156), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,289][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,307][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,314][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=152),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/152]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 5.0 (TID 358). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 157.0 in stage 5.0 (TID 363, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 157.0 in stage 5.0 (TID 363)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,321][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 152.0 in stage 5.0 (TID 358) in 105 ms on localhost (executor driver) (154/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,329][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=157), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,330][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=154),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/154]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 5.0 (TID 360). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 158.0 in stage 5.0 (TID 364, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 158.0 in stage 5.0 (TID 364)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 154.0 in stage 5.0 (TID 360) in 70 ms on localhost (executor driver) (155/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,348][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=158), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=156),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/156]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 5.0 (TID 362). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,352][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 159.0 in stage 5.0 (TID 365, localhost, executor driver, partition 159, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 156.0 in stage 5.0 (TID 362) in 69 ms on localhost (executor driver) (156/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=155),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/155]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,353][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 5.0 (TID 361). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 160.0 in stage 5.0 (TID 366, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 160.0 in stage 5.0 (TID 366)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 155.0 in stage 5.0 (TID 361) in 79 ms on localhost (executor driver) (157/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,354][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 159.0 in stage 5.0 (TID 365)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,357][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=160), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,358][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,369][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=159), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,370][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,371][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,388][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=157),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/157]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,396][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 5.0 (TID 363). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,397][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 161.0 in stage 5.0 (TID 367, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 161.0 in stage 5.0 (TID 367)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,398][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 157.0 in stage 5.0 (TID 363) in 78 ms on localhost (executor driver) (158/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=161), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=158),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/158]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,409][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 5.0 (TID 364). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 162.0 in stage 5.0 (TID 368, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 158.0 in stage 5.0 (TID 364) in 66 ms on localhost (executor driver) (159/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,410][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 162.0 in stage 5.0 (TID 368)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=162), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,414][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,415][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,423][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=159),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/159]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 5.0 (TID 365). 3742 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,424][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 163.0 in stage 5.0 (TID 369, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 163.0 in stage 5.0 (TID 369)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,425][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 159.0 in stage 5.0 (TID 365) in 73 ms on localhost (executor driver) (160/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,428][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=163), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,429][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,432][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,454][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=160),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/160]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,455][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 5.0 (TID 366). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 164.0 in stage 5.0 (TID 370, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 160.0 in stage 5.0 (TID 366) in 102 ms on localhost (executor driver) (161/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 164.0 in stage 5.0 (TID 370)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,461][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=164), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=162),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/162]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 5.0 (TID 368). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 165.0 in stage 5.0 (TID 371, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 165.0 in stage 5.0 (TID 371)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 162.0 in stage 5.0 (TID 368) in 54 ms on localhost (executor driver) (162/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,469][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=165), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,470][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,476][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,481][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=161),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/161]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,483][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 5.0 (TID 367). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 166.0 in stage 5.0 (TID 372, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 166.0 in stage 5.0 (TID 372)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,484][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 161.0 in stage 5.0 (TID 367) in 87 ms on localhost (executor driver) (163/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,513][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,514][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,518][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=166), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,519][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,521][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=164),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/164]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,543][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 5.0 (TID 370). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 167.0 in stage 5.0 (TID 373, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 164.0 in stage 5.0 (TID 370) in 89 ms on localhost (executor driver) (164/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,544][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 167.0 in stage 5.0 (TID 373)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,545][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=165),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/165]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 5.0 (TID 371). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 168.0 in stage 5.0 (TID 374, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 165.0 in stage 5.0 (TID 371) in 82 ms on localhost (executor driver) (165/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,546][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 168.0 in stage 5.0 (TID 374)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,549][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=168), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,550][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=167), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,554][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,564][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=163),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/163]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 5.0 (TID 369). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,571][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 169.0 in stage 5.0 (TID 375, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 163.0 in stage 5.0 (TID 369) in 148 ms on localhost (executor driver) (166/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,572][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 169.0 in stage 5.0 (TID 375)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,575][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,575][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=169), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,576][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,593][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,595][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,596][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,618][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=169),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/169]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 5.0 (TID 375). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,619][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 170.0 in stage 5.0 (TID 376, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 169.0 in stage 5.0 (TID 375) in 49 ms on localhost (executor driver) (167/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 170.0 in stage 5.0 (TID 376)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,620][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,623][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=166),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/166]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,624][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=168),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/168]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,625][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=170), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 5.0 (TID 372). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,626][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 5.0 (TID 374). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 171.0 in stage 5.0 (TID 377, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,627][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 172.0 in stage 5.0 (TID 378, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 171.0 in stage 5.0 (TID 377)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,628][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 166.0 in stage 5.0 (TID 372) in 144 ms on localhost (executor driver) (168/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,630][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 172.0 in stage 5.0 (TID 378)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,631][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 168.0 in stage 5.0 (TID 374) in 85 ms on localhost (executor driver) (169/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=172), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,635][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,636][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,644][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=171), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,647][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,655][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,666][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,687][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=170),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/170]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 5.0 (TID 376). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 173.0 in stage 5.0 (TID 379, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 170.0 in stage 5.0 (TID 376) in 69 ms on localhost (executor driver) (170/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,688][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 173.0 in stage 5.0 (TID 379)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,692][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=173), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,693][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,701][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=167),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/167]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 5.0 (TID 373). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 174.0 in stage 5.0 (TID 380, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,702][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 167.0 in stage 5.0 (TID 373) in 158 ms on localhost (executor driver) (171/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,703][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 174.0 in stage 5.0 (TID 380)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=174), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,706][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,707][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=172),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/172]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,719][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 5.0 (TID 378). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 175.0 in stage 5.0 (TID 381, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 175.0 in stage 5.0 (TID 381)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,720][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 172.0 in stage 5.0 (TID 378) in 92 ms on localhost (executor driver) (172/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=175), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,725][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=171),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/171]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,731][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,732][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 5.0 (TID 377). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 176.0 in stage 5.0 (TID 382, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,737][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 171.0 in stage 5.0 (TID 377) in 110 ms on localhost (executor driver) (173/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,738][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 176.0 in stage 5.0 (TID 382)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,764][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=176), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,765][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,777][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=174),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/174]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,778][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 5.0 (TID 380). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 177.0 in stage 5.0 (TID 383, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 174.0 in stage 5.0 (TID 380) in 77 ms on localhost (executor driver) (174/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,779][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 177.0 in stage 5.0 (TID 383)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,783][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=177), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,784][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,792][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,807][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=175),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/175]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 5.0 (TID 381). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,808][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 178.0 in stage 5.0 (TID 384, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 175.0 in stage 5.0 (TID 381) in 89 ms on localhost (executor driver) (175/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,809][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 178.0 in stage 5.0 (TID 384)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,812][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=178), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,813][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=176),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/176]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 5.0 (TID 382). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,815][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 179.0 in stage 5.0 (TID 385, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 176.0 in stage 5.0 (TID 382) in 79 ms on localhost (executor driver) (176/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,816][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 179.0 in stage 5.0 (TID 385)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=179), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,829][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,847][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=173),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/173]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 5.0 (TID 379). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,848][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 180.0 in stage 5.0 (TID 386, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 180.0 in stage 5.0 (TID 386)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,849][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 173.0 in stage 5.0 (TID 379) in 161 ms on localhost (executor driver) (177/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,852][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=180), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,853][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,861][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,874][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=179),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/179]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,884][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=178),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/178]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 5.0 (TID 385). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 5.0 (TID 384). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,885][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 181.0 in stage 5.0 (TID 387, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 182.0 in stage 5.0 (TID 388, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 179.0 in stage 5.0 (TID 385) in 71 ms on localhost (executor driver) (178/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 182.0 in stage 5.0 (TID 388)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,886][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 178.0 in stage 5.0 (TID 384) in 78 ms on localhost (executor driver) (179/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 181.0 in stage 5.0 (TID 387)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,887][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=177),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/177]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 5.0 (TID 383). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 183.0 in stage 5.0 (TID 389, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,888][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 177.0 in stage 5.0 (TID 383) in 110 ms on localhost (executor driver) (180/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=182), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,889][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=181), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,890][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,891][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,895][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,897][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 183.0 in stage 5.0 (TID 389)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,904][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=180),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/180]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 5.0 (TID 386). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 184.0 in stage 5.0 (TID 390, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 180.0 in stage 5.0 (TID 386) in 57 ms on localhost (executor driver) (181/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,905][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 184.0 in stage 5.0 (TID 390)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,906][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,912][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=183), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=184), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,913][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,914][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,918][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,927][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,936][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,939][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=182),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/182]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 5.0 (TID 388). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 185.0 in stage 5.0 (TID 391, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,940][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 182.0 in stage 5.0 (TID 388) in 54 ms on localhost (executor driver) (182/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,941][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 185.0 in stage 5.0 (TID 391)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,944][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=185), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,945][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=181),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/181]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,950][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 5.0 (TID 387). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,953][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 186.0 in stage 5.0 (TID 392, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 181.0 in stage 5.0 (TID 387) in 69 ms on localhost (executor driver) (183/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,954][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 186.0 in stage 5.0 (TID 392)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,961][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=186), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,962][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,975][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=184),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/184]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 5.0 (TID 390). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 187.0 in stage 5.0 (TID 393, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,976][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 184.0 in stage 5.0 (TID 390) in 71 ms on localhost (executor driver) (184/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,977][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 187.0 in stage 5.0 (TID 393)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,978][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=187), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,981][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,982][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,984][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=183),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/183]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,985][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 5.0 (TID 389). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 188.0 in stage 5.0 (TID 394, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,986][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 183.0 in stage 5.0 (TID 389) in 98 ms on localhost (executor driver) (185/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,987][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 188.0 in stage 5.0 (TID 394)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,988][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,991][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=188), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,992][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:10,993][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,021][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,022][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=185),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/185]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 5.0 (TID 391). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 189.0 in stage 5.0 (TID 395, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 189.0 in stage 5.0 (TID 395)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,024][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 185.0 in stage 5.0 (TID 391) in 84 ms on localhost (executor driver) (186/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,030][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,030][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=189), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,031][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,037][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=186),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/186]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 5.0 (TID 392). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,038][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 190.0 in stage 5.0 (TID 396, localhost, executor driver, partition 190, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 186.0 in stage 5.0 (TID 392) in 86 ms on localhost (executor driver) (187/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,039][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 190.0 in stage 5.0 (TID 396)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,043][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=190), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,044][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,062][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=188),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/188]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 5.0 (TID 394). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,063][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 191.0 in stage 5.0 (TID 397, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 188.0 in stage 5.0 (TID 394) in 78 ms on localhost (executor driver) (188/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,064][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 191.0 in stage 5.0 (TID 397)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,067][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=187),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/187]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 5.0 (TID 393). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=191), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,068][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 192.0 in stage 5.0 (TID 398, localhost, executor driver, partition 192, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 187.0 in stage 5.0 (TID 393) in 93 ms on localhost (executor driver) (189/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,069][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 192.0 in stage 5.0 (TID 398)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,070][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=192), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 1 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,074][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,088][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,106][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,116][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=190),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/190]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=189),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/189]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=192),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/192]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,199][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=191),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/191]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,200][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 5.0 (TID 396). 3739 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 5.0 (TID 395). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 5.0 (TID 398). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 5.0 (TID 397). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 193.0 in stage 5.0 (TID 399, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 193.0 in stage 5.0 (TID 399)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,201][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 194.0 in stage 5.0 (TID 400, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 190.0 in stage 5.0 (TID 396) in 164 ms on localhost (executor driver) (190/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 194.0 in stage 5.0 (TID 400)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 195.0 in stage 5.0 (TID 401, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,202][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 189.0 in stage 5.0 (TID 395) in 179 ms on localhost (executor driver) (191/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 195.0 in stage 5.0 (TID 401)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 196.0 in stage 5.0 (TID 402, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 192.0 in stage 5.0 (TID 398) in 135 ms on localhost (executor driver) (192/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 196.0 in stage 5.0 (TID 402)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,203][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 191.0 in stage 5.0 (TID 397) in 140 ms on localhost (executor driver) (193/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,207][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,208][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,239][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=195), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=196), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,240][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=194), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,244][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=193), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,247][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,248][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,249][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 2 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,251][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,323][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=195),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/195]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=196),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/196]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 5.0 (TID 401). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,341][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 5.0 (TID 402). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 197.0 in stage 5.0 (TID 403, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 198.0 in stage 5.0 (TID 404, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 197.0 in stage 5.0 (TID 403)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 195.0 in stage 5.0 (TID 401) in 140 ms on localhost (executor driver) (194/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,342][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 198.0 in stage 5.0 (TID 404)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,343][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 196.0 in stage 5.0 (TID 402) in 140 ms on localhost (executor driver) (195/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,344][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=193),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/193]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 5.0 (TID 399). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,345][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=198), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 199.0 in stage 5.0 (TID 405, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 193.0 in stage 5.0 (TID 399) in 145 ms on localhost (executor driver) (196/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 199.0 in stage 5.0 (TID 405)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=194),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/194]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,346][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,347][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 5.0 (TID 400). 3738 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,349][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 194.0 in stage 5.0 (TID 400) in 148 ms on localhost (executor driver) (197/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=197), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Retrieved version 1 of HDFSStateStoreProvider[id = (op=0, part=199), dir = /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199] for update
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,350][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Getting 0 non-empty blocks out of 1 blocks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,351][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,374][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,375][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Committed version 2 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199] to file /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199/2.delta
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,400][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=197),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/197]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 5.0 (TID 403). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,401][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 197.0 in stage 5.0 (TID 403) in 59 ms on localhost (executor driver) (198/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=199),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/199]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Aborted version 2 for HDFSStateStore[id=(op=0,part=198),dir=/private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648/state/0/198]
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 5.0 (TID 405). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,403][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 5.0 (TID 404). 3695 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 199.0 in stage 5.0 (TID 405) in 59 ms on localhost (executor driver) (199/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 198.0 in stage 5.0 (TID 404) in 62 ms on localhost (executor driver) (200/200)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 5.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,404][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 5 (start at StreamingFile.scala:61) finished in 4.518 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,405][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 3 finished: start at StreamingFile.scala:61, took 4.643375 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,438][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 4 (start at StreamingFile.scala:61) with 1 output partitions
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 6 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,439][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,440][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 6 (MapPartitionsRDD[27] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,442][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_12 stored as values in memory (estimated size 8.7 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,444][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_12_piece0 in memory on 192.168.1.33:55782 (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 12 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(0))
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,445][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 6.0 with 1 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,446][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 6.0 (TID 406, localhost, executor driver, partition 0, PROCESS_LOCAL, 6076 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,447][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 6.0 (TID 406)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 6.0 (TID 406). 1087 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,452][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 6.0 (TID 406) in 6 ms on localhost (executor driver) (1/1)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 6.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 6 (start at StreamingFile.scala:61) finished in 0.007 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,453][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 4 finished: start at StreamingFile.scala:61, took 0.015013 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting job: start at StreamingFile.scala:61
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Got job 5 (start at StreamingFile.scala:61) with 3 output partitions
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,456][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Final stage: ResultStage 7 (start at StreamingFile.scala:61)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,457][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting ResultStage 7 (MapPartitionsRDD[27] at start at StreamingFile.scala:61), which has no missing parents
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,462][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_13 stored as values in memory (estimated size 8.7 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,463][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.6 KB, free 910.7 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Added broadcast_13_piece0 in memory on 192.168.1.33:55782 (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,464][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Created broadcast 13 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at start at StreamingFile.scala:61) (first 15 tasks are for partitions Vector(1, 2, 3))
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,465][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Adding task set 7.0 with 3 tasks
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 7.0 (TID 407, localhost, executor driver, partition 1, PROCESS_LOCAL, 6130 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,466][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 7.0 (TID 408, localhost, executor driver, partition 2, PROCESS_LOCAL, 6124 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 7.0 (TID 409, localhost, executor driver, partition 3, PROCESS_LOCAL, 6136 bytes)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 0.0 in stage 7.0 (TID 407)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 1.0 in stage 7.0 (TID 408)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,467][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Running task 2.0 in stage 7.0 (TID 409)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,489][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 7.0 (TID 407). 1107 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,492][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 7.0 (TID 407) in 26 ms on localhost (executor driver) (1/3)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,494][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 7.0 (TID 408). 1112 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,496][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 7.0 (TID 408) in 30 ms on localhost (executor driver) (2/3)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,497][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 7.0 (TID 409). 1122 bytes result sent to driver
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 7.0 (TID 409) in 31 ms on localhost (executor driver) (3/3)
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed TaskSet 7.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,498][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | ResultStage 7 (start at StreamingFile.scala:61) finished in 0.032 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,499][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Job 5 finished: start at StreamingFile.scala:61, took 0.042782 s
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,522][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:06.001Z",
  "numInputRows" : 100,
  "inputRowsPerSecond" : 50.100200400801604,
  "processedRowsPerSecond" : 18.145527127563057,
  "durationMs" : {
    "addBatch" : 5301,
    "getBatch" : 28,
    "getOffset" : 76,
    "queryPlanning" : 16,
    "triggerExecution" : 5501,
    "walCommit" : 58
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 15
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 0
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 100,
    "inputRowsPerSecond" : 50.100200400801604,
    "processedRowsPerSecond" : 18.145527127563057
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[31m[WARN ][0;39m [35m[2017-11-05 15:56:11,610][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logWarning][0;39m | Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 5609 milliseconds
[34m[INFO ][0;39m [35m[2017-11-05 15:56:11,614][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:11.610Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:56:22,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:56:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:56:46,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:46.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 8,
    "triggerExecution" : 9
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:56:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:56:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:57:08,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:57:08.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:57:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:57:20.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:57:32,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:57:32.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:57:44,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:57:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:57:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:57:56.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:58:06,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:58:06.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:58:16,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:58:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:58:28,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:58:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:58:38,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:58:38.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:58:48,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:58:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 10
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:59:00,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:59:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:59:12,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:59:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:59:22,053][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:59:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 13,
    "triggerExecution" : 15
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:59:34,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:59:34.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:59:44,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:59:44.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 15:59:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T17:59:54.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:00:04,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:00:04.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:00:14,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:00:14.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:00:26,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:00:26.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:00:36,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:00:36.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:00:46,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:00:46.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:00:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:00:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:01:06,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:01:06.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:01:18,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:01:18.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:01:28,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:01:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:01:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:01:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:01:50,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:01:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:02:02,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:02:02.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:02:14,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:02:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:02:26,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:02:26.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 11,
    "triggerExecution" : 12
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:02:38,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:02:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:02:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:02:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:02:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:02:58.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:03:08,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:03:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:03:20,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:03:20.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:03:32,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:03:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:03:42,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:03:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:03:52,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:03:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:04:02,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:04:02.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:04:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:04:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:04:24,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:04:24.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:04:34,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:04:34.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:04:46,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:04:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:04:56,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:04:56.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:05:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:05:08.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:05:18,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:05:18.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:05:28,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:05:28.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:05:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:05:40.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:05:50,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:05:50.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:06:02,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:06:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:06:12,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:06:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:06:22,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:06:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:06:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:06:34.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:06:44,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:06:44.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:06:56,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:06:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:07:08,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:07:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:07:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:07:20.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:07:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:07:30.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:07:40,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:07:40.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:07:50,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:07:50.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:08:00,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:08:00.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:08:10,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:08:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:08:20,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:08:20.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:08:30,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:08:30.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:08:42,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:08:42.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:08:52,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:08:52.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:09:02,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:09:02.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:09:12,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:09:12.007Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:09:22,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:09:22.009Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:09:34,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:09:34.008Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:09:44,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:09:44.008Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:09:54,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:09:54.009Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:10:06,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:10:06.008Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:10:18,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:10:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:10:28,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:10:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:10:31,789][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_13_piece0 on 192.168.1.33:55782 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:10:31,791][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 229
[34m[INFO ][0;39m [35m[2017-11-05 16:10:31,794][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_12_piece0 on 192.168.1.33:55782 in memory (size: 4.6 KB, free: 912.1 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:10:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:10:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:10:50,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:10:50.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:11:02,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:11:02.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:11:14,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:11:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:11:24,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:11:24.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:11:36,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:11:36.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:11:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:11:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:11:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:11:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:12:08,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:12:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:12:20,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:12:20.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:12:32,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:12:32.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:12:42,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:12:42.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:12:54,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:12:54.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:13:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:13:04.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:13:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:13:14.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:13:26,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:13:26.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:13:36,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:13:36.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:13:46,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:13:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:13:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:13:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:14:08,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:14:08.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:14:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:14:20.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:14:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:14:30.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:14:42,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:14:42.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:14:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:14:54.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:15:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:15:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:15:16,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:15:16.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:15:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:15:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:15:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:15:40.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:15:50,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:15:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:16:02,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:16:02.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:16:12,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:16:12.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:16:22,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:16:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:16:32,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:16:32.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 8,
    "triggerExecution" : 8
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:16:44,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:16:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:16:54,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:16:54.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:17:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:17:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:17:16,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:17:16.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:17:26,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:17:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:17:36,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:17:36.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:17:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:17:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:17:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:17:58.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:18:10,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:18:10.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:18:20,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:18:20.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:18:30,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:18:30.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:18:42,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:18:42.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:18:52,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:18:52.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:19:04,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:19:04.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:19:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:19:14.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:19:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:19:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:19:34,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:19:34.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:19:44,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:19:44.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:19:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:19:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:20:06,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:20:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:20:16,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:20:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:20:26,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:20:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:20:38,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:20:38.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:20:50,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:20:50.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:21:00,014][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:21:00.010Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:21:12,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:21:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:21:22,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:21:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:21:34,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:21:34.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:21:44,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:21:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:21:54,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:21:54.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:22:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:22:04.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:22:16,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:22:16.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:22:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:22:26.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:22:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:22:38.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:22:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:22:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:23:00,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:23:00.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:23:10,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:23:10.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:23:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:23:20.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:23:30,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:23:30.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:23:40,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:23:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:23:50,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:23:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:24:00,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:24:00.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:24:10,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:24:10.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:24:22,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:24:22.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:24:34,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:24:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:24:44,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:24:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:24:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:24:56.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:25:06,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:25:06.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:25:16,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:25:16.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,814][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 4
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,817][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_11_piece0 on 192.168.1.33:55782 in memory (size: 21.9 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,818][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_9_piece0 on 192.168.1.33:55782 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,830][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned shuffle 0
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_8_piece0 on 192.168.1.33:55782 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 12
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 25
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,831][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 17
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 21
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 5
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 23
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,832][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on 192.168.1.33:55782 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 16
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 14
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 9
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 13
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,833][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 6
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 20
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 29
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 8
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 3
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 15
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 11
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 10
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,834][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 28
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,835][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on 192.168.1.33:55782 in memory (size: 20.7 KB, free: 912.2 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 7
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,836][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 22
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_4_piece0 on 192.168.1.33:55782 in memory (size: 21.9 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 26
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 19
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,837][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 18
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 27
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,838][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Cleaned accumulator 24
[34m[INFO ][0;39m [35m[2017-11-05 16:25:20,839][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on 192.168.1.33:55782 in memory (size: 20.7 KB, free: 912.3 MB)
[34m[INFO ][0;39m [35m[2017-11-05 16:25:28,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:25:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:25:38,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:25:38.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:25:48,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:25:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:26:00,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:26:00.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:26:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:26:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:26:20,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:26:20.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:26:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:26:30.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:26:40,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:26:40.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:26:52,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:26:52.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:27:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:27:04.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:27:14,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:27:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:27:24,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:27:24.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:27:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:27:36.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:27:46,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:27:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:27:56,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:27:56.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:28:08,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:28:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:28:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:28:18.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:28:30,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:28:30.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:28:42,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:28:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:28:52,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:28:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:29:02,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:29:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:29:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:29:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:29:24,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:29:24.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:29:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:29:34.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:29:46,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:29:46.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:29:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:29:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:30:06,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:30:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:30:18,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:30:18.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:30:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:30:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:30:40,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:30:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:30:50,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:30:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:31:00,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:31:00.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:31:12,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:31:12.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:31:24,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:31:24.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:31:34,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:31:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:31:44,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:31:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:31:56,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:31:56.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:32:06,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:32:06.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:32:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:32:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:32:28,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:32:28.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:32:38,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:32:38.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:32:48,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:32:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:33:00,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:33:00.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:33:10,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:33:10.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:33:20,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:33:20.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:33:32,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:33:32.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:33:44,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:33:44.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:33:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:33:56.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:34:06,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:34:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:34:18,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:34:18.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:34:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:34:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:34:40,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:34:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:34:50,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:34:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:35:00,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:35:00.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:35:12,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:35:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:35:22,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:35:22.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:35:32,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:35:32.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:35:44,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:35:44.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:35:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:35:54.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:36:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:36:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:36:16,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:36:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:36:26,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:36:26.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:36:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:36:38.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:36:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:36:48.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:36:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:36:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:37:08,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:37:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:37:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:37:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:37:30,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:37:30.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:37:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:37:42.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:37:52,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:37:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:38:02,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:38:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:38:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:38:14.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:38:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:38:24.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:38:36,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:38:36.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:38:46,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:38:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:38:56,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:38:56.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:39:06,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:39:06.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:39:18,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:39:18.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:39:28,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:39:28.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:39:40,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:39:40.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:39:52,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:39:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:40:04,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:40:04.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:40:16,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:40:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:40:28,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:40:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:40:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:40:38.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:40:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:40:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:40:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:40:58.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:41:10,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:41:10.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:41:20,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:41:20.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:41:32,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:41:32.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:41:44,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:41:44.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:41:54,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:41:54.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:42:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:42:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:42:16,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:42:16.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:42:28,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:42:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:42:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:42:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:42:52,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:42:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:43:02,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:43:02.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:43:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:43:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:43:24,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:43:24.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:43:34,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:43:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:43:44,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:43:44.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:43:56,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:43:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:44:08,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:44:08.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:44:18,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:44:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:44:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:44:28.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:44:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:44:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:44:50,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:44:50.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:45:00,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:45:00.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:45:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:45:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:45:20,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:45:20.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:45:30,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:45:30.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:45:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:45:40.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:45:52,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:45:52.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:46:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:46:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:46:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:46:14.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:46:26,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:46:26.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:46:36,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:46:36.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:46:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:46:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:46:58,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:46:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:47:10,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:47:10.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:47:22,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:47:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:47:34,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:47:34.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:47:44,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:47:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:47:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:47:54.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:48:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:48:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:48:16,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:48:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:48:28,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:48:28.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:48:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:48:40.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:48:52,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:48:52.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:49:02,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:49:02.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:49:14,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:49:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:49:24,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:49:24.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:49:34,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:49:34.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:49:46,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:49:46.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:49:58,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:49:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:50:10,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:50:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:50:20,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:50:20.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:50:30,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:50:30.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:50:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:50:40.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:50:50,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:50:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:51:00,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:51:00.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:51:10,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:51:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:51:22,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:51:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:51:32,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:51:32.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:51:44,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:51:44.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:51:54,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:51:54.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:52:04,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:52:04.012Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:52:16,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:52:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:52:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:52:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:52:38,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:52:38.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:52:48,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:52:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 10,
    "triggerExecution" : 10
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:53:00,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:53:00.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:53:12,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:53:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:53:22,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:53:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:53:34,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:53:34.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:53:46,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:53:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:53:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:53:58.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:54:10,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:54:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:54:22,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:54:22.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:54:32,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:54:32.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:54:44,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:54:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:54:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:54:54.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:55:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:55:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:55:16,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:55:16.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:55:28,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:55:28.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 8,
    "triggerExecution" : 8
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:55:40,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:55:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:55:50,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:55:50.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:56:02,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:56:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:56:12,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:56:12.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:56:22,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:56:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:56:34,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:56:34.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:56:44,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:56:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:56:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:56:54.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:57:06,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:57:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:57:16,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:57:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:57:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:57:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:57:38,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:57:38.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:57:50,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:57:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:58:00,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:58:00.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:58:10,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:58:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:58:22,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:58:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:58:32,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:58:32.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:58:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:58:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:58:52,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:58:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:59:02,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:59:02.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:59:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:59:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:59:24,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:59:24.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:59:34,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:59:34.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 10,
    "triggerExecution" : 10
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:59:46,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:59:46.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 16:59:56,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T18:59:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:00:06,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:00:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:00:16,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:00:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:00:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:00:26.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:00:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:00:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:00:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:00:48.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:00:58,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:00:58.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:01:08,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:01:08.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:01:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:01:18.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:01:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:01:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:01:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:01:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:01:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:01:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:01:58,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:01:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:02:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:02:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:02:20,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:02:20.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:02:32,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:02:32.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:02:44,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:02:44.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:02:54,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:02:54.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:03:06,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:03:06.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:03:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:03:18.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:03:28,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:03:28.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:03:38,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:03:38.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:03:48,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:03:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:03:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:03:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:04:08,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:04:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:04:18,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:04:18.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:04:30,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:04:30.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:04:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:04:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:04:50,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:04:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:05:02,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:05:02.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:05:12,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:05:12.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:05:22,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:05:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:05:34,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:05:34.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:05:46,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:05:46.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:05:56,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:05:56.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:06:06,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:06:06.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:06:18,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:06:18.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:06:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:06:30.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:06:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:06:40.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:06:52,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:06:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:07:02,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:07:02.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:07:12,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:07:12.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:07:24,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:07:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:07:36,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:07:36.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:07:46,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:07:46.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:07:56,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:07:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:08:08,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:08:08.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:08:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:08:18.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:08:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:08:28.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:08:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:08:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:08:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:08:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:08:58,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:08:58.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:09:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:09:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:09:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:09:18.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:09:30,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:09:30.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:09:40,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:09:40.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:09:52,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:09:52.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:10:02,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:10:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:10:12,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:10:12.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:10:22,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:10:22.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:10:32,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:10:32.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:10:44,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:10:44.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:10:54,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:10:54.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:11:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:11:04.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:11:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:11:14.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:11:26,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:11:26.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:11:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:11:36.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:11:46,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:11:46.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:11:56,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:11:56.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:12:08,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:12:08.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:12:20,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:12:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:12:30,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:12:30.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:12:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:12:42.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:12:52,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:12:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:13:02,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:13:02.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:13:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:13:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:13:24,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:13:24.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:13:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:13:34.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:13:46,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:13:46.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:13:58,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:13:58.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:14:08,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:14:08.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:14:18,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:14:18.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:14:30,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:14:30.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:14:42,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:14:42.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:14:54,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:14:54.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:15:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:15:04.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:15:14,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:15:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:15:24,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:15:24.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:15:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:15:36.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:15:46,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:15:46.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:15:56,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:15:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:16:06,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:16:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:16:16,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:16:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:16:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:16:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:16:36,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:16:36.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:16:46,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:16:46.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:16:56,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:16:56.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:17:08,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:17:08.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:17:20,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:17:20.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:17:32,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:17:32.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:17:42,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:17:42.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:17:54,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:17:54.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:18:04,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:18:04.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:18:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:18:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:18:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:18:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:18:36,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:18:36.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:18:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:18:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:18:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:18:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:19:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:19:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:19:20,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:19:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:19:32,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:19:32.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:19:42,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:19:42.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:19:54,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:19:54.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:20:04,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:20:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:20:16,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:20:16.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:20:26,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:20:26.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:20:36,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:20:36.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:20:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:20:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:20:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:20:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:21:10,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:21:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:21:22,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:21:22.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:21:32,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:21:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:21:44,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:21:44.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:21:54,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:21:54.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:22:06,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:22:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:22:16,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:22:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 8
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:22:28,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:22:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:22:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:22:38.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:22:48,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:22:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:23:00,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:23:00.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:23:10,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:23:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 8,
    "triggerExecution" : 8
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:23:20,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:23:20.007Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:23:32,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:23:32.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:23:42,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:23:42.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:23:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:23:54.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:24:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:24:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:24:16,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:24:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:24:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:24:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:24:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:24:38.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:24:50,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:24:50.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:25:00,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:25:00.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:25:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:25:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:25:22,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:25:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:25:32,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:25:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:25:42,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:25:42.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:25:54,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:25:54.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:26:04,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:26:04.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:26:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:26:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:26:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:26:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:26:34,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:26:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:26:44,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:26:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 11,
    "triggerExecution" : 11
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:26:56,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:26:56.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:27:08,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:27:08.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:27:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:27:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:27:30,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:27:30.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:27:40,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:27:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:27:50,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:27:50.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:28:02,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:28:02.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:28:12,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:28:12.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:28:22,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:28:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:28:32,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:28:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:28:42,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:28:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:28:52,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:28:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:29:02,023][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:29:02.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 14,
    "triggerExecution" : 14
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:29:14,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:29:14.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:29:26,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:29:26.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:29:38,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:29:38.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:29:50,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:29:50.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:30:02,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:30:02.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:30:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:30:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:30:24,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:30:24.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:30:34,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:30:34.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 6,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:30:46,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:30:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:30:58,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:30:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:31:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:31:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:31:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:31:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:31:32,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:31:32.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:31:44,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:31:44.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:31:54,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:31:54.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:32:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:32:04.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:32:14,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:32:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:32:24,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:32:24.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:32:36,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:32:36.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:32:48,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:32:48.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:33:00,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:33:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:33:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:33:10.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:33:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:33:20.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:33:30,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:33:30.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:33:40,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:33:40.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:33:52,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:33:52.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:34:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:34:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:34:16,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:34:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:34:26,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:34:26.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:34:36,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:34:36.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:34:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:34:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:34:58,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:34:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:35:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:35:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:35:22,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:35:22.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:35:32,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:35:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:35:42,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:35:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:35:52,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:35:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:36:04,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:36:04.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:36:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:36:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:36:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:36:24.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:36:34,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:36:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:36:46,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:36:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:36:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:36:58.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:37:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:37:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:37:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:37:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:37:30,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:37:30.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:37:40,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:37:40.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:37:52,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:37:52.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:38:02,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:38:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:38:12,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:38:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:38:22,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:38:22.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:38:32,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:38:32.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:38:44,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:38:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:38:54,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:38:54.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 9,
    "triggerExecution" : 9
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:39:06,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:39:06.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:39:18,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:39:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:39:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:39:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:39:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:39:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:39:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:39:48.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:39:58,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:39:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:40:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:40:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:40:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:40:20.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:40:32,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:40:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:40:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:40:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:40:54,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:40:54.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:41:04,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:41:04.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:41:16,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:41:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:41:26,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:41:26.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:41:38,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:41:38.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:41:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:41:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:41:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:41:58.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:42:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:42:10.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:42:20,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:42:20.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:42:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:42:30.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:42:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:42:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:42:52,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:42:52.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:43:02,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:43:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:43:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:43:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:43:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:43:24.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:43:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:43:36.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:43:46,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:43:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:43:58,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:43:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:44:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:44:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:44:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:44:18.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:44:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:44:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:44:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:44:38.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:44:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:44:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:44:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:44:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:45:10,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:45:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:45:22,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:45:22.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:45:34,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:45:34.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:45:44,016][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:45:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 15,
    "triggerExecution" : 15
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:45:56,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:45:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:46:06,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:46:06.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:46:16,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:46:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:46:26,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:46:26.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:46:38,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:46:38.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:46:50,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:46:50.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:47:00,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:47:00.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:47:12,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:47:12.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:47:22,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:47:22.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:47:32,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:47:32.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:47:44,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:47:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:47:56,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:47:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:48:06,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:48:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:48:16,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:48:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:48:26,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:48:26.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:48:36,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:48:36.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:48:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:48:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:48:58,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:48:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:49:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:49:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:49:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:49:18.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:49:28,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:49:28.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:49:40,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:49:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:49:50,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:49:50.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:50:02,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:50:02.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:50:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:50:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:50:24,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:50:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:50:34,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:50:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:50:44,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:50:44.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:50:54,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:50:54.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:51:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:51:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:51:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:51:14.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:51:24,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:51:24.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:51:36,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:51:36.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:51:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:51:48.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:51:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:51:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:52:08,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:52:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:52:18,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:52:18.007Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:52:30,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:52:30.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:52:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:52:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:52:50,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:52:50.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:53:02,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:53:02.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:53:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:53:14.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:53:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:53:26.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:53:38,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:53:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:53:48,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:53:48.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:54:00,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:54:00.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:54:12,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:54:12.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:54:22,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:54:22.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:54:34,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:54:34.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:54:46,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:54:46.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:54:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:54:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:55:08,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:55:08.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:55:18,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:55:18.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 9,
    "triggerExecution" : 9
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:55:30,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:55:30.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:55:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:55:42.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:55:52,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:55:52.008Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:56:04,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:56:04.009Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:56:16,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:56:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:56:28,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:56:28.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:56:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:56:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:56:50,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:56:50.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:57:00,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:57:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:57:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:57:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:57:20,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:57:20.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:57:32,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:57:32.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:57:42,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:57:42.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:57:52,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:57:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:58:02,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:58:02.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:58:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:58:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:58:24,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:58:24.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:58:36,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:58:36.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:58:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:58:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:58:58,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:58:58.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:59:08,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:59:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:59:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:59:18.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:59:30,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:59:30.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:59:42,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:59:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 17:59:52,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T19:59:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:00:02,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:00:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:00:14,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:00:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:00:26,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:00:26.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:00:36,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:00:36.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:00:46,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:00:46.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:00:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:00:58.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:01:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:01:10.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:01:20,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:01:20.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:01:32,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:01:32.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:01:44,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:01:44.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:01:54,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:01:54.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:02:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:02:06.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:02:16,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:02:16.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:02:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:02:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:02:38,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:02:38.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:02:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:02:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:02:58,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:02:58.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:03:08,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:03:08.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:03:18,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:03:18.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:03:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:03:28.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:03:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:03:38.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:03:48,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:03:48.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:04:00,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:04:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:04:12,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:04:12.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:04:24,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:04:24.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:04:34,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:04:34.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:04:44,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:04:44.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:04:56,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:04:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:05:06,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:05:06.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:05:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:05:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:05:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:05:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:05:38,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:05:38.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:05:48,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:05:48.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:06:00,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:06:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:06:12,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:06:12.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:06:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:06:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:06:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:06:36.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:06:46,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:06:46.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:06:56,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:06:56.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:07:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:07:06.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:07:16,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:07:16.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:07:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:07:28.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:07:38,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:07:38.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 8,
    "triggerExecution" : 9
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:07:50,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:07:50.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:08:00,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:08:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:08:10,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:08:10.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:08:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:08:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:08:30,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:08:30.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:08:42,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:08:42.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:08:52,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:08:52.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:09:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:09:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:09:14,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:09:14.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:09:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:09:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:09:36,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:09:36.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:09:46,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:09:46.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:09:56,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:09:56.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:10:08,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:10:08.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:10:20,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:10:20.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:10:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:10:30.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:10:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:10:42.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:10:54,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:10:54.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:11:04,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:11:04.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:11:16,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:11:16.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:11:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:11:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:11:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:11:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:11:52,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:11:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:12:02,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:12:02.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:12:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:12:14.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:12:26,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:12:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:12:36,025][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:12:36.023Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:12:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:12:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:13:00,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:13:00.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:13:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:13:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:13:20,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:13:20.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:13:32,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:13:32.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 10,
    "triggerExecution" : 10
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:13:44,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:13:44.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:13:56,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:13:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:14:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:14:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:14:18,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:14:18.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:14:30,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:14:30.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:14:42,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:14:42.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:14:52,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:14:52.008Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:15:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:15:04.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:15:14,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:15:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:15:24,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:15:24.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:15:34,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:15:34.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:15:46,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:15:46.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:15:56,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:15:56.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:16:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:16:06.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:16:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:16:18.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:16:30,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:16:30.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:16:40,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:16:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:16:50,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:16:50.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:17:00,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:17:00.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:17:10,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:17:10.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:17:22,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:17:22.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:17:32,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:17:32.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:17:44,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:17:44.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:17:54,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:17:54.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:18:06,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:18:06.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:18:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:18:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:18:28,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:18:28.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:18:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:18:38.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:18:50,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:18:50.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:19:02,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:19:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:19:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:19:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:19:26,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:19:26.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:19:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:19:36.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:19:46,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:19:46.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:19:56,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:19:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:20:06,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:20:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:20:16,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:20:16.008Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:20:28,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:20:28.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:20:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:20:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:20:52,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:20:52.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:21:04,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:21:04.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:21:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:21:14.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:21:26,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:21:26.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:21:36,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:21:36.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:21:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:21:48.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:21:58,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:21:58.009Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:22:10,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:22:10.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:22:20,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:22:20.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:22:30,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:22:30.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:22:42,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:22:42.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:22:52,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:22:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:23:04,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:23:04.007Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:23:16,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:23:16.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:23:26,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:23:26.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:23:38,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:23:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 4,
    "triggerExecution" : 4
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:23:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:23:48.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:23:58,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:23:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:24:08,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:24:08.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:24:18,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:24:18.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 6
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:24:30,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:24:30.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:24:40,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:24:40.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:24:52,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:24:52.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:25:02,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:25:02.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:25:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:25:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:25:24,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:25:24.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:25:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:25:36.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:25:46,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:25:46.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:25:58,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:25:58.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:26:08,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:26:08.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:26:20,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:26:20.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:26:30,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:26:30.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:26:42,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:26:42.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:26:54,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:26:54.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:27:04,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:27:04.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:27:16,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:27:16.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:27:26,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:27:26.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:27:36,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:27:36.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:27:48,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:27:48.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:27:58,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:27:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:28:08,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:28:08.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:28:18,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:28:18.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:28:28,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:28:28.007Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:28:40,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:28:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:28:50,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:28:50.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:29:00,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:29:00.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:29:10,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:29:10.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:29:22,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:29:22.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:29:32,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:29:32.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:29:42,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:29:42.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:29:54,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:29:54.009Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:30:06,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:30:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:30:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:30:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:30:28,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:30:28.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:30:38,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:30:38.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:30:48,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:30:48.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:30:58,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:30:58.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:31:10,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:31:10.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:31:20,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:31:20.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:31:32,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:31:32.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:31:42,011][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:31:42.009Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:31:54,006][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:31:54.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:32:06,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:32:06.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:32:16,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:32:16.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:32:26,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:32:26.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:32:38,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:32:38.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:32:48,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:32:48.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:32:58,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:32:58.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:33:08,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:33:08.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:33:18,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:33:18.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:33:28,026][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:33:28.023Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:33:40,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:33:40.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:33:50,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:33:50.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:34:02,012][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:34:02.010Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:34:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:34:14.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:34:24,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:34:24.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:34:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:34:36.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:34:46,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:34:46.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:34:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:34:56.003Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:35:08,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:35:08.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:35:18,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:35:18.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:35:28,008][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:35:28.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:35:40,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:35:40.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:35:50,013][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:35:50.011Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:36:02,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:36:02.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:36:12,007][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:36:12.005Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 1,
    "triggerExecution" : 1
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:36:22,010][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:36:22.004Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 5,
    "triggerExecution" : 5
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:36:34,004][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:36:34.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:36:44,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:36:44.006Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:36:56,005][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:36:56.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:37:06,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:37:06.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 12,
    "triggerExecution" : 12
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:37:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:37:18.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:37:30,002][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:37:30.000Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:37:40,018][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:37:40.016Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:37:52,009][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:37:52.002Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 7,
    "triggerExecution" : 7
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:38:04,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:38:04.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:38:14,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:38:14.017Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:38:24,020][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:38:24.018Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:38:36,003][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:38:36.001Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 2,
    "triggerExecution" : 2
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:38:46,015][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Streaming query made progress: {
  "id" : "43b2ac4a-2ddd-45e0-a80b-646801594e28",
  "runId" : "eaf727d7-0e91-4b6a-a51b-98dcd528c284",
  "name" : null,
  "timestamp" : "2017-11-05T20:38:46.012Z",
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "getOffset" : 3,
    "triggerExecution" : 3
  },
  "stateOperators" : [ {
    "numRowsTotal" : 15,
    "numRowsUpdated" : 0
  } ],
  "sources" : [ {
    "description" : "FileStreamSource[file:/Volumes/PANZER/Github/spark-cases/spark-structured-streaming/dataset/stream_in/*]",
    "startOffset" : {
      "logOffset" : 1
    },
    "endOffset" : {
      "logOffset" : 1
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleSink@2275424e"
  }
}
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,097][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Invoking stop() from shutdown hook
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,197][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.1.33:4040
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,245][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,319][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,320][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,322][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,328][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,333][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,334][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,336][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/spark-60a28690-6e0f-4d03-87f5-72b9b9819361
[34m[INFO ][0;39m [35m[2017-11-05 18:38:53,337][0;39m [33m[][0;39m [35m[org.apache.spark.internal.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/c_/lppyy4ps2252qd4zb5cy22180000gn/T/temporary-a6276c47-788d-4b5d-bb0c-d160018e1648
